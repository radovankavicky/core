% Copyright (c) 2013, authors of "Core Econometrics;" a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\chapter{Optimality properties of point estimation}

\section{Optimality focusing on mean and variance}

\begin{itemize}

\item In this section, we're going to view estimators as algorithms
  and evaluate them based on how well they typically perform.  This
  means that we're going to evaluate estimators on the basis of their
  distributions, called their \emph{sampling distribution}.  

\item Typically, the class of possible point estimators is too large
  to make general statements about optimality.  But we can make
  statements about a restricted class of estimators.  A natural
  restriction to impose is that the estimator must be correct ``on
  average,'' i.e. that it should not make systemic and predictable
  errors.  Such an estimator is called a ``unbiased'' estimator.

  \begin{defn}
    If $\θh$ is an estimator of $θ₀$, the \emph{bias} of $\θh$ is the
    quantity $\E \θh - θ₀$.  An estimator is \emph{unbiased} if its
    bias equals zero for every value of $θ₀$
  \end{defn}

  It can make sense to compare unbiased estimators through their
  dispersion: a more disperse estimator is probably worse than one
  that's more tightly concentrated around the true parameter value
  $θ₀$.

  \begin{defn}
    If $\θh₁$ and $\θh₂$ are two unbiased estimators of a parameter
    $θ$, $\θh₁$ is \emph{more efficient} than $\θh₂$ if the variance
    of $\θh₁$ is less than that of $\θh₂$.
  \end{defn}

  An estimator $\θh$ of $θ$ is said to be the \emph{best unbiased
  estimator of $θ$} if it is unbiased and has smaller variance than
  any other unbiased estimator

\item For parameter vectors, we need the variance of $γ'\θh₁$ to be
  smaller than that of $γ'\θh₂$ for all nonzero $γ$.  This is the same
  as requiring that $\var(\θh₂) - \var(\θh₁)$ is positive definite
  (you should verify this on your own).

\item A particular result comes up in many different contexts
  \begin{thm}
    Suppose $\E \θh₁ = θ$; $\θh₁$ is the best unbiased estimator of
    $θ$ iff $\θh₁$ and $\θh₁ - \θh₂$ are uncorrelated for any other
    unbiased estimator $\θh₂$ of $θ$.
  \end{thm}
  This also holds when restricted to other classes.

  The first part of the proof is easy.  Assume that $\θh₁$ and $\θh₁ -
  \θh₂$ are uncorrelated.  Then
  \begin{align*}
    \var(\θh₂) &= \var(\θh₂ - \θh₁ + \θh₁) \\
    &= \var(\θh₂ - \θh₁) + \var(\θh₁) \\
    &≥ \var(\θh₁)
  \end{align*}
  as required.

  For the second part of the proof, assume $\θh₁$ is the best unbiased
  estimator and consider the variance of the unbiased estimator $a
  \θh₁ + (1 - a) \θh₂$.  Then
  \begin{align*}
    \var(a \θh₁ + (1 - a) \θh₂)
    &= \var(\θh₁ + (1 - a) (\θh₂ - \θh₁)) \\
    &= \var(\θh₁) + (1 - a)² \var(\θh₂ - \θh₁)
    + 2 (1 - a) \cov(\θh₁, \θh₂ - \θh₁).
  \end{align*}

  We can choose $a$ to minimize this variance and find that it is
  smallest for
  \begin{equation*}
    a = 1 + \frac{\cov(\θh₁, \θh₂ - \θh₁)}{\var(\θh₂ - \θh₁)}.
  \end{equation*}
  Plugging in above gives
  \begin{equation*}
    \var(a \θh₁ + (1 - a) \θh₂)
    = \var(\θh₁) - \frac{(\cov(\θh₁, \θh₂ - \θh₁))²}{\var(\θh₂ - \θh₁)}.
  \end{equation*}
  This last quantity is less than $\var(\θh₁)$ (a contradiction)
  unless $\cov(\θh₁, \θh₂ - \θh₁) = 0$, completing the proof.

\item It can also be useful to know the smallest possible variance
  that an unbiased estimator (or estimator in general) could have in a
  particular problem.  Then, if we have an unbiased estimator that has
  that variance, we know that it is ``the best'' estimator possible in
  a certain sense and can stop looking for better estimators.

  The Cramer-Rao lower bound gives us such a bound.
  \begin{thm}
    Suppose $X₁,...,X_n$ have a joint density with likelihood $L(θ;
    X)$ and let $\θh$ be an estimator of $θ$ with finite variance and
    \begin{equation*}
      (d/dθ) ∫_\Xc \θh(x) L(θ; x) \dx = ∫_\Xc \θh(x) (d/dθ) L(θ; x) \dx
    \end{equation*}
    where $\Xc$ is the support of $X₁,...,X_n$.
    Then 
    \begin{equation*}
      \var(\θh) ≥  \frac{((d/dθ) \E \θh(X))²}{\E ((d/dθ) \log L(θ; X))²}.
    \end{equation*}
  \end{thm}
  If the data are \iid\ then this result can be simplified.

  The proof is relatively straightforward and follows from the
  Cauchy-Schwarz inequality.  For any random variables $Y$ and $Z$,
  the CS inequality implies that
  \begin{equation*}
    \cov(Y, Z)² ≤ \var(Y) \var(Z).
  \end{equation*}
  If we let $Y = \θh$ and $Z = (d/dθ) \log L(θ; X)$, we get
  \begin{equation*}
    \var(\θh) ≥
    \frac{\cov(\θh, (d/dθ) \log L(θ; X))²}
         {\var((d/dθ) \log L(θ; X))}
  \end{equation*}
  (as long as $\var((d/dθ) \log L(θ; X)) > 0$), and the result holds
  after we show that
  \begin{equation*}
    \cov(\θh, (d/dθ) \log L(θ; X)) = (d/dθ) \E \θh
  \end{equation*}
  and
  \begin{equation*}
    \E (d/dθ) \log L(θ; X)) = 0,
  \end{equation*}
  which I need to fill in in the future.

  \begin{ex}
    Suppose that $X₁,...,X_n ∼ \iid N(μ, σ²)$ with $σ$² known.  We can
    use the CR lower bound to find the best-case variance of an
    unbiased estimator.  Since
    \begin{equation*}
      d/dμ \log L(μ, σ²; X) = ∑_i (x_i - μ)/σ²,
    \end{equation*}
    we have
    \begin{equation*}
      \E(d/dμ \log L(μ, σ²; X))² = n/σ²,
    \end{equation*}
    and the CR lower bound for any unbiased estimator of $μ$ is
    $σ²/n$.  The sample mean is unbiased and has this variance, so
    it's the best unbiased estimator.
  \end{ex}

\item It is possible that no estimator, even the best one, achieves
  the CR lower bound.

\end{itemize}

\section{Loss-function based optimality}

\begin{itemize}

\item We can also consider other measures of optimality.  One is to
  consider the average loss associated with an estimator.  A
  \emph{loss function} $L$ is a convex function s.t. $L(0) = 0$ that
  measures the cost associated with misestimating $\theta$: the cost
  associated with the error $\θh - θ$ is $L(\θh - θ)$.  An estimator
  $\θh₁$ has lower risk than $\θh₂$ if
  \begin{equation*}
    \E L(\θh₁ - θ) ≤ \E L(\θh₂ - θ)
  \end{equation*}
  
  A standard loss criterion is MSE, where $L(x) = x²$.  Notice that
  MSE weights (squared) bias and variance equally:
  \begin{align*}
    \E (\θh - θ)² &= \E((\θh - \E \θh) + (\E \θh - θ))² \\
    &= \E(\θh - \E \θh)² + 2 \E (\θh - \E \θh) (\E \θh - θ) + (\E \θh - θ)² \\
    &= \var(\θh) + \bias(\θh)²
  \end{align*}
  since the middle term in the second to last equation is zero.  Loss
  functions can also be derived directly from economic criteria like
  utility functions.

\item Loss-function based criteria are rarely decisive, in the sense
  that the best estimator can depend on the parameter values, as shown
  in the next example.
  \begin{ex}
    Suppose that $X₁,...,X_n ∼ \binomial(p)$ and consider two
    estimators of $p$, $\Xb$ and $1/2$.  Then
    \begin{equation*}
      \mse(\Xb) = \bias(\Xb)² + \var(\Xb) = p(1-p)/n
    \end{equation*}
    and
    \begin{equation*}
      \mse(1/2) = \bias(1/2)² + \var(1/2) = (1/2 - p)².
    \end{equation*}
    When $p$ is near $1/2$, the second estimator has lower MSE.  When
    $p$ is not near $1/2$, the first estimator will typically have
    lower MSE.
  \end{ex}

  For loss-function based optimality to be useful, the researcher
  needs to weight the parameter space by importance; if $W(θ)$ is a
  weighting function for the parameter space, we can choose the
  estimator $\θh$ associated with the lowest average risk:
  \begin{equation*}
    ∫ \E L(\θh - θ) W(θ) \dθ.
  \end{equation*}

\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../core_econometrics"
%%% End: 

%  LocalWords:  iff MSE
