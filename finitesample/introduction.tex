% Copyright © 2013, authors of "Core Econometrics;" a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Introduction}%
\addcontentsline{toc}{part}{Introduction}

\input{common/frontmatter}

\section{Finite sample statistics}

\begin{itemize}

\item The \emph{Probability Overview} deals with hypothetical and
  theoretical relationships—where $X$ is a random vector that
  summarizes some experiment.  In this document, we imagine that some
  aspects of that experiment are known and some are unknown and that
  we observe $n$ outcomes, $X₁,…,X_n$, drawn from that experiment.
  The question we'll start to answer is, what can we learn about the
  unknown aspects of the experiment from the observations?

\item Give a concrete example from linear regression.

\item Finite sample, so we need to make implausibly strong assumptions
  about the DGP.  We'll look at ways to address this in the
  \emph{Asymptotics} document.  We'll find that a lot carries over.

\item There are many ways that one could split estimation strategies
  into different groups.  We'll find it useful to use these three
  categories,
  \begin{description}
  \item[Point Estimation:] An estimate of the parameter of interest,
    say $β$ in the linear regression examplem, ideally coupled with a
    measure of the precision of that estimate.

  \item[Hypothesis testing:] A binary outcome, yes or no, indicating
    whether or not the data are consistent with a proposed
    hypothesis.  For the regression example, we might have a
    hypothesis that the slope is zero: the mean of $y$ is the same,
    regardless of the value of $x$.  Note that, in any given data set,
    we might see a positive slope even if this hypothesis is true,
    since the data are generated by a random process.  This means that
    simply looking at the slope and rejecting whenever it's positive
    is a \emph{poor} strategy.

  \item[Bayesian inference:] In contrast to the other two, Bayesian
    inference is concerned with communicating the uncertainty about
    the unknown parameter (again, here it's $β$).  To do this, the
    researcher treats $β$ itself as another random variable with its
    own density, called a \emph{prior density}.  After observing
    $X₁,…,X_n$, the researcher then computes the \emph{posterior
      density}, $f_β(· ∣ X₁,…,X_n)$, which describes uncertainty over
    $β$ after observing the data.  As you can imagine, the choice of
    the prior density sometimes has a strong effect on the analysis
    and sometimes does not, and researchers are usually relieved when
    it does not.

    Sometimes it is natural to view $β$ as a random variable and
    sometimes it is quite unnatural.  Whether or not this assumption
    is realistic or not is irrelevant as far as applying a Bayesian
    analysis goes.  You can think of it as a mathematical model of
    uncertainty without taking it as an assumption about the real
    world.
  \end{description}
  
\item If $x₁,…,x_n$ is a sequence of random variables, any function of
  that sequence, $T(x₁,…,x_n)$ is called a \emph{statistic} or
  \emph{estimator}.  The important part of this defnintion is that an
  estimator can feasibly be constructed from any sequence possible
  outcomes of the random sequence, it can not depend on any additional
  information about the DGP.  For example, the sample mean $\Xb =
  n^{-1} ∑_{i=1}ⁿ X_i$ is an estimator because it is only a function
  of the observed data; the statistic $\max(\Xb, 10)$ is as well,
  because the number 10 is arbitrary and unconnected with the DGP.
  But the random variable $\max(\Xb, \E X₁ + 1)$ is not an estimator,
  because it depends on unknown information about the DGP
  (specifically, $\E X₁$).

\item Also note that some things that are probably pretty bad
  estimators meet our definition of \emph{estimator}.  For example,
  the constants ``30'' and ``1/2'' are both estimators, since they
  don't depend on any unknown features of the DGP.

\item When it is important to distinguish between the algorithm for
  generating a statistic from a dataset and the value that the
  statistic takes on for a particular dataset, we will call the
  algorithm \emph{the estimator} and the particular realization
  \emph{the estimate}.

\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../finitesample"
%%% End: 
