% Copyright (c) 2013â€“2014, authors of "Core Econometrics;" a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\chapter{The Likelihood Ratio Test}

\begin{itemize}[leftmargin=0pt]

\item If we know the distribution of the random sample, and want to
  test a hypothesis about the unknown parameters, we can use the
  likelihood ratio test to get a test statistic.  This can also be
  effective asymptotically when the likelihood function isn't known.

  The \emph{Likelihood Ratio Test statistic} is given by
  \begin{equation*}
    \Lambda(x_1,\dots,x_n) =
    \frac{\sup_{\theta \in \Theta_0} L(\theta; x_1,\dots,x_n)}{\sup_{\theta \in \Theta_0 \cup \Theta_a} L(\theta; x_1,\dots,x_n)}
  \end{equation*}
  and the test rejects if $\Lambda(x_1,...,x_n) \leq c$ where $c$ is chosen so
  that the test has the correct prespecified size.

  [You can draw some pictures connecting this to the maximum
  likelihood estimator]

  This is a useful method because it usually gives you a starting
  approach for a statistic.  As we'll see later, it often gives you
  the best test.

\item An example can help:
  \begin{ex}
    Suppose that $X_1,...,X_n \sim \iid\ \uniform(0, b)$ and we want to
    test the null hypothesis $b \geq 1$ vs $b < 1$.  (add some pictures).
    Intuitively, the test should always reject if $\max_i X_i > 1$,
    but it should also reject for values below but very close to 1.
    
    To start, calculate the likelihood:
    \begin{equation*}
      L(b; x_1,...,x_n) = b^{-n} \ind\{b \geq \max_i X_i\}.
    \end{equation*}
    
    Then (for the numerator)
    \begin{equation*}
      \sup_{b \geq 1} L(b; x_1,\dots,x_n) =
      \begin{cases}
        1 & \text{if } 1 > \max_i X_i \\
        (\max_i X_i)^{-n} & \text{if } 1 \leq \max_i X_i.
      \end{cases}
    \end{equation*}
    and (for the denominator)
    \begin{equation*}
      \sup_{b > 0} L(b; x_1,\dots,x_n) = (\max_i X_i)^{-n}.
    \end{equation*}
    Putting them together, we get
    \begin{equation*}
      \Lambda(x) =
      \begin{cases}
        (\max_i X_i)^n & \text{if } \max_i X_i \leq 1. \\
        1 & \text{if } \max_i X_i > 1 \\
      \end{cases}
    \end{equation*}

    Now, to find $c$ so that $\sup_{b \geq 1} \Pr_b[(\max_i X_i)^n \leq c] = \alpha$
    \begin{align*}
      \sup_{b \geq 1} \Pr_b[(\max_i X_i)^n \leq c]
      &= \sup_{b\geq 1} \Pr_b[\max_i X_i/b \leq c^{1/n}/b] \\
      &= \sup_{b \geq 1} \Pr_b[X_1/b \leq c^{1/n}/b, ..., X_n/b \leq c^{1/n}/b] \\
      &= \sup_{b \geq 1} \Pr_b[X_1/b \leq c^{1/n}/b] \cdots \Pr_b[X_n/b \leq c^{1/n}/b] \\
      &= \sup_{b \geq 1} c/b^n \\
      &= c
    \end{align*}
    where we're using the fact that $X_i/b \sim \uniform(0,1)$.  So
    $c=\alpha$.  This test is obviously equivalent to comparing $\max_i
    X_i$ to $\alpha^{1/n}$.
  \end{ex}

\item We can run some simulations to verify that this works and makes
  sense (in R).
\begin{verbatim}
n <- 1000
crit <- 0.05
b <- 1
stats <- replicate(10000, max(runif(n, 0, b))^n)
hist(stats, 40, col = "alice blue")
mean(stats <= crit)
\end{verbatim}
  And repeat for $n = 100$ and $n = 1000$ as well.
\end{itemize}

\section{Neyman-Pearson Lemma}

\begin{itemize}[leftmargin=0pt]
\item Helps us think about the best possible power
\item Def of UMP: Let C be a class of tests for testing $\theta \in \Theta_0$
  against $\theta \in \Theta_0^c$.  A test in C with power function $\beta(\theta)$ is a
  \emph{uniformly most powerful} (UMP) class C test if $\beta(\theta) \geq \beta'(\theta)$
  for all $\theta \in \Theta^c$ and all $\beta'$ that are power functions of tests in
  C.
\item Usefulness really shows up in asymptotics
\item Often there is no UMP class C test
\begin{itemize}
\item sometimes we can get a UMP test by restricting C further
\end{itemize}
\end{itemize}

\subsection{Statement of Lemma}

\begin{itemize}
\item Consider testing $\theta = \theta_0$ vs $\theta = \theta_1$ where the pdf for each
  parameter value is $f(x; \theta_i)$, $i = 1,2$, using a test with
  rejection region $R$ such that, for some $k \geq 0$,
\begin{itemize}
\item $x \in R$ if $f(x; \theta_1) > k f(x; \theta_0)$
\item $x \notin R$ if $f(x; \theta_1) < k f(x; \theta_0)$
\end{itemize}
\item Then
\begin{enumerate}
\item Any size $\alpha$ test with such a rejection region is a UMP level $\alpha$-test
\item If there is a size $\alpha$ test with such a rejection region for $k
  > 0$, then every UMP level $\alpha$ test has size $\alpha$ and every UMP level
  $\alpha$ test has this rejection region almost surely.
\end{enumerate}
\item Notes
\begin{itemize}
\item both are strict inequalities to allow for discrete random
         variables
\item justifies the LRT (at least for simple tests)
\item most UMP results for more complicated settings are extensions
         of this lemma
\item more or less how we think of power in econometrics
\end{itemize}
\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../core_econometrics"
%%% End:
