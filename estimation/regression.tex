% Copyright © 2013, authors of the "Core Econometrics Textbook;" a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Estimation of linear regression}%
\addcontentsline{toc}{part}{Estimation of linear regression}

\section{Properties of quadratic forms}

\begin{itemize}

\item Quadratic forms will come up a lot when we study linear
  regression.  If $X$ be a random $k$-vector and let $A$ be a $k × k$
  deterministic matrix then $X'A X$ is a (random) quadratic form.
  These are (relatively) easy to study because they can be written as
  summations:
  \begin{equation*}
    X'AX = ∑_{i,j} X_i X_j A_{ij}.
  \end{equation*}

\item Usually, when we discuss quadratic forms, we assume (at least
  implicitly) that $A$ is symmetric.  This assumption is usually
  without loss of generality because
  \begin{equation*}
    X' A X = X'(A/2 + A'/2)X
  \end{equation*}
  almost surely, and $X'(A/2 + A'/2)X$ is symmetric by construction.

\item Here's a representative result for quadratic forms that
  illustrates a useful trick.
  \begin{thm}
    Let $X$ be an $n × 1$ vector of random variables, and
    let $A$ be an $n × n$ symmetric matrix.  If $\E X = μ$ and $\var(X) =
    Σ$, then
    \begin{equation}
      \E X'AX = \tr(A Σ) + μ'Aμ.
    \end{equation}
  \end{thm}

  The proof uses three concepts that show up often: a scalar equals
  its own trace, linearity of the expectation, and a property of the
  trace: $\tr(XY) = \tr(YX)$ as long as both are conformable (the last
  property is difficult to see, so write out the matrix operations to
  convince yourself it is true).
  \begin{align*}
    \E(X'AX) &= \tr(\E(X'AX)) \\
    &= \E(\tr(X'AX)) \\
    &= \E(\tr(AXX')) \\
    &= \tr(\E(AXX')) \\
    &= \tr(A \E(XX')) \\
    &= \tr(A (\var(X) + μμ')) \\
    &= \tr(AΣ) + \tr(A μμ') \\
    &= \tr(AΣ) + μ'Aμ.
  \end{align*}

\item Similar results obviously exist for other moments, but the
  algebra isn't quite as nice (and doesn't illuminate any key
  technique other than raw tenacity).  Here's an example for the
  variance under a few simplifying assumptions (this particular
  statement comes from \citealp{SL03})
  \begin{thm}
    let $X₁,...,X_n$ be independent random variables with means
    $θ₁,...,θ_n$, and the same 2nd, 3rd and 4th central moments $μ₂$,
    $μ₃$, $μ₄$.  If $A$ is an $n × n$ symmetric matrix and $a$ is a
    column vector of the diagonal elements of $A$, then
    \begin{equation*}
      \var(X'AX) = 
      (μ₄ - 3 μ₂²)a'a + 2 μ₂² \tr(A²) + 4 μ₂ θ'A² θ + 4 μ₃ θ' A a.
    \end{equation*}
  \end{thm}
  To prove the result, take
  \begin{equation*}
    X'A X = ∑_{ij} ((X_i - θ_i) + θ_i) ((X_j - θ_j) + θ_j) A_{ij},
  \end{equation*}
  multiply out and take the variance.

\end{itemize}

\section{Algebraic and geometric properties of linear regression}

\begin{itemize}

\item Before worrying too much about statistical issues, we'll discuss
  a few issues that come up just because of the math involved in the
  linear regression estimator.  These issues have no statistical
  content, but are numeric identities (almost, that's not a fantastic
  way to say it).

\item Draw pictures in $\RR^n$ for $n = 3$.  A good set of values is
  \begin{align*}
    Y &= \begin{pmatrix}1.5 \\ 0.5 \\ 3\end{pmatrix}&
    X &=
    \begin{pmatrix}
      1  &  3 \\
      1  &  1 \\
      1  &  2
    \end{pmatrix}
  \end{align*}

\item The same pictures can be useful to understand the operations
  when adding a variable.  Suppose you regress $Y$ on $X₁$ and get an
  estimate $\βt₁$, then decide to regress $Y$ on $X₁$ and $X₂$, giving
  $\βh₁$ and $\βh₂$.  There is a relationship between these estimates
  \begin{thm}[Frisch-Waugh-Lovel Theorem]
    Let $e = Y - X₁\βt₁$ and define
    \begin{equation*}
      Z = (I - X₁(X₁'X₁)X₁')X₂,
    \end{equation*}
    (i.e. $Z$ consists of the residuals after regressing each column
    of $X₂$ on $X₁$.  Then $\βh₂ = (Z'Z)^{-1}Z'e$.
  \end{thm}
  Similar formulae exist for getting $\βh₁$ from $\βt₁$

  [Need to scan pictures and add them]

  This result is less useful than it used to be, because computers are
  far more powerful.  But the intuition is still just as useful—this
  is what we mean by ``partial out''—and the result itself can still
  be useful when dealing with very large data sets.

\item We may be interested in knowing how far $Y$ is from $\Yh$; a
  good comparison is how far relative to the distance between $Y$ and
  $ι \Yb$.  This is what the centered $R$-squared measures:
  \begin{equation*}
    R² = 1 - \frac{ \lVert Y - \Yh \rVert₂²}{\lVert Y - ι \Yb \rVert²}.
  \end{equation*}
  As long as the model contains a constant, this will be between zero
  and one.

  You should realize that $R²$ is almost useless as a guide to whether
  or not your model is ``good.''  There are a few specific
  applications (mostly forecasting) where I was somewhat interested in
  the model's $R²$, but that's it (knowing that it's high can
  sometimes reassure you that there's no need to look for a model with
  more predictive power).

\end{itemize}

\section{Gaussian random variables}

\begin{itemize}

\item The \emph{Normal distribution} is particularly important when
  studying linear regression.  To the extent that we have finite
  sample results, many will hold only if the errors are Normal.
  Moreover, whenever we use the Central Limit Theorem, we get an
  approximately normal random variable.

\item If $X$ is an $N(μ, Σ)$ random vector, its density is
  \begin{equation*}
    f(x) = \frac{1}{\sqrt{2ⁿ πⁿ \det(Σ)}} exp(-1/2(x - μ)'Σ^{-1}(x- μ)).
  \end{equation*}
  The parameters $μ$ and $Σ$ can be shown to be the mean and variance
  of the r.v. and completely determine its distribution.

  [you should draw the shape of the density and some elliptical
  contour plots]

  If $X$ is a $k$-dimensional multivariate normal with mean $μ$
  and variance $Σ$, $A X + b$ is also multivariate normal for any
  constant $n × k$ matrix $A$ and $n$-vector $b$.

  If $X$ is multivariate normal with mean $μ$ and variance $Σ$,
  and $Σ^{1/2}$ is a symmetric matrix such that $Σ^{1/2} Σ^{1/2} = Σ$,
  then $(Σ^{1/2})^{-1} (X - μ)$ is multivariate standard normal.

\item Independence of normal random variables is especially easy: if
  $X$ and $Y$ are uncorrelated normal random vectors, then they are
  independent.  The result follows from simply multplying out the
  square terms in the density function and then factoring it.

  Marginal and conditional distributions are also especially easy to
  work with.  If 
  \begin{equation*}
    (X₁,X₂) ∼ N((μ₁,μ₂), Σ)
  \end{equation*}
  with
  \begin{equation*}
    Σ = \begin{pmatrix}
      Σ_{11} & Σ_{12} \\ Σ_{12}' & Σ_{22}
    \end{pmatrix}
  \end{equation*}
  then $X₁ ∼ N(μ₁, Σ_{11})$, $X₂ ∼ N(μ₂, Σ_{22})$, and
  \begin{equation*}    
    X₁ ∣ X₂ ∼ N(μ₁ + Σ_{12} Σ_{22}^{-1} (X₂ - μ₂),
               Σ_{11} - Σ_{12}'Σ_{22}^{-1} Σ_{12}).
  \end{equation*}
  Notice that the conditional mean of $X₁$ depends on $X₂$, but the
  conditional variance doesn't.

\item The fact that zero correlation implies independence also makes
  it easy to determine whether functions of normal r.v.s are
  independent.  Linear combinations and quadratic forms of normal
  r.v.s will come up often, so we'll single those results out.  For
  the next results, let $X$ be a $k$-dimensional standard normal
  random vector (in practice, we will often work with $Σ^{-1/2} X$).
  \begin{enumerate}
  \item If $A$ is a $k × j$ matrix and $B$ a $k × l$ matrix such that
    $A'B = 0$, then $A'X$ and $B'X$ are independent.
  \item If $P$ is a $k × k$ idempotent matrix and $PB = 0$ then $X'PX$
    and $B'X$ are independent (an idempotent matrix is a square and
    symmetric matrix that satisfies $P = PP$).
  \item If $P$ and $Q$ are idempotent and $PQ = 0$ then $X'PX$ and
    $X'QX$ are independent.
  \end{enumerate}
  
\item Idempotent matrices come up often, as they represent projections
  in $\RRⁿ$.  If $z$ is a point in $\RRⁿ$ and $x₁,...,x_k$ is a set of
  vectors in $\RRⁿ$, we may want to project $z$ into the space spanned
  by $x₁,...,x_k$.  This amounts to finding a linear operator $P$ so
  that $P z$ is a linear combination of the $x_i$'s and $P z$ is as
  close as possible to $z$ subject to the first constraint.

  We'll worry about making sure that $Pz$ is a linear combination of
  the $x_i$'s later—that's going to be the focus of linear regression.
  But to ensure that $P z$ is as close as possible to $z$, we just
  need to have $z - Pz$ and $Pz$ perpendicular, so $(z - Pz)' Pz = 0$.
  But this requires that $z'(I - P)'P z = 0$ for any $z$, so $(I -
  P)'P = 0$, which amounts to our condition that $P = PP$.

  An idempotent matrix, $P$, has the property that all of its
  eigenvalues must be zero or one, so $\rank(P) = \tr(P)$.

\item Two statistics that are particularly relevant to the normal
  distribution are the sample mean, $\Xb = (1/n) ∑_{i=1}^n X_i$, and
  the sample variance, $S² = (1/(n-1)) ∑_{i=1}^n (X_i - \Xb)²$.  Note
  that we can write $\Xb$ as a linear combination of the $X_i$'s:
  \begin{equation*}
    \Xb = (1/n,...,1/n) · (X₁,...,X_n)' = (1/n) ι'X,
  \end{equation*}
  where $ι$ is a vector of $n$ ones, and
  \begin{equation*}
    S² = (1/(n-1)) (X - ι \Xb)'(X - ι \Xb) = (1/(n-1)) X'(I - (1/n) ιι')X.
  \end{equation*}
  If $X ∼ N(μ, σ² I)$ then we have $\Xb ∼ N(μ, (σ²/n) I)$.  Also,
  since $(I - (1/n) ιι')$ is idempotent, we know right away that $\Xb$
  and $S²$ are independent.

\item Some derived distributions are important

\item %
  \begin{defn}
    Let $X₁,...,X_k$ be independent standard normal.  The
    chi-squared distribution with $k$ degrees of freedom is the
    distribution of $∑_{i=1}^k X²_i$.
  \end{defn}

  \begin{thm}
    Let $X ∼ N(0, I_n)$ and let $P$ be a symmetric $n × n$ matrix.
    Then $X'PX$ is chi-squared with $k$ degrees of freedom iff $P$ is
    idempotent with rank $k$.
  \end{thm}

  \begin{thm}
    if $X$ and $Y$ are independent chi-squared with $df_X$ and $df_Y$
    degrees of freedom, then $X + Y$ is chi-square with $df_X + df_Y$
    degrees of freedom.
  \end{thm}

  If $X ∼ N(μ, σ² I)$ then $S²$ is chi-square with $n-1$ degrees of
  freedom.

\item %
  \begin{defn}
    If $X$ is a standard normal and $Y$ is a chi-squared with $n$ dof
    and $X$ and $Y$ are independent, then $X / \sqrt{Y/n}$ is a $t(n)$
    random variable.
  \end{defn}
  From the quadratic form results, this is going to come up when $Z$
  is multivariate standard normal and $X = B'Z$, $Y = Z'PZ$, and $B'P
  = 0$.

\item %
  \begin{defn}
    If $X$ and $Y$ are independent chi-squared with $df_X$ and $df_Y$
    degrees of freedom, then $\frac{X/df_X}{Y/df_Y}$ is an $F(df_X,
    df_Y)$ random variable.
  \end{defn}

  Expect this to come up when $Z$ is multivariate standard normal and
  $X = Z'QZ$, $Y = Z'PZ$ and $Q$ and $P$ are idempotent and orthogonal.

\end{itemize}

\section{Statistical aspects of linear regression}

\begin{itemize}
\item We typically will invoke some of the following assumptions:
  \begin{enumerate}
  \item $\E(\ep_i ∣ X) = 0$ for all $i$
  \item $\var(\ep_i ∣ X) = σ²$
  \item $\cov(\ep_i, \ep_j ∣ X) = 0$ for $i ≠ j$
  \item $\ep_i ∼ \iid(0,σ²)$ given $X$
  \item $\ep_i ∼ \iid\; N(0,σ²)$ given $X$
  \end{enumerate}
  note that 5 implies 4 which implies 1, 2, and 3.

\item To derive OLS as the MLE, we assume that $\ep ∼ N(0,σ² I)$.
  The mathematical derivation is presented in earlier sections, so
  we'll discuss some of the intuition now.

  This part is pretty terrible and it needs to be replaced with the
  actual drawings:
  \begin{itemize}
  \item draw plane for $n = 3$ and $k = 1$
  \item draw spheres for the different epsilons (sphere here is a
    consequence of normality and independence); each layer of the
    sphere is an iso-probability line
  \item draw a new picture to show estimating $Y$
    \begin{itemize}
    \item start with $Y$ and $X$ given
    \item choose some $β = B₁$
    \item draw different spheres for the ``epsilon'' corresponding to that mean
    \item draw a second picutre corresponding to $β = B₂$
    \item We can judge which of the values of $β$ is more plausible by
      looking at the density function, and choosing the one that's
      larger (which is just MLE); it should be clear that the
      coefficients that maximize the likelihood are going to be the
      coefficients that minimize the distance between $Y$ and $\Yh$.
    \end{itemize}
  \end{itemize}

\item Define a few terms: the OLS estimator is given by
  \begin{equation*}
    \βh = (X'X)^{-1}X'Y = \left(∑_i x_i x_i'\right) ∑_i x_i y_i;
  \end{equation*}
  The \emph{fitted values} are defined as $\Yh = X \βh$ and the OLS
  \emph{residuals} are defined as $\eph = Y - \Yh$.

\item Some of the finite sample properties are relatively easy to
  prove.  The first is unbiasedness; or really conditional
  unbiasedness.
  \begin{thm}
    Suppose that $Y = Xβ + \ep$ where $\E(\ep ∣ X) = 0$ and $X$ has
    full rank.  Then $\E(\βh ∣ X) = β$.
  \end{thm}
  This conditional result implies unconditional unbiasedness through
  the LIE: $\E \βh = \E \E(\βh ∣ X) = β$.  The proof is pretty
  straightforward:
  \begin{align*}
    \E(\βh ∣ X) &= \E((X'X)^{-1} X'Y ∣ X) \\
    &= (X'X)^{-1} X'\E(Y ∣ X) \\
    &= (X'X)^{-1} X'\E(Xβ + e ∣ X) \\
    &= (X'X)^{-1} X'X β \\
    &= β.
  \end{align*}
  One thing to notice is that this is a slightly stronger result than
  unbiasedness on its own: the expected value of the OLS coefficient
  estimator is $β$ \emph{regardless} of the value of $X$.

\item If we also assume that the errors are homoskedastic and
  uncorrelated, the same sort of approach gives us a formula for the
  variance of the OLS estimator.
  \begin{thm}
    Suppose that $Y = Xβ + \ep$ where $\E(\ep ∣ X) = 0$ and $X$ has
    full rank, and also assume that $\E(\ep \ep' ∣ X) = σ² I$ for some
    $σ²$.  Then $\var(\βh ∣ X) = σ²(X'X)^{-1}$.
  \end{thm}

  The proof is similar to before.  Notice that now the variance
  \emph{does} depend on the particular value of the $X$'s drawn;
  inspection will show you that the variance of the $i$th coefficient
  estimator increases with the variance of the $i$th regressor.

  Since $\βh - β = (X'X)^{-1}X'\ep$, we have
  \begin{align*}
    \var(\βh ∣ X) &= \E((\βh - β)(\βh-β)' ∣ X) \\
    &= \E((X'X)^{-1}X'\ep\ep'X(X'X)^{-1} ∣ X) \\
    &= (X'X)^{-1} X' \E(\ep\ep' ∣ X) X (X'X)^{-1} \\
    &= σ² (X'X)^{-1} X'X (X'X)^{-1} \\
    &= σ² (X'X)^{-1}
  \end{align*}

\item It's also worth thinking about optimality properties under only
  the exogeneity and heteroskedasticity assumtions; you've probably
  heard of the Gauss-Markov Theorem before.  Notice that OLS can be
  viewed as a weighted average: $\βh = (X'X) X'Y = ∑_{i=1}ⁿ w_i y_i$
  where $w_i$ is a $k$-vector, $w_i = (X'X)^{-1} x_i$.  The
  Gauss-Markov Theorem shows that OLS has the smallest variance of all
  of the unbiased linear estimators.
  \begin{thm}
    Assume that $Y = Xβ + \ep$ where $\ep ∼ (0, σ² I)$ given $X$.
    Then $\βh$ is the unique estimator with minimum variance (given
    $X$) among all linear, unbiased estimators (i.e. OLS is BLUE).
  \end{thm}

  Just as in our earlier proofs of optimaility, the trick here will be
  to show that any other linear, unbiased estimator can be written as
  $\βh$ plus some additional uncorrelated noise term.  Suppose that
  $V$ is a $k × n$ matrix s.t. $\E V'Y = β$ is unbiased for $β$, so
  $V'Y$ is a linear unbiased estimator.  We can write down immediately
  that
  \begin{equation*}
    V'Y = (X'X)^{-1}X'Y + [V'Y - (X'X)^{-1} X'Y],
  \end{equation*}
  and the proof then follows after showing that these two parts are
  uncorrelated.
  
  By construction, $V'Y = V'Xβ + V'e$, so
  \begin{equation*}
    β = \E(V'Y ∣ X) = \E(V'Xβ + V'e ∣ X) = V'X β + V'\E(e ∣ X) = V'Xβ.
  \end{equation*}
  This holds for any choice of $β$, so $V'X$ must be equal to $I$ and,
  as a result, $V'Y - β = V'e$.

  Now it is straightforward to calculate the covariance between
  $(X'X)^{-1}X'Y$ and $(V - (X'X)^{-1}X')Y$,
  \begin{align*}
    \cov((X'X)^{-1}X'Y, (V - (X'X)^{-1}X')Y ∣ X)
    &= \E((X'X)^{-1}X'e e'(V - X(X'X)^{-1}) ∣ X) \\
    &= σ² (X'X)^{-1}X'(V - X(X'X)^{-1}) \\
    &= σ² [(X'X)^{-1}X'V - (X'X)^{-1} X'X (X'X)^{-1}].
  \end{align*}
  Both equal $(X'X)^{-1}$, so the whole term is zero and we've shown
  that the covariance is zero, completing the proof.

\item We've proved this result conditional on $X$, but the
  unconditional extension is easy.  If $\var(\βh ∣ X) ≤ \var(V'Y ∣ X)$
  for all values of $X$, then $\E \var(\βh ∣ X) ≤ \E \var(V'Y ∣ X)$ as
  well.

\item The sampling distributions also matter.  Finite sample
  distributions are going to depend on the distribution of the error
  term, $\ep$.  $\βh$ is relatively easy to work with, since it equals
  $β + (X'X)^{-1} X'\ep$; if $\ep ∼ N(0, σ²)$ given $X$, clearly
  $\βh ∼ N(β, σ²(X'X)^{-1})$ given $X$.

  For $s²$, we can get results pretty quickly by relating it back to
  the quadratic form results. We have $s² = (n-k)^{-1} ∑_{i=1}ⁿ \eph²_i$
  which can also be written as
  \begin{align*}
    s² &= (n-k)^{-1} ∑_i (y_i - x_i'\βh)² \\
    &= (n-k)^{-1} (Y - X\βh)'(Y - X\βh) \\
    &= (n-k)^{-1} (Y - X(X'X)^{-1}X'Y)'(Y - X(X'X)^{-1}X'Y) \\
    &= (n-k)^{-1} Y'(I - X(X'X)^{-1}X')'(I - X(X'X)^{-1}X')Y \\
    &= (n-k)^{-1} Y'(I - X(X'X)^{-1}X')Y \\
    &= (n-k)^{-1} (Xβ + \ep)'(I - X(X'X)^{-1}X')(Xβ + \ep) \\
    &= (n-k)^{-1} \ep'(I - X(X'X)^{-1}X')\ep.
  \end{align*}
  Now $I - X(X'X)^{-1}X'$ is a projection matrix with rank $n-k$, so
  $s² ∼ (n-k)^{-1} χ²(n-k)$ given $X$.  Moreover, it is easy to verify
  that $s²$ and $\βh$ are independent given $X$.

\item Two unconditional results are more useful.  If we let $q_i$ be
  the $i,i$ element of $(X'X)^{-1}$ and look at the unconditional
  density of $(\βh_i - β_i)/\sqrt{q_i σ²}$, we see that
  \begin{align*}
    f_{(\βh_i - β_i)/\sqrt{q_i σ²}}(b)
    &= ∫ f_{(\βh_i - β_i)/\sqrt{q_i σ²}, q_i} (b, q) dq \\
    &= ∫ f_{(\βh_i - β_i)/\sqrt{q_i σ²}}(b ∣ q_i = q) f_{q_i}(q) dq \\
    &= ∫ φ(b) f_{q_i}(q) dq \\
    &= φ(b) ∫ f_{q_i}(q) dq \\
    &= φ(b).
  \end{align*}
  Similarly, replacing $σ²$ with $s²$ gives us a $t(n-k)$ distribution.

\end{itemize}

\section{Making predictions from regression models}

\subsection{motivation and setup}
\begin{itemize}
\item Say we've estimated a regression model.  What do we do with it?
\item Easiest possible use: make forecasts
\item Setup:
\begin{itemize}
\item have observations $(y_i, x_i)$ for $i = 1,...,n$
\item observe regressors for period $n+1$
\item want to predict $y_{n+1}$
\end{itemize}
\item model \[ y_i = x_i'β + ε_i \]
\item Estimated by OLS (or, maybe, GLS)
\end{itemize}

\subsection{natural forecast}

\begin{itemize}
\item The natural forecast for $y_{n+1}$ is $x_{n+1}'\βh$
\begin{itemize}
\item $x_{n+1}'β$ would be the forecast that minimizes expected
         squared error
\item $\βh$ is our best (minimum-variance) estimator of $β$
\item Expected value of $\hat y_{n+1}$ is $y_{n+1}$
\end{itemize}
\item How reliable is our forecast?
\begin{itemize}
\item more precisely, what is the variance of the forecast error?
\item Define the forecast error as 
  \[ e_{n+1} = y_{n+1} - \hat y_{n+1} = ε_{n+1} - x_{n+1}'(\βh - β) \]
\end{itemize}
\item Variance of forecast erro is going to reflect
\begin{itemize}
\item variance of $\βh$
\item variance of $ε_{n+1}$
\end{itemize}
\end{itemize}

\subsection{calculation of variance}

     We're going to assume that the errors are uncorrelated (like we
     have) and homoskedastic.
\begin{itemize}
\item conditionally heteroeskedastic errors can be dealt with by GLS
\item The usual calculation gives us
  \[ \var(e_{n+1} ∣ X, x_{n+1}) = \E(e_{n+1}² ∣ X_{n+1}) = \E(ε_{n+1}²
  ∣ X, x_{n+1}) + \E(((\βh - β)'x_{n+1})² ∣ X, x_{n+1})\]
\item the first term is just $σ²$
\item the second is 
  \[ x_{n+1}' \var(\βh ∣ X) x_{n+1} = σ² x_{n+1}'(X'X)^{-1}x_{n+1}\]
\item draw a scatterplot to illustrate why $\hat y$ will depend on
       the particular value of $x$.
\end{itemize}

\subsection{common use of this variance}

\begin{itemize}
\item \textbf{if the errors are normal}, we can construct a confidence
       interval for $y_{n+1}$ from this result
\item $\hat y_{n+1} ± t_{α/2} \sqrt{s² (1 + x_{n+1}'(X'X)^{-1}x_{n+1})}$
\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../estimation"
%%% End: 
