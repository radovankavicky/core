% Copyright © 2013, authors of the "Econometrics Core" textbook; a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Bootstrap}%
\addcontentsline{toc}{part}{Bootstrap}

\begin{itemize}
\item Basic premise: (and use the computer for this, obviously)
\begin{itemize}
\item say we have a statistic $\Xb$, $n = 100$, data are iid.
\begin{itemize}
\item n <- 100
\item X <- rlogis(n) + 2
\end{itemize}
\item LLN says that $\Xb$ is going to be close to $E X₁$ as long
       as $n$ is large (which it is)
\item CLT says that $\sqrt{n} (\Xb - \E X₁)$ is going to be
       approximately $N(0, \var X₁)$
\item All of this holds regardless of the distribution of the
       $X_i$-s.
\item Do some histograms for normal, uniform, and exponential distributions
\begin{itemize}
\item nsim = 2000
\item hist(replicate(nsim, mean(rnorm(n))), 100)
\item hist(replicate(nsim, mean(runif(n))), 100)
\item hist(replicate(nsim, mean(rexp(n))), 100)
\end{itemize}
\item So, if we want to know things about the distribution of $\Xb$,
  we have a few options
\begin{itemize}
\item use the approximate normal distribution
\item choose an arbitrary distribution that is convenient to work
         with, and calculate the \underline{exact} distribution of $\Xb$ if
         that were the correct distribution
\begin{itemize}
\item assuming the X's are normal would probably be the easiest here
\item could assume any distribution and use the computer to
           calculate the distribution of $\Xb$ numerically
\begin{description}
\item[.] plot(ecdf(replicate(20, mean(rexp(n)))))
\item[.] plot(ecdf(replicate(2000, mean(rexp(n)))))
\end{description}
\end{itemize}
\item choose an abitrary distribution that has some optimality
         properties. (ie is likely to be closest to the true unknown
         distribution).
\begin{itemize}
\item so, we observe X and don't know it's distribution.  What
           distribution is likely to be closest to the unknown F?
\item Imagine that we're estimating F.
\item $\Fh$ = empirical distribution of the X's
\begin{description}
\item[.] plot(ecdf(X))
\end{description}
\item So, at most basic level, we can use the computer to
           calculate the distribution, etc, of $\Xb$ under the
           assumption that the true distribution of the Xs is $\Fh$
\begin{description}
\item[.] sometimes we know more about the distribution, or don't
             believe in iid, or need to impose more constraints, and
             then we'll use different $\Fh$-s
\item[.] talk more about this later
\end{description}
\end{itemize}
\end{itemize}
\end{itemize}
\end{itemize}

\section{variance estimation}

    So, keep the same setup and suppose that we want to estimate the
    variance of $\Xb$

\subsection{bootstrap approach:}

\begin{itemize}
\item calculate the empirical distribution function, $\Fh$
\item draw a random sample of size 100 (n) from $\Fh$ with
         replacement and calculate the mean on that sample
\item draw many more random samples of size 100, and calculate the
         mean for each one, until you have $B$ total values of the mean
         (B should be large).
\begin{itemize}
\item call them $\Xb₁^*,..., \Xb_B^*$
\end{itemize}
\item calculate the variance of the bootstrap means.
\end{itemize}

\subsection{code}

\begin{itemize}
\item drawing a random sample from $\Fh$ is easy:
\begin{itemize}
\item sample(X, n, replace = TRUE)
\end{itemize}
\item calculating the mean is easy
\begin{itemize}
\item mean(sample(X, n, replace = TRUE))
\item do a few times so they see it's random
\end{itemize}
\item doing this B times is easy
\begin{itemize}
\item B <- 600
\item replicate(B, mean(sample(X, n, replace = TRUE)))
\end{itemize}
\item calculating the variance is easy
\begin{itemize}
\item var(replicate(B, mean(sample(X, n, replace = TRUE))))
\end{itemize}
\end{itemize}

\subsection{comments}

\begin{itemize}
\item obviously, this isn't a huge advantage over
  $\var(X) / n$
\item suppose that the statistic of interest is $(\Xb)^(1.2)$
\begin{itemize}
\item bootstrap approach doesn't change
\begin{itemize}
\item var(replicate(B, mean(sample(X, n, replace = TRUE))\^(1.2)))
\end{itemize}
\item calculating variance directly is now going to be hard
\item probably need to use delta method to get approximate variance
\end{itemize}
\item the bootstrap approximation is usually more accurate than
       other approximations (see \citealp{Hal91}, for details)
\item doesn't always work (need conditions like you need for delta
       method to work)
\end{itemize}

\section{hypothesis testing}

\begin{itemize}
\item might want to use bootstrap to test the null hypothesis that 
  $\E X₁ = 0$ (say) against two-sided alternative
\item so you want to use the bootstrap to approximate the distribution
  of $\Xb$ under the null hypothesis.
\end{itemize}

\subsection{naive previous approach (there is an error here!)}

\begin{itemize}
\item draw $B$ samples from $\Fh$ and calculate $\Xb^*,...,\Xb_B^*$
\begin{itemize}
\item the empirical distribution of the $\Xb_i^*$ approximates the
  unknown distribution of $\Xb$
\item plot(ecdf(replicate(B, mean(sample(X, n, replace=TRUE)))))
\end{itemize}
\item reject the null if $\Xb$ (the original mean) is in the tails (ie
  less than the $α/2$-percentile or greater than the
  $1-α/2$-percentile)
\begin{itemize}
\item quantile(replicate(B, mean(sample(X, n, replace=TRUE))), 
         c(0.025, 0.975))
\item mean(X)
\end{itemize}
\end{itemize}

\subsection{correct approach}

\begin{itemize}
\item we fail to reject, but the mean is basically 2, and the
       variance is small
\item look at empirical distribution of bootstrap means again
\begin{itemize}
\item plot(ecdf(replicate(B, mean(sample(X, n, replace=TRUE)))))
\end{itemize}
\item is this anything like the distribution under the null that
  $\E X₁ = 0$?
\begin{itemize}
\item of course not
\item the mean is nowhere near zero
\end{itemize}
\item so, the key idea is that we have to estimate the distribution
       of F while imposing the null hypothesis.
\begin{itemize}
\item here it's easy:
\begin{itemize}
\item don't sample from $X₁,...,X_n$
\item sample from $X₁ - \Xb, X₂ - \Xb,..., X_n - \Xb$
\begin{itemize}
\item plot(ecdf(X - mean(X)))
\end{itemize}
\item guaranteed to have mean zero, regardless of the true mean
           of the random variables.
\end{itemize}
\end{itemize}
\item computationally:
\begin{itemize}
\item to get the null distribution:
\begin{itemize}
\item plot(ecdf(replicate(B, mean(sample(X - mean(X), n, replace = TRUE)))))
\end{itemize}
\item now testing gives us:
\begin{itemize}
\item quantile(replicate(B, mean(sample(X - mean(X), n, replace = TRUE))), c(0.025, 0.975))
\end{itemize}
\item this basically rejects
\end{itemize}
\end{itemize}

\subsection{parting words (need to check this!)}

\begin{itemize}
\item main analogy: population is to sample as sample is to bootstrap
\begin{itemize}
\item we are creating a ``bootstrap world'' that we can analyze
         exactly
\begin{itemize}
\item we can see the sample; so if we take that as the ``true''
           distribution, we can calculate everything we want
\end{itemize}
\item this ``bootstrap world'' is an approximation to the real world.
\end{itemize}
\item for testing, usually works better if you ``studentize'' the
       statistic first
\begin{itemize}
\item sample from $X - \Xb / \σh$ instead of $X - \Xb$
\item compare to $\Xb / \σh$
\end{itemize}
\item again, doesn't always work
\begin{itemize}
\item rough rule of thumb: works when the delta-method does
\begin{itemize}
\item normal statistics, smooth transformations
\end{itemize}
\item when it doesn't work, there are other computationally
         intensive techniques that can
\begin{itemize}
\item subsampling
\end{itemize}
\end{itemize}
\item There are lots of issues we don't touch on
\begin{itemize}
\item bootstrap with heterogeneous or dependent data
\item \citet{DH97} is a good resource, and has an R package to do a lot
  of bootstraps.
\end{itemize}
\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../asymptotics"
%%% End:
