% Copyright (c) 2013, authors of "Core Econometrics;" a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.
\chapter{Bootstrap}
\tryinput{asymptotics/bootstrap_macros.tex}
\providecommand{\bootquantiles}{?}
\providecommand{\bootleft}{?}
\providecommand{\bootright}{?}
\providecommand{\medv}{?}
\providecommand{\powv}{?}
\providecommand{\ymedian}{?}
\providecommand{\bootinference}{?}

\section{Introduction to bootstrap material}

\begin{itemize}[leftmargin=0pt]

\item There are two approaches one could take, just based on what
  we've done so far.  One is to use the asymptotic distribution (i.e.,
  assume that the OLS estimator is normal).  The second approach is to
  assume a particular finite-sample distribution for the errors, and
  then work out the finite-sample distribution of the OLS estimator
  under that error structure.  If $n$ is large, this will be more or
  less the same as the asymptotic distribution, but there may be
  differences.  This is the approach we advocated when we suggested
  using $t$ or $F$ critical values instead of normal or chi-squared.

  Assuming the errors are normal makes it easy to get explicit numbers
  for the critical values, but it's not necessary.  We could assume
  that the errors are $\uniform(-1,1)$ and generate critical values by
  simulation.

  Here are some examples.\footnote{This section's analysis is done in
    R \citep{R} and uses the \emph{knitr} package \citep{Xie13} to
    embed the R code in this document.}

  First, define a simple function that will generate \lstinline|nsims|
  averages, each of \lstinline|nobs| draws from the distribution
  specified in \lstinline|rerror|, as follows
  \begin{lstlisting}[firstline=2,gobble=4]
    set.seed(76983)
    rstats <- function(rerror, nobs, stat = mean, nsims = 150000) {
      replicate(nsims, stat(rerror(nobs)))
    }
  \end{lstlisting}
  and a simple function to plot the histogram along with the
  best-fitting Gaussian curve
  \begin{lstlisting}[gobble=4]
    nicehist <- function(rerror, nobs,...) {
      stats <- rstats(rerror, nobs,...)
      statMean <- mean(stats)
      statSD <- sd(stats)
  
      xlim <- c(min(min(stats), qnorm(0.001, statMean, statSD)),
                max(max(stats), qnorm(0.999, statMean, statSD)))

      hist(stats, 50, col = "alice blue", freq = FALSE, xlab = "",
           main = paste(nobs, "observations"), xlim = xlim)
      curve(dnorm(x, mean = statMean, sd = statSD),
            from = xlim[1], to = xlim[2], lwd = 2, add = TRUE)
    }
  \end{lstlisting}

  We'll generate averages from the uniform distribution:
  \begin{lstlisting}[print=false]
    pdf(file = "asymptotics/bootstrap_fig1.pdf", width=6, height=2.5)
    par(mfrow = c(1,3))
    nicehist(runif, 2)
    nicehist(runif, 5)
    nicehist(runif, 50)
    dev.off()
  \end{lstlisting}
  \begin{figure*}
    \tryincludegraphics{asymptotics/bootstrap_fig1.pdf}
    \caption{Finite-sample distribution of the mean of uniform r.v.s.}
  \end{figure*}

  Observe that the normal approximation for the uniform mean is
  reasonable even when $n$ is as small as 5.

  Same plots for the exponential distribution:
  \begin{lstlisting}[print=false]
    pdf(file = "asymptotics/bootstrap_fig2.pdf", width=6, height=2.5)
    par(mfrow = c(1,3))
    nicehist(rexp, 2)
    nicehist(rexp, 15)
    nicehist(rexp, 50)
    dev.off()
  \end{lstlisting}
  \begin{figure*}
    \tryincludegraphics{asymptotics/bootstrap_fig2.pdf}
    \caption{Finite-sample distribution of the mean of exponential random
      variables.}
  \end{figure*}

  and we can see that the asymmetry has an effect even when $n$ is 50.

  Note that the histograms can be made arbitrarily close to the true
  \emph{finite sample} densities just by increasing the number of
  simulations.

\item Notice that, by and large, it does not matter if we know the
  exact distribution of the errors---using that information would not
  affect our critical values for the statistic.

\item Of course, another approach would be to use the \emph{best}
  finite sample distribution---one with potential optimality properties.
  If we knew the true distribution of the errors, then we would have a
  slightly more accurate distribution for the OLS estimators.  Since
  we don't know the true distribution, we can try estimating it.  This
  is the basic idea behind the bootstrap.

  Suppose we observe the data $y_1,...,y_n$ (univariate for now), and
  let $Y^*$ be a random variable s.t.
  \begin{equation*}
    Y^* =
    \begin{cases}
      y_1 & \text{with probability } 1/n \\
      y_2 & \text{with probability } 1/n \\
      \vdots \\
      y_n & \text{with probability } 1/n.
    \end{cases}
  \end{equation*}
  The corresponding distribution function is called the empirical cdf,
  \begin{equation*}
    \Fh(y) = (1/n) \sum_{i=1}^n \ind\{ Y_i \leq y \}.
  \end{equation*}
  Now we can estimate the sampling distribution of $\Yb$ by
  calculating it's finite sample distribution under the (incorrect)
  assumption that each $Y_i \sim \iid\ \Fh$.  We'll do that through
  simulation.

  \begin{lstlisting}[gobble=4]
    Y <- rexp(40)
    boots <- rstats(function(n) sample(Y, size = n, replace = TRUE), 40)
    trues <- rstats(rexp, 40)
  \end{lstlisting}

  We can mimic a testing scenario.  The endpoints for 5\% one-sided
  intervals based on the bootstrap distribution are given by
  \lstinline|quantile(boots, c(0.05, 0.95))| and equal \bootquantiles.
  They would have true coverage of \bootleft\ and \bootright\
  (\lstinline|mean(trues >= quantile(boots, 0.05))| and
  \lstinline|mean(trues >= quantile(boots, 0.05))| respectively)

\end{itemize}

\section{Estimating the variance}

\begin{itemize}[leftmargin=0pt]

\item One potential use of the bootstrap is to estimate the variance
  of an estimator.  To do this, calculate the empirical distribution
  function, $\Fh$, of the observed data, then repeat the following $B$
  times:
  \begin{itemize}
  \item Draw a random sample of size $n$ from $\Fh$ with replacement
    and estimate the statistic on that sample; call it $\thetah^*_b$.
  \end{itemize}
  This gives $\thetah^*_1,...,\thetah^*_B$ replications of the estimator.
  Calculate their variance, and this is an estimator of the variance
  of $\thetah$.

  We can illustrate this for the median with the command
  \begin{lstlisting}[firstline=2,gobble=6]
    boot2 <-
      var(replicate(1000, median(sample(Y, replace = TRUE))))
  \end{lstlisting}
  which equals \medv.

\item When the variance is easy to calculate, this isn't a big
  advantage.  But if we (say) want to calculate the variance of the
  median raised to the 1.2 power, our code becomes
  \begin{lstlisting}[firstline=2,gobble=6]
    boot3 <-
      var(replicate(1000, median(sample(Y, replace = TRUE))^(1.2)))
  \end{lstlisting}
  which is a trivial code change (the variance estimate is now \powv).
  Now, the bootstrap doesn't always work (you need conditions like you
  need for delta method to work).

\item Another advantage is that the bootstrap approximation is usually
  more accurate than other approximations (see \citealp{Hal91}, for
  details).

\end{itemize}

\section{Inference}

\begin{itemize}[leftmargin=0pt]

\item A complication shows up when you want to use the bootstrap for
  testing: resampling the data will typically not impose the null
  hypothesis, so you need to manually impose it.  Suppose you want to
  test the null hypotheses that $\E Y = 0$ so you use the bootstrap to
  generate a critical value for the median.  To do this, you need to
  resample from $Y_1 - m(Y),...,Y_n - m(Y)$.
  \begin{lstlisting}[firstline=2,gobble=6]
    boot4 <-
      replicate(1000, median(sample(Y - median(Y), replace = TRUE)))
  \end{lstlisting}
  Here the median is \ymedian\ and the 95\% bootstrap quantile is
  \bootinference, so we would reject the null that the median is zero.

\item This works better if you use studentized statistics.
  
\item The main thing to keep in mind is the analogy: population is to
  sample as sample is to bootstrap.  The bootstrap builds a
  ``bootstrap world'' that we can analyze exactly.  This ``bootstrap
  world'' is an approximation to the real world.

\end{itemize}

% Write macros used in this section
\begin{lstlisting}[print=false]
  texcommand <- function(macro, tex, fmt = "%s")
    sprintf("\\newcommand{\\%s}{%s}\n", macro, sprintf(fmt, tex))

  cat(file = "asymptotics/bootstrap_macros.tex",
      texcommand("bootquantiles",
                 paste(sprintf("%.2f", quantile(boots, c(0.05, 0.95))),
                 collapse = " and ")),
      texcommand("bootleft",  mean(trues >= quantile(boots, 0.05)), "%.2f"),
      texcommand("bootright", mean(trues <= quantile(boots, 0.95)), "%.2f"),
      texcommand("medv", boot2, "%.4f"),
      texcommand("powv", boot3, "%.4f"),
      texcommand("ymedian", median(Y), "%.2f"),
      texcommand("bootinference", quantile(abs(boot4), 0.95), "%.2f"))
\end{lstlisting}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../core_econometrics"
%%% End:
