% Copyright © 2013, authors of "Core Econometrics Notes;" a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Linear regression}%
\addcontentsline{toc}{part}{Linear regression}

\section{Asymptotic properties of OLS}

\begin{itemize}
\item Notice that the OLS estimator can be written as a function of
  averages:
  \begin{equation*}
    \βh = β + \big( (1/n) ∑_{i=1}^n x_i x_i' \big)^{-1}
    (1/n) ∑_{i=1}^n x_i \ep_i.
  \end{equation*}
  So it is easy to analyze the asymptotic behavior of $\βh$ whenever
  $(1/n) ∑_{i=1}^n x_i x_i'$ obeys an LLN and $(1/n) ∑_{i=1}^n x_i \ep_i$
  obeys an LLN or CLT.

  If we maintain the assumption that the observations are independent
  but not necessarily identically distributed, which is a common
  working assumption in cross-sectional economics, we can ensure
  convenient asymptotic properties with the following technical
  conditions:
  \begin{asmp}\label{xmoments}
    The regressors $\{X_i\}$ are independent with uniformly bounded
    fourth moments and the $\E x_i x_i'$ are uniformly positive
    definite.
  \end{asmp}
  Under Assumption~\ref{xmoments},
  \begin{equation*}
    (1/n) ∑_{i=1}^n x_i x_i' →^p Q
  \end{equation*}
  where $Q = \lim_{n→∞} (1/n) ∑_{i=1}^n \E x_i x_i'$.

  \begin{asmp}~\label{epmoments}
    The r.v.s $\{x_i \ep_i\}$ are independent and mean zero with
    uniformly bounded $2 + δ$ moments and uniformly positive definite
    variances.
  \end{asmp}
  Under Assumption~\ref{epmoments},
  \begin{equation*}
    (1/\sqrt{n}) ∑_{i=1}^n x_i \ep_i →^d N(0, V)
  \end{equation*}
  where $V = \lim_{n→∞} (1/n) ∑_{i=1}^n \E x_i x_i' \ep_i²$

\item Now, under Assumptions~\ref{xmoments} and~\ref{epmoments}, it is
  straightforward to show that the OLS estimator is consistent and
  asymptotically normal.
  \begin{defn}
    An estimator $\θh_n$ of the parameter $θ$ is \emph{consistent} if
    $\θh_n → θ$ in probability as $n → ∞$.
  \end{defn}
  Consistency can be viewed as the minimal property that an estimator
  should have: if you have arbitrarily many observations, the
  estimator should be arbitrarily close to the true
  parameter.\footnote{Note that econometricians will sometimes use
    nonstandard asymptotic theory to work with approximations that
    break consistency: see, e.g., the weak instruments literature.}

\item So, assume that Assumptions~\ref{xmoments} and~\ref{epmoments}
  hold.  Then
  \begin{equation*}
    \βh 
    = β + \big( (1/n) ∑_{i=1}^n x_i x_i' \big)^{-1}
       (1/n) ∑_{i=1}^n x_i \ep_i
    = β + \big( (1/n) ∑_{i=1}^n x_i x_i' \big)^{-1} o_p.
  \end{equation*}
  and, since $(1/n) ∑_{i=1}^n x_i x_i' →^p Q$ and $Q$ is invertible by
  assumption, we have $\βh = β + o_p$.

\item For asymptotic normality, we have
  \begin{equation*}
    \sqrt{n} (\βh - β)
    = \big( (1/n) ∑_{i=1}^n x_i x_i' \big)^{-1}
      (1/\sqrt{n}) ∑_{i=1}^n x_i \ep_i →^d Q^{-1} Z
  \end{equation*}
  where $Z ∼ N(0, V)$, so $\sqrt{n} (\βh - β) ∼^a N(0, Q^{-1} V
  Q^{-1})$, where ``$∼^a$'' designates asymptotic distribution.

\item This asymptotic distribution is different than the finite sample
  distribution.  Specifically, the variance is different.  In our
  finite sample results, we assumed conditional homoskedasticity;
  namely that $\var(\ep_i ∣ X) = σ²$ for all $i$.  Making that
  assumption here changes $V$:
  \begin{align*}
    V
    &= \lim_{n → ∞} (1/n) ∑_{i=1}^n \E x_i x_i' \ep_i² \\
    &= \lim_{n → ∞} (1/n) ∑_{i=1}^n \E \big( x_i x_i' \E(\ep_i² ∣ X) \big) \\
    &= \lim_{n → ∞} (1/n) ∑_{i=1}^n \E( x_i x_i' ) σ² \\
    &= σ² Q.
  \end{align*}
  Then we have $\sqrt{n} (\βh - β) ∼^a N(0, σ² Q^{-1})$, which is the
  limit of the finite sample distribution under homoskedasticity.  The
  proof that $(1/n) X'X → Q$ in probability has been done, so it
  remains to show that $s² → σ²$ in probability:
  \begin{align*}
    s² &= (1/(n-k)) ∑_i (y_i - x_i'\βh)² \\
    &= (1/(n-k)) ∑_i (\ep_i - x_i'(\βh - β))² \\
    &= (1/(n-k)) ∑_i (\ep_i² - 2 \ep_i x_i'(\βh-β) + (x_i'(\βh-β))²) \\
    &= (1/(n-k)) ∑_i \ep_i²
       - 2 (\βh - β)' (1/(n-k)) ∑_i  x_i \ep_i
       + (\βh - β)' (1/(n-k)) ∑_i x_i x_i' (\βh - β) \\
    &= (1/n) ∑_i \ep_i² + o_p(1) o_p(1) + o_p(1)'O_p(1)o_p(1) \\
    &= (1/n) ∑_i \ep_i² + o_p(1).
  \end{align*}
  And $(1/n) ∑_i \ep_i² → σ²$ through the LLN.

\end{itemize}

\section{Hypothesis testing}

\begin{itemize}

\item To be able to use this asymptotic approximation, we need to
  estimate the parameters $Q$ and $V$.  We already know an estimator
  for $Q$, simply $(1/n) ∑_{i=1}^n x_i x_i'$ which is consistent under
  Assumption~\ref{xmoments}.  But we don't observe $\ep_i$, so the
  obviously correct average $(1/n) ∑_{i=1}^n \ep_i² x_i x_i'$ is not
  an option.

  Of course, since we know $(1/n) ∑_{i=1}^n \ep_i² x_i x_i' →ᵖ V$
  under Assumption~\ref{epmoments}\footnote{Add the argument later.}
  it suffices to find a quantity that converges to $(1/n) ∑_{i=1}^n
  \ep_i² x_i x_i'$ in probability.  One option to replace $\ep_i$ with
  the OLS residual $\eph_i$.  This gives the estimator
  \begin{equation*}
    \Vh = (1/n) ∑_{i=1}^n \eph_i² x_i x_i',
  \end{equation*}
  which was first proposed in the econometrics literature by
  \citet{Whi80}.

\item Let's define $W_n = (1/n) ∑_{i=1}^n x_i x_i' \ep_i²$.  We can
  rewrite the difference between $W_n$ and $\Vh$ as
  \begin{align*}
    \Vh - W_n
    &= (1/n) ∑_{i=1}^n x_i x_i' (\eph²_i - \ep_i²) \\
    &= (1/n) ∑_{i=1}^n x_i x_i' (x_i'(\βh - β))²
    - (2/n) ∑_{i=1}^n  x_i x_i' \ep_i x_i'(\βh - β))
  \end{align*}

  Now, if we look at the $(j,k)$ element of the first matrix, we see
  \begin{equation*}
    (1/n) ∑_{i=1}^n x_{ij} x_{ik}(x_i'(\βh - β))² =
    (\βh - β)' \big((1/n) ∑_{i=1}^n x_i x_{ij} x_{ik} x_{i}'\big) (\βh- β).
  \end{equation*}
  Under usual conditions, the term inside the sum is going to converge
  to its average and each $\βh - β$ is going to converge to zero in
  probability.  So this term is $o_p$.

  If we look at the $(j,k)$ element of the second matrix, we see
  \begin{equation*}
    (1/n) ∑_{i=1}^n x_{ij} x_{ik}(\ep_i x_i'(\βh - β)) =
    \big((1/n) ∑_{i=1}^n \ep_i x_{ij} x_{ik} x_{i}'\big) (\βh- β).
  \end{equation*}
  Again, the term inside the sum converges to its average (zero, in
  this case) and $\βh - β$ converges in probability to zero.

  Consequently, $\Vh = W_n + o_p = V + o_p$.

\item We can test linear restrictions on a single coefficient with a
  modification of the \ttest.  Instead of normalizing with the $(j,j)$
  element of $n · s² (X'X)^{-1}$, however, we need to normalize with
  the $(j,j)$ element of $\Qh^{-1} \Vh \Qh^{-1}$ (define $\Ωh =
  \Qh^{-1} \Vh \Qh^{-1}$) .  So, under the null hypothesis $β_j = b₀$,
  we have
  \begin{equation*}
    \sqrt{n} (\βh_j - b₀) / \Ωh_{jj}^{1/2} =
    \sqrt{n} (\βh_j - b₀) / Ω_{jj}^{1/2} × (Ω_{jj} / \Ωh_{jj})^{1/2}.
  \end{equation*}
  Since $\sqrt{n} (\βh_j - b₀) / Ω_{jj}^{1/2} → N(0, 1)$ in
  distribution and $(Ω_{jj} / \Ωh_{jj})^{1/2} → 1$ in probability,
  \begin{equation*}
    \sqrt{n} (\βh_j - b₀) / \Ωh_{jj}^{1/2} →^d N(0,1)
  \end{equation*}

\item Under conditional homoskedasticity, we obviously can use $n · s²
  (X'X)^{-1}$ instead of $\Qh^{-1} \Vh \Qh^{-1}$.

\item Also, notice that the $t_{n-k}$ distribution converges to a
  standard normal as well.
  \begin{equation*}
    t_{n-k} =^d \frac{Z₀}{\sqrt{(1/(n-k)) ∑_{i=1}^{n-k} Z_i²}}
  \end{equation*}
  where the $Z_i$ are independent standard normal random variables.
  Since the denominator obeys an LLN and converges in probability to
  1, we have $t_{n-k} → N(0,1)$ in distribution.

  This means that we can compare the test statistic $\sqrt{n} (\βh_j -
  b₀) / \Ωh_{jj}^{1/2}$ to standard normal critical values or to
  critical values from the $t_{n-k}$ distribution.  These approaches
  will almost always give you the same answer, but the $t_{n-k}$
  critical values will always be slightly larger.

  If the two critical values don't give you the same answer, it is
  probably best to be conservative and use the larger critical values
  (which will lead you to not reject the null hypothesis).  The
  discrepency is telling you that you don't have enough observations
  to get reliable asymptotic approximations, so you should probably
  play it safe and not reject.

\item We can extend the \ftest\ in the same way to test several linear
  restrictions.  But an easier way to motivate the statistic is to
  observe that, if the null hypothesis $R β = q$ holds, where $R$ and
  $q$ are both known to the researcher,\footnote{Assume that $q$ has
  $p$ elements and that $R$ has full rank.} then
  \begin{equation*}
    \sqrt{n} (R \βh - q) →^d N(0, R Q^{-1} V Q^{-1} R').
  \end{equation*}
  Using the previous arguments, we have
  \begin{equation*}
    \sqrt{n} (R \Ωh R')^{-1/2} (R \βh - q) →^d N(0, I)
  \end{equation*}
  and so
  \begin{equation*}
    n (R \βh - q)' (R \Ωh R')^{-1} (R \βh - q) →^d χ²_p.
  \end{equation*}
  This is called the Wald test.

  If we are willing to assume conditional homoskedasticity, then $\Ωh$
  becomes $n · s² (X'X)^{-1}$ and the Wald test statistic becomes a
  scaled \ftest.

\item Notice that we can make the same argumet as above: since the
  $F_{p,n-k}$ converges to a $χ²_p$ distribution as $n-k → ∞$, we can
  use either $χ²$ or $F$ critical values for our test statistic.  Just
  like with the \ttest, the $F$ critical values will be somewhat more
  conservative (here it can actually make a noticable difference), so
  you should use them.

\item So far, we've discussed testing linear restrictions on the
  coefficients.  But we can also test nonlinear restrictions of the
  form $c(β) = q$, where $c$ is a function from $\RR^{k+1}$ to
  $\RR^p$.

  If we are willing to specify a distribution for the errors, we could
  crank through the algebra and derive the distribution of $c(\βh)$.

  But we can also use the delta method if $c$ is differentiable with
  nonzero continuous derivative at $β$.  Let $C(β) = (∂/∂β) c(β)$ and
  assume that $C(β)$ has full rank.  Then
  \begin{equation*}
    \sqrt{n} (c(\βh) - q) →^d N(0, C(β)' Ω C(β))
  \end{equation*}
  and, since $C(\βh) →^p C(β)$ as $n → ∞$ by continuity, we have
  \begin{equation*}
    \sqrt{n} (C(\βh)' \Ωh C(\βh))^{-1/2} (c(\βh) - q) →^d N(0, I)
  \end{equation*}
  and
  \begin{equation*}
    n (c(\βh) - q)' (C(\βh)' \Ωh C(\βh))^{-1} (c(\βh) - q) →^d χ²_p.
  \end{equation*}

  Again, $\Ωh$ can be estimated under homoskedasticity if the
  researcher believes that assumption is justified.

  If $C(β)$ does not have full rank, the same approach can work but it
  will be more awkward.

\item We could also take another route.  Instead of correcting the
  asymptotic distribution of the OLS estimator, we could account for
  heteroskedasticity in estimating $β$.  This would give us a feasible
  version of the GLS estimator discussed earlier.

  So, suppose that $Y = X β + \ep$ and $\ep ∼ (0, Σ)$ given $X$ (where
  $Σ$ is diagonal for now for convenience).
  Further assume that $Σ$ has only a few unknown parameters,
  so
  \begin{equation*}
    \E(\ep²_i ∣ X) = z_i'α
  \end{equation*}
  where $z_i$ is some function of the regressors.  Since $\eph_i$ is
  consistent for $\ep_i$, we can regress $\eph²_i$ on $z_i$ to
  estimate $α$.

  The two-step process is
  \begin{enumerate}
  \item Regress $Y$ on $X$ with OLS to get $\eph$
  \item Regress $\eph$ on $Z$ to estimate $α$
  \item Regress $Y_i/w_i$ on $X_i/w_i$ to estimate $\βh_{FGLS}$ where
    $w_i = sqrt{z_i'\αh}$
  \end{enumerate}

  If we let $\Σh$ be the estimate of $Σ$ that uses $\αh$, consistency
  of $\αh$ ensures that $\Σh$ behaves asymptotically like $Σ$.  All of
  the asymptotic results from the GLS estimator then automatically
  apply to the Feasible GLS estimator.  Note that the finite sample
  properties of the GLS estimator (efficiency via Gauss-Markov, for
  example) do not necessarily hold.

\item We can also test for heteroskedasticity.  Note that $\βh$ is
  consistent even under heteroskedasticity, so $\eph_i → \ep_i$ for
  each $i$.  If $σ²_i$ is different for different individuals, it
  implies that $\ep²_i$ should be correlated with $x_i$, so we might
  regress $\ep²_i$ on $x_i$ and do an F-test for the significance of
  the overall regression?

  More formally, the null hypothesis of homoskedasticity can be
  expressed as
  \begin{equation*}
    H₀: \quad σ²_i = σ² \qquad i = 1,...,n.
  \end{equation*}
  If the null hypothesis is true, then $n s² (X'X)^{-1} → Ω$
  in probability as $n → ∞$,
  so
  \begin{equation*}
    (1/n) ∑_{i=1}^n (\eph²_i - s²) x_i x_i' →^p 0.
  \end{equation*}
  If the null is false, this convergence fails.  Since this looks like
  an average, we can apply the CLT to get critical values.

  Let $ψ_i$ denote the vector of unique elements of $x_ix_i'$, along
  with a constant term (if not in $x_i$), and let $p$ denote the
  length of $ψ_i$.  One can show under some assumptions that
  \begin{equation*}
    \sqrt{n} \big( ∑_{i=1}^n (\eph²_i - s²) ψ_i \big) →^d N(0, W)
  \end{equation*}
  where $W$ can be estimated.  But an easier way to test for
  heteroskedasticity is through an equivalent but simpler procedure
  (derived by \citealp{Whi80}):
  
  \begin{enumerate}
  \item Regress $y_i$ on $x_i$ and save the OLS residuals.
  \item Regress $\eph_i$ on $ψ_i$ and calculate the $R²$
    from this regression.
  \item $n R²$ is asymptotically $χ²_{p-1}$ under the null
    hypothesis.
  \end{enumerate}

\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../asymptotics"
%%% End: 
