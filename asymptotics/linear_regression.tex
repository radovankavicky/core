% Copyright © 2013, authors of "Core Econometrics Notes;" a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Linear regression}%
\addcontentsline{toc}{part}{Linear regression}

\begin{itemize}
\item we've gone back and forth on assuming normality
\item some of the properties of the OLS estimator require normality
\item others don't
\end{itemize}

\subsection{OLS properties without normality}

\begin{itemize}
\item algebraic properties from chapter 3
\item unbiasedness of the estimators
\item formula for variance of the estimators
\item gauss-markov theorem
\item unbiasedness of $s²$
\item zero correlation between errors and beta-hat
\end{itemize}

\subsection{Properties of OLS that use normality}

\begin{itemize}
\item distribution
\item independence of errors and beta-hat
\item hypothesis testing and confidence intervals
\item does this mean we can't do hypothesis testing if we don't have
       normality?
\begin{itemize}
\item of course not
\end{itemize}
\end{itemize}

\subsection{asymptotics}

\begin{itemize}
\item we could plug in any distribution for $ε$ and then use
       transformation formulas to get the density of $\βh$ given $X$.
\item each distribution for the errors would give us a different
       distribution for $\βh$.
\item \textbf{open up R}
\begin{itemize}
\item make a function 
         getbeta = function(errdist, nerrs, nbetas = 800) \{
         errs = matrix(errdist(nerrs * nbetas, nerrs, nbetas))
         apply(errs, 2, mean)
         \}
\item plot hist for different densities with nerrs = 50
\begin{itemize}
\item rt (errdist = function(n) rt(n, df = 10))
\item rexp
\item runif
\end{itemize}
\end{itemize}
\item notice that these are pretty close to each other
\item so we could do one of two things in practice
\begin{itemize}
\item try to guess what the distribution of the errors is and
         calculate the distribution of beta-hat from that
\item try to choose a distribution for beta-hat that's going to be
         close to \textbf{most} of the different dists you'll get for
         different error dist
\end{itemize}
\item big insight
\begin{itemize}
\item these procedures are the same
\item it turns out that the distribution of beta-hat \textbf{usually}
         behaves pretty similar to the one you get assuming normal
         errors, even if the errors are not normal
\end{itemize}
\item a side point, you'll never get approximate distributions
       that are good for all error dists.
\begin{itemize}
\item do example with rcauchy
\end{itemize}
\end{itemize}

\section{asymptotic behavior of beta-hat}

\begin{itemize}
\item assumptions
\begin{itemize}
\item $Y = Xβ + ε$
\item $ε$ is iid $(0, σ²)$ given $X$ and has finite $2+δ$ moments for
  some positive number $δ$
\item $x_i$ is independent across $i$
\item for each i, $x_i x_i'$ has uniformly finite variance
\item $\E x_i x_i' = Q_i$ which is uniformly finite and positive definite.
\end{itemize}
\item conclusion
\begin{itemize}
\item $\βh → β$ in probability
\item $s² → σ²$ in probability
\item $\sqrt{n}(\βh - β) → N(0, σ² Q^{-1})$ in distribution.
\end{itemize}
\end{itemize}

\subsection{discussion of assumption on X'X}

\begin{itemize}
\item Greene makes the assumption that $n^{-1} X'X → Q$ in
       probability where $Q$ is a positive definite matrix.
\item this condition is implied by the one we're using
\begin{itemize}
\item $n^{-1} X'X = n^{-1} ∑_i x_i x_i'$, so define $Q = \lim_{n → ∞}
  n^{-1} ∑_i Q_i$ and we get $n^{-1} ∑_i x_i x_i' - Q → 0$ in
  probability as a consequence of Chebychev's weak law of large
  numbers
\end{itemize}
\end{itemize}

\subsection{proof of consistency}

     We want to prove that $\βh → β$ in probablilty.  We
     can do that by plugging through the algebra
\begin{itemize}
\item going to show that $\βh - β → 0$ in probability
  \begin{align*}
    \βh - β &= (∑_i x_i x_i')^{-1} ∑_i x_i \ep_i \\
    &= (n^{-1} ∑_i x_i x_i')^{-1} n^{-1} ∑_i x_i ε_i \\
    &= \left[(n^{-1} ∑_i x_i x_i')^{-1} n^{-1}
       - Q^{-1}\right] ∑_i x_i ε_i + Q^{-1} n^{-1} ∑_i x_i ε_i \\
    &= O_p(1) n^{-1} ∑_i x_i ε_i
  \end{align*}
\item remember what $n^{-1} ∑_i x_i x_i' = O_p(1)$ means -- it
           means that for each element of the matrix (we'll call the ij
           element $(n^{-1} X'X)_{ij}$) for every positive $δ$,
           there exists a finite constant $C_δ$ and a number
           $N_δ$ such that
           \[ \Pr[|(n^{-1} X'X)_{ij} | > C_δ] < δ \] for all $n > N_δ$
\item we know from chebychev's inequality that
  \begin{align*}
    \Pr[|(n^{-1} X'X)_{ij} | > C]
    &< \E |(n^{-1} X'X)_{ij} |² / C² \\
    &≤ n^{-1} ∑_t \E |(x_{it} x_{jt}| / C²
  \end{align*}
\item this is finite because we are assuming finite means and variances.
\item so, for any $δ$, we just choose $N$ and $C$ big enough
           to ensure that the inequality holds for all $n > N$.
\item now, $\E(x_i ε_i) = \E \E(ε_i ∣ x_i) = 0$
\item also,
  \begin{align*}
    \var(x_i ε_i) &= \E(x_i x_i' ε_i²) \\
    &= \E \E(x_i x_i' ε_i² ∣ x_i) \\
    &= σ² \E(x_i x_i') \\
    &= σ² Q_i 
  \end{align*}
\item so $n^{-1} ∑_i x_i ε_i$ satisfies Chebychev's LLN
         and converges to its mean, zero.
\item $\βh - β = O_p(1) o_p(1) = o_p(1)$ and converges to
       zero in probability.
\end{itemize}

\subsection{consistency of $s²$}

     we want to show that $s² → σ²$ in probability
\begin{itemize}
\item
  \begin{align*}
    s² &= \ov{n-k-1} ∑_i (y_i - x_i'\βh)² \\
    &= \ov{n-k-1} ∑_i (ε_i - x_i'(\βh-β))² \\
    &= \ov{n-k-1} ∑_i (ε_i² - 2 ε_i x_i'(\βh-β) + (x_i'(\βh-β))²) \\
    &= \ov{n-k-1} ∑_i ε_i²
       - 2 (\βh - β)' \ov{n-k-1} ∑_i  x_i ε_i
       + (\βh - β)' \ov{n-k-1} ∑_i x_i x_i' (\βh - β) \\
    &= \ov{n-k-1} n^{-1} ∑_i ε_i²
       + o_p(1) o_p(1) + o_p(1)'O_p(1)o_p(1) \\
    &= n^{-1} ∑_i ε_i² + o_p(1)
  \end{align*}
\item finally, $n^{-1} ∑_i ε_i²$ converges in probability to $σ²$ by
  Markov's lln.
\end{itemize}

\subsection{asymptotic normality of $\βh$}

     We'll do a similar argument for asymptotic normality
\begin{itemize}
\item
  \begin{align*}
    \sqrt{n}(\βh - β)
    &= (n^{-1} ∑_i x_i x_i')^{-1} n^{-1/2} ∑_i x_i ε_i \\
    &= ((n^{-1} ∑_i x_i x_i')^{-1} - Q^{-1}) n^{-1/2} ∑_i
       x_i ε_i + Q^{-1} n^{-1/2} ∑_i x_i ε_i
  \end{align*}
\item so, we're going to show two things
\begin{itemize}
\item $n^{-1/2} ∑_i x_i ε_i$ is asymptotically normal
\item $((n^{-1} ∑_i x_i x_i')^{-1} - Q^{-1}) → 0$ in
         probability (which we discussed earlier)
\end{itemize}
\item for normality
\begin{itemize}
\item we already showed that each $x_i ε_i$ has mean zero and variance
  $σ² Q_i$
\item we can appy the lindberg-feller CLT to show $n^{-1/2} ∑_i x_i
  ε_i$ converges in distribution to $N(0, Q)$
\end{itemize}
\item consequently, $\sqrt{n}(\βh - β) = o_p(1) + Q^{-1} ∑_i x_i ε_i$
  and converges in distribution to a normal with mean zero and
  variance $σ² Q^{-1}$ which is just what we wanted to show.
\end{itemize}


\section{Hypothesis testing in the limit}

\subsection{Testing linear hypotheses}

\paragraph{t-test}

\paragraph{justifying the t-test}
\begin{itemize}
\item what does the previous asymptotic result tell us about the
  t-statistic?
\item suppose we want to test the null hypothesis
  \[ β_j ≤ 1 \] so we construct the t-statistic
\item can we use it?
\item we know that, if the null is true,
  \[ \frac{β_j - 1}{\sqrt{s² q_i}} → N(0,1) \] in distribution
  (where $q_i$ is the $(i,i)$ element of $(X'X)^{-1}$)
\item we also know that $t_{n-k-1} → N(0,1)$ as $n → ∞$
\begin{itemize}
\item
  \[t_{n-k-1} = \frac{Z₀}{\sqrt{(n-k-1)^{-1} ∑_{i=1}^{n-k-1} Z_i²}}\]
  in distribution, where each of the $Z_i$ is an independent
  standard normal.
\item The sum in the denominator obeys a law of large numbers, so it
  converges to its mean, 1, in probability
\end{itemize}
\end{itemize}

\paragraph{interpretation/conclusion}
\begin{itemize}
\item We have two approaches for testing the hypothesis that $\βh_i =
  β_i^*$ (or has an inequality) based on asymptotic approximations
\begin{enumerate}
\item calculate $\frac{\βh_i - β_i}{\sqrt{s² q_i}}$ and compare to
  critical values from the normal distribution.
\item calculate and compare to critical values from the
  $t$-distribution.
\end{enumerate}
\item These will almost always give you the same answer.
  \begin{itemize}
  \item the critical values for the t-distribution will always be a
    little bit bigger
  \item this happens because we replace the scaled chi-squared
    distribution in the denominator with the constant 1
  \end{itemize}
\item If they don't give you the same answer, watch out.
  \begin{itemize}
  \item you should probably go with the $t$-distribution (since it's
    more conservative)
  \item this discrepency is telling you that you don't have enough
    observations to get reliable asymptotic approximations
  \item doesn't mean that you should believe the errors are normal,
    but it means that you should probably play it safe and not reject.
  \item If you run into this situation, you'll want to think about
    what you're doing a little more carefully.
  \end{itemize}
\end{itemize}

\paragraph{F test/Wald test}

\paragraph{introduction}
\begin{itemize}
\item Suppose we want to test the general restriction $Rβ = q$
\item If the errors are normal, we know that the F-test is valid
\item What is the asymptotic distribution of the F-test if the errors
  aren't normal?
\end{itemize}

\paragraph{asymptotic distribution}
We can rewrite the F-statistic as
\[ J^{-1} (R\βh - q)'[s² R(X'X)^{-1}R']^{-1}(R\βh - q) \]
\begin{itemize}
\item under the null, we know that $q = Rβ$, so we also know that
  $R(\βh - β) = R\βh - q$
\item we know from earlier that $\sqrt{n} R(\βh - β)$ converges in
  distribution to $N(0, σ² R Q^{-1} R')$
\item so, we know that \[\sqrt{n} (R\βh - q)' (σ² R Q^{-1} R')^{-1}
  \sqrt{n} (R\βh - q) \] converges in distribution to a chi-square
  with $J$ degrees of freedom
\begin{itemize}
\item $\sqrt{n}(R\βh - q)$ is a $J$-vector and is asymptotically
  normal.
\item $\sqrt{n}(σ² R Q^{-1} R')^{-1/2} (R\βh - q)$ is an
  asymptotically standard normal $J$ vector.
\end{itemize}
\item we want to show that \[ (R\βh - q)'[s² R(X'X)^{-1}R']^{-1}(R\βh
  - q) = J^{-1} (R\βh - q)'[σ² RQ^{-1}R']^{-1}(R\βh - q) + o_p(1)\]
  \begin{itemize}
  \item this will show that $J F$ is asymptotically chi-square with
    $J$ degrees of freedom.
  \item multiply $(R\βh - q)$ by $\sqrt{n}$ and bring $n^{-1}$ next to
    $X'X$.
  \item we know that $s² → σ²$ and $n^{-1} X'X → Q$ in probability
  \item so, we get the convergence we need.
  \end{itemize}
\item So, we know that the F statistic is equal in distribution to a
  $J^{-1} χ_J²$ random variable as $n → ∞$.
\end{itemize}

\paragraph{conclusion/interpretation}

\paragraph{behavior of F distribution}

\begin{itemize}
\item have the same issue we saw with the t-distribution
\item the $F_{J, n-k-1}$ distribution is defined to be the same as the
  distribution of
  \[{J^{-1} ∑_{i=1}^J Z_i² \over (n-k-1)^{-1} ∑_j W_j²}\] where each
  $Z$ and $W$ is an independent standard normal.
\item as $n → ∞$, the denominator converges to 1 in probability
\item the numerator is unaffected and is a $χ²_J / J$ random variable.
\item so the F-distribution becomes more and more like the chi square
  as well.
\end{itemize}

\paragraph{conclusion}
\begin{itemize}
\item You can use either the $F_{J, n-k-1}$ or the Chi-square $J$ to
  construct a hypothesis test.
  \begin{itemize}
  \item calculate the F test statistic
  \item get the critical values from a table or the computer
  \item reject if test stat is greater than critical values
  \end{itemize}
\item Just like with t-test, using the finite sample test (F) is more
  conservative than the asymptotic test
  \begin{itemize}
  \item critical values are a little bigger
  \item this is a bigger deal when you're testing lots of restrictions
    simultaneously, so you are more likely to get disagreement than
    you did with the t-test
  \item use the F critical values!
  \end{itemize}
\end{itemize}

\subsection{Testing nonlinear hypotheses}
\paragraph{motivation}
What if we want to test the hypothesis that $c(β) = q$ where $c$ is a
function from $\RR^{k+1}$ to $\RR^J$?
\begin{itemize}
\item Do we know anything about the finite-sample distribution of
  $c(\βh)$ that we could use to construct a test statistic?
  \begin{itemize}
  \item as usual, if we specify a distribution for the errors, we
    could crank through the algebra and derive the distribution of
    $c(\βh)$
  \item won't give us a general procedure
  \item won't necessarily be easy to impose the null hypothesis.
  \end{itemize}
\item Maybe we can get a procedure that uses the asymptotic
  distribution of $c(\βh)$ under the null
\end{itemize}

\paragraph{initial asymptotics}
\begin{itemize}
\item suppose that $c$ is differentiable
\item we can use the delta-method to show that $\sqrt{n}(c(\βh) -
  c(β))$ is asymptotically normal
\item a first-order taylor expansion gives us
  \[ \sqrt{n} c(\βh) = \sqrt{n} c(β) + {\partial c(β) \over \partial
    β'} \sqrt{n} (\βh - β) + o_p(1) \] where $\partial c(β)
  \over \partial β'$ is a $J × k+1$ matrix of partial derivatives.
\item We know that \[{\partial c(β) \over \partial β'} \sqrt{n} (\βh -
  β)\] converges in distribution to a normal r.v. with mean zero and
  variance \[ \left({\partial c(β) \over \partial β'}\right) σ² Q^{-1}
  \left({\partial c(β) \over \partial β'}\right)'\]
\item Then $\sqrt{n} (c(\βh) - c(β))$ also converges to \[ N\left(0,
    \left({\partial c(β) \over \partial β'}\right) σ² Q^{-1}
    \left({\partial c(β) \over \partial β'}\right) \right)\]
\end{itemize}

\paragraph{hypothesis testing}
\begin{itemize}
\item Under the null hypothesis, we know that $c(β) = q$, so
  $\sqrt{n}(c(\βh) - q)$ has the same asymptotic distribution as
  $\sqrt{n}(c(\βh) - c(β))$
\item so we can do the same trick as before to get a chi-square test
  statistic
  \begin{itemize}
  \item we estimate the variance with \[ \left({\partial c(\βh)
        \over \partial \βh'}\right)s² (n^{-1}X'X)^{-1} \left({\partial
        c(\βh) \over \partial \βh'}\right)\]
    \begin{itemize}
    \item know $\βh → β$ so $\left({\partial c(\βh) \over \partial
          \βh'}\right) → \left({\partial c(β) \over \partial
          β'}\right)$ in probability
    \item know that $s² (n^{-1} X'X)^{-1} → σ²Q^{-1}$ in probability
    \end{itemize}
    \[{c(\βh) \over \partial \βh'}\Big)s² (n^{-1}X'X)^{-1}
    \left({\partial c(\βh) \over \partial \βh'}\right)\] - know $\βh →
    β$ so $\left({\partial c(\βh) \over \partial \βh'}\right) →
    \left({\partial c(β) \over \partial β'}\right)$ in probability -
    know that $s² (n^{-1} X'X)^{-1} → σ²Q^{-1}$ in probability
  \end{itemize}
  \[{c(\βh) \over \partial \βh'}\Big)s² (n^{-1}X'X)^{-1}
  \left({\partial c(\βh) \over \partial \βh'}\right)\] - know $\βh →
  β$ so $\left({\partial c(\βh) \over \partial \βh'}\right) →
  \left({\partial c(β) \over \partial β'}\right)$ in probability -
  know that $s² (n^{-1} X'X)^{-1} → σ²Q^{-1}$ in probability
\item Our test statistic is
  \[ (c(\βh) - q)' \left[\left({\partial c(\βh) \over \partial
        \βh'}\right)s² (n^{-1}X'X)^{-1} \left({\partial c(\βh)
        \over \partial \βh'}\right) \right]^{-1} (c(\βh) - q) \]
\item asymptotically chi-square with $J$ degrees of freedom
\item one additional point
\begin{itemize}
\item we need to verify that the population variance matrix is
  invertible
\item ie we need \[\left({\partial c(β) \over \partial β'}\right)\] to
  have full rank
\item in ``standard'' applications this is true, but it \textbf{may}
  not be true
\item can use a similar procedure but higher order expansion and
  usually won't wind up with a chi-square statistic.
\end{itemize}
\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../asymptotics"
%%% End: 
