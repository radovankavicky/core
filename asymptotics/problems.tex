% Copyright © 2013, authors of "Core Econometrics Notes;" a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Problems}%
\addcontentsline{toc}{part}{Problems}

\begin{enumerate}

\item  Suppose that $X₁,...,X_n ∼$ i.i.d. uniform($a$,$b$).
  \begin{enumerate}
  \item Derive the MLE of $\E X_i$ and prove that it is consistent.
  \item Please find the asymptotic distribution of the MLE of $\E
    X_i$.  You will need to rescale the estimator to find an
    asymptotic distribution.  Two hints: it is not asymptotically
    normal; and you may need to use the result $(1 + \tfrac{1}{n})^n →
    e$ as $n → ∞$ (``may'' meaning that my solution uses that result).
  \end{enumerate}

\item Prove the assertions of Example~\ref{convergenceInDist}.

\item Suppose $X₁,...,X_n ∼$ uniform$(-θ, θ)$.  Find
  the method-of-moments estimator of $θ$ based on
  \begin{equation}
    \label{eq:prob_1}
    n^{-1} ∑_{i=1}^n X_i².
  \end{equation}
  Is it consistent?  Asymptotically normal?

\item Let $\{e_t\}$ be a sequence of i.i.d. $(0,1)$ random variables
  and let $X_t = e_t + 0.5\, e_{t-1}$.  Such a process is called a
  Moving Average of order 1, abbreviated as MA(1).  What is the
  probability limit of $n^{-1} ∑_{t=1}^n X_t$ as $n → ∞$?

\item Suppose that $Z$ is a random variable with mean $μ$ and variance
  $σ$ and let $e₁,e₂,...$ be a sequence of i.i.d. random variables with
  mean zero and variance 1 that are independent of $Z$.  Show that the
  sequence $X_i = Z + e_i$ obeys the following weak law of large
  numbers: as $n → ∞$, $n^{-1} ∑_{i=1}^n ( X_i - \E(X_i ∣ Z) ) → 0$ in
  probability.  Note that $X₁,X₂,...$ is \emph{not} an i.i.d.\ sequence,
  but is an example of an \emph{exchangeable} sequence of random
  variables.

\item Suppose that $X_n →^p X$ and that $g$ is a continuous function.
  Prove that $g(X_n) →^p g(X)$.

\item Let $X₁,...,X_n ∼ N(θ, θ)$.  Derive the MLE of $θ$ and show that
  it is asymptotically normal.

\item Suppose that $X₁,...,X_n$ are an i.i.d. sample from the density
  $f(x) = 1/θ$, $0 ≤ x ≤ θ$.  Prove that $\max_i X_i$ is consistent
  for $θ$.

\item Suppose that $X₁,...,X_n ∼$ i.i.d. uniform($a$,$b$).
  \begin{enumerate}
  \item Show that $(\min_i X_i, \max_i X_i)$ is the maximum likelihood
    estimator of $(a,b)$.
  \item Prove that the MLE is consistent.
  \item Please find the asymptotic distribution of the MLE.  You will
    need to rescale the estimator to find an asymptotic distribution.
    Two hints: it is not asymptotically normal; and you may need to
    use the result $(1 + \tfrac{1}{n})^n → e$ as $n → ∞$.
  \item Use your answer to the previous question to construct a
    two-sided 90\% confidence interval for $b$.
  \item When we derived the asymptotic distribution earlier, we used
    only some aspects of the assumption that $X_i ∼$ uniform($a$,$b$).
    Can you show that $\max_i X_i$ and $\min_i X_i$ have the same
    asymptotic distribution that we derived above under weaker
    assumptions?
  \end{enumerate}

\end{enumerate}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../asymptotics"
%%% End: 
