% Copyright (c) 2013, authors of "Core Econometrics;" a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\chapter{Central Limit Theorem and convergence in distribution}

\section{Convergence in distribution}

\begin{itemize}

\item We've been talking about convergence to random variables or
  constants.  Sometimes that's too limiting a concept: we often have
  cases where there's not a particular \emph{variable} that our
  sequence converges to, but instead its behavior gets closer and
  closer to that of a different distribution.

  Fortunately, just such a concept already exists.

  \begin{defn}
    Let $\{X_n\}$ be a sequence of random variables with cdfs $F_n$.
    $X_n$ converges in distribution to the distribution $F$ if
    \begin{equation*}
      \lvert F_n(c) - F(c) \rvert \to 0
    \end{equation*}
    as $n \to \infty$ for all continuity points $c$ of $F$.
  \end{defn}

  The restriction to continuity points rules out uninteresting (from a
  probability or statistics standpoint) counterexamples where the the
  limit of $F_n$ is not technically a distribution function but
  trivial modifications are.

  \begin{ex}\label{convergenceInDist}
    Suppose that $\{X_n\}_n$ is a sequence of r.v.s that take on the
    values
    \begin{equation*}
      X_n =
      \begin{cases}
        -1/n & \text{with probability 1/4} \\
        1/n  & \text{with probability 1/4} \\
        1 - 1/n & \text{with probability 1/4} \\
        1 + 1/n & \text{with probability 1/4}
      \end{cases}
    \end{equation*}
    Then $X_n \to \bernoulli(1/2)$ in distribution but the limit of the
    cdfs of $X_n$ is not a proper distribution.
  \end{ex}

\item There are many ways to signify convergence in distribution.  In
  this text, we will typically write $X_n \to^d F$ or $X_n \to^d X$ where
  $X \sim F$.  The definition for random vectors is the same as for
  scalars as long as $c$ is understood to be a vector with the same
  length as $X_n$.

\item In later parts of the book, we'll use the following simple
  results.
  \begin{enumerate}
  \item If $Y_n \to^d c$ where $c$ is a constant, then $Y_n \to^p c$.
  \item If $X_n \to^d X$ and $Y_n \to^p c$, where $c$ is a constant, then
    $X_n Y_n \to^d c X$ and $X_n + Y_n \to^d X + c$.
  \item If $Y_n \to^d Y$ and $X_n - Y_n \to^p 0$ then $X_n \to^d Y$.
  \end{enumerate}
  The third is an obvious implication of the second, but it underpins
  many asymptotic approximation results: if $Y_n$ is relatively easy
  to analyze, we can find its limiting distribution and then show
  that $X_n$ must have the same limiting distribution as long as its
  deviations from $Y_n$ vanish in the limit.

  A more general result is the continuous mapping theorem,
  \begin{thm}[Continuous Mapping Theorem]
    If $X_n \to^d X$ and $g$ is a continuous function, then $g(X_n) \to^d
    g(X)$.
  \end{thm}
  
\end{itemize}

\section{Central limit theorem}

\begin{itemize}

\item The biggest application of convergence in distribution is the
  central limit theorem.  (I assume you're familiar).

  \begin{thm}[Lindeberg-L\'evy CLT]
    If $X_1,...,X_n$ are \iid\ $(\mu, \sigma^2)$ then
    \begin{equation*}
      \sqrt{n} (\Xb - \mu) \to^d N(0,\sigma^2).
    \end{equation*}
  \end{thm}

  \begin{thm}[Lindeberg-Feller CLT]
    Suppose that $X_1,...,X_n$ are independent random variables with
    $X_i \sim (0, \sigma^2_i)$ and $\sigmab^2 = (1/n) \sum_{i=1}^n \sigma^2_i$.  If $\sigmab^2 \to 1$
    and
    \begin{equation}\label{a2}
      (1/n) \sum_{i=1}^n \E( X_i^2 \ind\{ |X_i| > c \sqrt{n} \} ) \to 0
    \end{equation}
    as $n \to \infty$ for all $c > 0$, then $\sqrt{n} \Xb \to^d N(0, 1)$.
  \end{thm}
  Note that the condition $\sigmab^2 \to 1$ is typically just a normalization
  requirement that can be satisfied by dividing $\sqrt{n} \Xb$ by
  $\sigmab/n$ and the condition that each $X_i$ have mean zero can be
  trivially satisfied by subtracting off each observation's mean.

\item Equation~\eqref{a2} is known as \emph{Lindeberg's condition}
  and is a restriction on the tails of the observations.  A more
  natural condition that ensures~\eqref{a2} is for $\E |X_i|^{2 +
  \delta}$ to be bounded for each $i$.

\item Proofs use the moment generating function or (for the least
  restrictive conditions) the characteristic function.

  \begin{defn}
    The \emph{moment generating function} of a random vector $X$ is
    defined as the function $M_X(t) = \E e^{t'X}$ and the
    \emph{characteristic function} of $X$ is the function $\psi_X(t) = \E
    e^{it'X}$ ($i = \sqrt{-1}$).
  \end{defn}

  The moment generating function is somewhat easier to work with but is
  not always finite.  The characteristic function is defined so that
  it's always finite and exists, but is harder to work with because it
  uses complex analysis.

  These are useful because if two random variables have the same
  moment generating function (as long as it is finite) or
  characteristic function then they have the same distribution
  function.

\item There are multivariate extensions.  A simple one is for \iid\
  random vectors.
  \begin{thm}[Multivariate Lindeberg-L\'evy CLT]\label{multivariateCLT}
    If $X_1,...,X_n$ are \iid\ random vectors with mean $\mu$ and vcv $\Sigma$,
    then
    \begin{equation*}
      \sqrt{n}(\Xb - \mu) \to^d N(0,\Sigma).
    \end{equation*}
  \end{thm}

  The proof uses the Crame\`er-Wold device.
  \begin{thm}
    Let $X$ be a random $k$-vector.  If $a'X$ is normally distributed
    for every nonzero deterministic $k$-vector $a$, then $X$ is
    multivariate normal.
  \end{thm}

  \begin{proof}[Proof of Theorem~\ref{multivariateCLT}]
    Let $a$ be any nonzero vector with the same dimension as $X_i$.
    The sequence $a'X_1,...,a'X_n$ satisfies the univariate
    Lindeberg-L\'evy CLT and has limiting mean $a'\mu$ and variance
    $a'\Sigma a$.
\end{proof}

\item It's important to remember that if the individual $X_i$'s are
  joint normal, $\Xb$ is normal in finite samples.  This suggests that
  the CLT will be an accurate approximation when the $X_i$'s are
  nearly normally distributed, but may be inaccurate when they're far
  from normal.  The important features of the normal here are thin
  tails and symmetry.

\end{itemize}

\section{Delta method}

\begin{itemize}
\item Suppose we have have a sequence of random variables that obeys a
  central limit theorem (an estimator, for example).  We might want to
  know the distribution of smooth functions of that r.v.  The
  \emph{delta method} gives a simple approximation:

  \begin{thm}
    Let $\{Y_n\}$ be a sequence of random variables that satisfies
    \begin{equation*}
      \sqrt{n} (Y_n - y_0) \to^d N(0,\sigma^2)
    \end{equation*}
    and let $g$ be a measurable function that is differentiable with
    nonzero derivative at $y_0$.  Then
    \begin{equation*}
      \sqrt{n}[g(Y_n) - g(y_0)] \to^d N(0, \sigma^2[g'(y_0)]^2).
    \end{equation*}
  \end{thm}

  This result follows from a Taylor expansion of $g(Y_n)$ around
  $g(y_0)$.

  Note that the assumption of nonzero derivative at $y_0$ is only
  necessary to ensure that the asymptotic variance of $\sqrt{n}
  g(Y_n)$ is positive.  If that condition holds, we can still get a
  distributional result by taking a second order expansion (a
  higher-order expansion if the necessary).
\end{itemize}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../core_econometrics"
%%% End: 

%  LocalWords:  Lindeberg's Lindeberg CLT univariate
