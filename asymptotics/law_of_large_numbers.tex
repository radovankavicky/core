% Copyright © 2013, authors of "Core Econometrics Notes;" a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Law of Large Numbers and convergence of random variables}%
\addcontentsline{toc}{part}{Law of Large Numbers and convergence of
  random variables}

\section{Convergence in $L_p$}

\begin{itemize}
\item Averages have interesting properties as the number of
  observations increases.  For many DGPs, the sample average collapses
  to a point, the population mean, and the difference between the
  statistic and its limit has an approximately known distribution when
  it is rescaled to offset the degeneracy—the normal distribution.
  These results are known as the \emph{Law of Large Numbers} (LLN) and
  the \emph{Central Limit Theorem} (CLT) respectively but that
  terminology is a little misleading since ``the LLN'' refers to any
  one of a family of theorems that show the sample average converges
  to the population mean and same for ``the CLT.''

  For either of these results to make sense, we need to define what it
  means for a sequence of random variables $\{\Xb_n\}_{n=1}^∞$
  to converge (where $\Xb_n = (1/n) ∑_{i=1}^n X_i$ and
  $\{X_i\}_{i=1}^∞$ is another sequence of random variables).

\item The simplest version of convergence is to say that $\Xb_n$
  converges to some quantity $μ$ if $\E \Xb_n$ converges to $μ$ and
  $\var(\Xb_n)$ converges to zero as $n → ∞$.  These conditions are
  equivalent to $\E(\Xb_n - μ)²$ converging to zero as $n → ∞$, which
  is called ``convergence in mean square'' or ``convergence in $L₂$''
  for reasons that will become immediately clear.

\item Notice that $(\E(X - Y)ᵖ)^{1/p} ≡ \lVert X - Y \rVert_p$ can be
  viewed as a distance between the random varriables $X$ and $Y$.  The
  operation $\lVert · \rVert_p$ denotes the \emph{$L_p$-norm} of its
  argument for $p ≥ 1$.\footnote{Remember that a \emph{norm} is an
    abstraction of the idea of length.  A norm is a function $ρ$ (on
    a vector space) that satisfies
    \begin{enumerate}
    \item $ρ(a v) = |a| ρ(v)$ for any scalar $a$,
    \item $ρ(u + v) ≤ ρ(u) + ρ(v)$ (i.e. the triangle inequality),
    \item $ρ(0) = 0$.
    \end{enumerate}
    Clearly $ρ(·) = \lVert · \rVert$ satisfies conditions 1 and 3; it
    can be shown to satisfy the triangle inequality by applying
    Holder's inequality.}
  We can define \emph{convergence in $L_p$} in the obvious way.

  \begin{defn}
    Let $\{X_n\}$ be a sequence of random variables and let $X$ be
    another r.v.  $X_n$ converges to $X$ in $L_p$ as $n → ∞$ (or $X_n
    →^{L_p} X$) if $\lVert X_n - X \rVert_p → 0$ as $n → ∞$.
  \end{defn}
  The sequence $\{\lVert X_n - X \rVert_p\}_n$ is just a sequence of
  numbers, so convergence of this quantity to zero is conceptually
  straightforward.

  Convergence of random vectors holds if their individual elements
  coverge.

\item We can also show that if $X_n → X$ in $L_p$, then $X_n → X$ in
  $L_q$ for any $q ∈ (0, p)$.  To see this, we can write
  \begin{align*}
    \E | X_n - X |^p
    &= \E | X_n - X |^{q · (p/q)} \\
    &≥ (\E | X_n - X |^{q})^{p/q}
  \end{align*}
  since $p/q$ is a convex function (Jensen's inequality).  Raising
  both sides to the $1/p$ power completes the argument.

\item If $X₁, X₂,...$ are an \iid$(μ, σ²)$ sequence, we can easily
  verify that $\Xb_n → μ$ in $L₂$ as $n → ∞$.  First, we have
  \begin{equation*}
    \E \Xb_n = n^{-1} ∑_{i=1}^n \E X_i = μ.
  \end{equation*}
  Second,
  \begin{align*}
    \var \Xb &= (1/n²) \var\Big( ∑_{i=1}^n X_i \Big)\\
    &= (1/n²) ∑_{i=1}^n \var(X_i) + (2/n²) ∑_{i=2}^n ∑_{j=1}^{i-1} \cov(X_i, X_j) \\
    &= σ² / n.
  \end{align*}
  
  Then $\lVert \Xb - μ \rVert₂ = σ²/n → 0$ as $n → ∞$.
  
\end{itemize}

\section{Convergence in probability}

\begin{itemize}

\item Convergence in $L_p$ doesn't cover everything that we'd think of
  as ``convergence''
  \begin{ex}
    Let $W_n$ be a sequence of random variables s.t.
    \begin{equation*}
      W_n =
      \begin{cases}
        n & \text{probability } 1/n \\
        0 & \text{otherwise}.
      \end{cases}
    \end{equation*}
    Then $\Pr[W_n = 0] → 1$ as $n → ∞$ but the sequence does not
    converge in $L_1$ (and consequently in $L_p$ for $p > 1$):
    \begin{equation*}
      \E | W_n | = 0 · \Pr[W_n = 0] + n · \Pr[W_n = n] = 1
    \end{equation*}
    for all $n$.
  \end{ex}
  It's pretty obvious that the small divergent mass at $n$ breaks
  $L_p$ convergence.

\item This partially motivates the next version of convergence,
  convergence in probability.
  \begin{defn}
    A sequence of random variables $Y_n$ \emph{converges in
    probability} to $Y$ if
    \begin{equation*}
      \Pr[ | Y_n - Y | > ε] → 0
    \end{equation*}
    for any positive constant $ε$.
  \end{defn}
  This convergence can be written as
  \begin{itemize}
  \item $\plim Y_n = Y$
  \item $Y_n →ᵖ Y$
  \item $Y_n → Y$ i.p. (or ``in probability'').
  \end{itemize}

  Convergence in probability is implied by convergence in $L_p$
  as a consequence of Markov's/Chebychev's inequality.
  \begin{thm}[Markov's inequality]
    If $X$ is an r.v., $\Pr[ |X|ᵖ ≥ ε ] ≤ \E |X|ᵖ /εᵖ$.
  \end{thm}
  \begin{proof}
    Start with the obvious inequality
    \begin{equation*}
      \1\{ |X|^p ≥ ε \} ≤ |X|^p / ε^p
    \end{equation*}
    and then take the expectation of both sides.
  \end{proof}
  When $p = 2$ this is called ``Chebychev's inequality.''

\item We've seen that convergence in probability does not imply $L_p$
  convergence.  But the problem in our $W_n$ example was that some of
  the mass diverged.  It turns out that a sufficient condition for
  convergence in probability to imply convergence in $L_p$ is for the
  sequence of random variables to have uniformly bounded $p + δ$
  moments for $δ > 0$.

\item A useful result holds for continuous functions: suppose that
  $X_n$ converges to $c$ in probability and that $g$ is a function and
  is continuous at $c$.  Then $g(X_n)$ converges in probability to
  $g(c)$.

  To prove this, remember the definition of a continuous function.
  $g$ is continuous at $c$ if for every $ε >0$, there exists a $δ > 0$
  such that $|g(c + h) - g(c)| < ε$ for all $-δ ≤ h ≤ δ$.

  Now we want to prove that $\Pr[|g(X_n) - g(c)| > γ] → 0$ for all $γ
  > 0$.  So take $γ$ as given.  Then define $δ$ so that $|g(c + h) -
  g(c)| ≤ γ$ for all $-δ ≤ h ≤ δ$.

  Since $|X_n - c| < δ$ implies $|g(X_n) - g(c)| ≤ γ$, we have the
  inequality
  \begin{equation*}
    \Pr[ |X_n - c| < δ] ≤ \Pr[ |g(X_n) - g(c)| ≤ γ ]
  \end{equation*}
  and then
  \begin{equation*}
    \Pr[|g(X_n) - g(c)| > γ] ≤ \Pr[|X_n - c| ≥ δ] → 0
  \end{equation*}
  since $X_n →^p c$.

  To wrap it up, since $X_n$ converges to $c$, there exists an $N$
  such that $P[|X_n - c| ≥ δ] < β$ for all $n ≥ N$ and this last step
  completes the proof.

\item This result can be very useful: suppose you show that the
  sequence of matrices $W_n$ converges in probability to $σ$.  Then we
  know that $W_n^{-1}$ converges in probability to $σ^{-1}$ automatically.


\item As mentioned earlier, the Weak LLN is a family of results that
  can be basically stated: Suppose $\{X_n\}$ is a sequence of random
  variables with mean $μ$.  Under some conditions, $n^{-1} ∑_{i=1}^n
  X_i → μ$ in probability.

  These conditions trade off the possibility of large deviations of
  individual observations (i.e. fat tails) for dependence conditions.
  The weakest WLLN for \iid\ sequences is \emph{Khinchine's WLLN},
  which requires $\{X_i\}$ to be an \iid\ sequence where each $X_i$
  has mean $μ$ but is not required to have any higher moments.
  Obviously, in this case we can't use the proof based on convergence
  of the mean and variance from earlier.

  A variation that can be proven with the earlier argument is
  \emph{Chebychev's WLLN},\footnote{These names are coming from
  Appendix D of \citet{Gre12} and need to be verified and sourced
  better.} which requires that the $X_i$'s be independent but not
  necessarily identically distributed with means $μ_i$ and varainces
  $σ²_i$ that satisfy $(1/n²) ∑_i σ²_i → 0$ as $n → ∞$.\footnote{An
  example where this variance condition fails is if $σ²_i = i$; then
  \begin{align*}
    (1/n²) ∑_{i=1}^n σ²_i
    &= (1/n²) × n (n+1) / 2 \\
    &→ 1/2.
  \end{align*}}
  Under those assumptions $\plim \Xb - \μb = 0$ where $\μb = (1/n)
  ∑_{i=1}^n μ_i$.

\item When working with asymptotic arguments, there are often
  remainder terms that will converge to zero when we take limits.
  Since it is a little awkward to carry around those terms in full,
  there are notational shortcuts to make work easier.

  The first notation is for terms that vanish in the limit:
  \begin{defn}
    A sequence of random variables $X_n$ \emph{is $o_p$ as $n → ∞$} if
    $X_n →^p 0$, which is written as $X_n = o_p$.
  \end{defn}
  Note that this expression uses an equal sign, but the order matters.
  $X_n = o_p$ is not the same as $o_p = X_n$ (the second statement is
  more or less meaningless).  As an example, if $\Xb_n$ satisfies the
  WLLN and converges in probability to $μ$, we could write
  \begin{equation*}
    \Xb = μ + o_p
  \end{equation*}
  which can be useful in an intermediate step of an argument.

\item There is also notation for terms that do not vanish but are
  non-explosive in the limit:
  \begin{defn}
    A sequence of random variables $X_n$ \emph{is $O_p$ as $n → ∞$}
    if, for each $ε > 0$, there are constants $c$ and $N$ such that
    $\Pr[|X_n| > c] < ε$ for every $n > N$.  Again, this is written as
    $X_n = O_p$.
  \end{defn}
  Order matters here too.  $X_n = O_p$ means that the sequence $X_n$
  is ``bounded in probability''—even if it is not a convergent
  sequence, it is not divergent either.

  Note that $o_p = O_p$ (any sequence that converges in probability to
  zero is also asymptotically bounded in probability) and, more
  generally, if $X_n → μ$ i.p., then $X_n = O_p$.  Moreover, if $X_n =
  o_p$ and $Y_n = O_p$ then $X_n Y_n = o_p$ or, put much terser, $o_p
  O_p = o_p$.

\item We can also introduce variations.  If $m_n$ is a sequence that
  either diverges or converges to zero as $n → ∞$, we can write $X_n =
  o_p(m_n)$ if $X_n / m_n = o_p$ and $X_n = O_p(m_n)$ if $X_n / m_n =
  O_p$.

\end{itemize}

\section{Almost sure convergence}
\begin{itemize}
\item This is a somewhat unintuitive convergence concept that is stronger
        than convergence in probability.
\item Remember that a random variable is a function from a sample space
        to the real line;
\begin{enumerate}
\item so if we have the probability space $(Ω, \Fs, P)$ and $ω$ is an
  element of $Ω$, we know that $X_j(ω)$ is equal to some number.
\item $X_j(ω')$ might be equal to a different number.
\item So we can talk about different sequences of numbers that
  depend on $ω$
\begin{itemize}
\item $X₁(ω), X₂(ω), ..., X_k(ω)$.
\item $X₁(ω'), X₂(ω'), ..., X_k(ω')$.
\end{itemize}
\item For a given $ω$, the sequence of outcomes might converge, for a
  different $ω$, the sequence might not.
\item If the probability of drawing a value of $ω$ that gives a
           convergent sequence of real numbers is one, this means that
           the sequence of random variables also converges, and in a very
           ``strong'' way.
\end{enumerate}
\item Definition: The sequence of random variables $Y_n$ converges
        \emph{almost surely} to the random variable $Y$ if, for every
        $\ep > 0$, \[ \Pr[\lim_{n → ∞} | Y_n - Y | < \ep] = 1 \]
\item notation -- arrow with a.s. over it
\item almost sure convergence implies convergence in probability, but
        not vice versa.
\end{itemize}

\paragraph{Example of almost sure convergence (from SLLN)}
\begin{itemize}
\item Remember that the SLLN implies that $\Xb_n → μ$ a.s.
\item \texttt{library(lattice)}
\item \texttt{d <- do.call(rbind, lapply(1:30, function(i) \{}
\begin{itemize}
\item \texttt{xs <- runif(300, -1, 1)}
\item \texttt{cbind(i = i, n = 1:300, xbar = cumsum(xs)/(1:300))\}))}
\end{itemize}
\item Plotting individual sequences:
\begin{itemize}
\item \texttt{xyplot(xbar ~ n, data = as.data.frame(d), group = i, col = "black", type = "l", subset = i==1)}
\item \texttt{xyplot(xbar ~ n, data = as.data.frame(d), group = i, col = "black", type = "l", subset = i==2)}
\item \texttt{xyplot(xbar ~ n, data = as.data.frame(d), group = i, col = "black", type = "l", subset = i==3)}
\end{itemize}
\item Plotting them all:
\begin{itemize}
\item \texttt{xyplot(xbar ~ n, data = as.data.frame(d), group = i, col = rgb(0,0,0,.4), type = "l")}
\end{itemize}
\end{itemize}

\paragraph{Example of convergence i.p. but not a.s.}
\begin{itemize}
\item a sequence of r.v. that converges to zero in probability, but
        not almost surely.
\item Let $U$ be a r.v. from the uniform(0,1) distribution.
\begin{itemize}
\item Define the sequence: (draw this sequence)
\begin{itemize}
\item $X₁(U) = 1$
\item $X₂(U) = \1\{U ≤ 1/2\}$,
\item $X₃(U) = \1\{U > 1/2\}$,
\item $X₄(U) = \1\{U ≤ 1/3\}$
\item $X₅(U) = \1\{1/3 < U ≤ 2/3\}$
\item $X_6(U) = \1\{U > 2/3\}$
\item etc.
\end{itemize}
\item Obviously, each $X_n$ is either zero or one, and the
          probability it is one decreases with $n$.
\item But, for any value of $U$, $X_n(U)$ does not converge to zero
          ---there are always later values that are equal to one.
\end{itemize}
\item to plot them:
\begin{itemize}
\item function to generate $X₁,...,X_n$
\begin{verbatim}
Xseq <- function(U, n) {
  den <- 1
  num <- 1
  X <- rep(NA, n)
  for (i in 1:n) {
    X[i] <- (num-1)/den < U && U < = num/den
  }
  if (num==den) {
    num = 1
    den = den + 1
  } else {
    num = num + 1
  }
  X
}
\end{verbatim}
\end{itemize}
\end{itemize}

To plot them (repeat the plot several times)
\begin{itemize}
\item \texttt{plot(X(runif(1), 500), type = "l")}
\end{itemize}
For plotting several at once:
\begin{itemize}
\item \texttt{d <- do.call(rbind, lapply(1:100, function(i)) \{}
\begin{itemize}
\item \texttt{U <- runif(1)}
\item \texttt{cbind(i = i, U = U, X = Xseq(U, 500), n = 1:500)\})}
\end{itemize}
\item \texttt{d <- as.data.frame(d)}
\item Marginals (to see convergence in probability)
\begin{verbatim}
d$jitter <- runif(nrow(d), -5, 5)
xyplot(X ~ n + jitter, data = d, col = rgb(0,0,0,.2), subset = n %% 100 = 1)
\end{verbatim}
\item Sample paths
\begin{verbatim}
xyplot(X ~ n, d, group = i, col = rgb(0,0,0,.3), type = "l", subset = i < 10)
\end{verbatim}
\end{itemize}

\paragraph{Strong law of large numbers}
\begin{itemize}
\item Similar to weak law of large numbers, but with almost sure
       convergence.  I'll give an example
\item Kolmogorov's SLLN :: If $\{X_n\}$ is a sequence of iid random
  variables with mean $μ$ then $\Xb → μ$ almost surely.
\end{itemize}

\section{Consistency}
     Consistency is an asymptotic version of unbiasedness

\paragraph{Definition}
an estimator $\θh$ is a consistent estimator of the parameter $θ$ if
$\θh →^d{p} θ$ for any value of $θ$.

\paragraph{Basic result for consistency \textbf{:hw:}}
\begin{itemize}
\item Statement of the result
\begin{itemize}
\item Let $\θh_n$ be a sequence of estimators of $θ$ such
          that, for every $θ$,
\begin{enumerate}
\item $\var_θ(\θh_n) → 0$
\item $\E_θ (\θh_n) → 0$
\end{enumerate}
\item then $W_n$ is consistent for $θ$
\end{itemize}
\item Proof:
\begin{itemize}
\item Follows immediately from Cauchy-Schwarz
\item Want to show $\Pr[|\θh_n - θ| > ε] → 0$ for all $ε > 0$
  \begin{align*}
    \Pr[|\θh_n - θ| > ε] 
    &≤ \dfrac{\E(\θh_n - θ)²}{ε²} \\
    &= \dfrac{\E((\θh_n - E\θh_n)+ (\E\θh_n - θ))²}{ε²} \\
    &= \dfrac{\E(\θh_n - E\θh_n)²}{ε²}
      + 2 \dfrac{\E((\θh_n - E\θh_n)(E\θh_n - θ))}{ε²} + \dfrac{(E\θh_n - θ)²}{ε²} \\
    &= o(1) + 0 + o(1)
  \end{align*}
\end{itemize}
\end{itemize}

\paragraph{Consistency of MLE (\citealp[Theorem 10.1.6]{CB02} and
  \citealp[Section 14.4]{Gre12})}
\begin{itemize}
\item We're just going to mention the result
\begin{itemize}
\item you'll see it in more detail next semester
\item we won't mention the assumptions
\item But, this is a big justification for using MLE as a general method
\end{itemize}
\item Suppose that $X₁,...,X_n ∼ i.i.d.\ f(x; θ)$ where
        $f$ satisfies some technical regularity conditions and suppose
        $\θh_n$ is the MLE of $θ$.  Then $\θh_n →^p θ$ as $n → ∞$.
\begin{itemize}
\item Note, doesn't always hold, but usually holds.
\item A good homework exercise---look up the regularity conditions
          in \citet{CB02} or \citet{Gre12} and see if common families (gamma,
          exponential, uniform, etc) satisfy them; then try to prove consistency
          directly.
\item There are weaker conditions than listed in \citet{CB02} (in particular,
          we can relax i.i.d)
\end{itemize}
\end{itemize}

\paragraph{Consistency of method of moments estimators}

Suppose that $X₁,...,X_n ∼ f(·; θ)$ where $θ$ is a $k$-vector, $(\E
X_i,...,\E X_i^k) = g(θ₁,...,θ_k)$, $g^{-1}$ is continuous, and that
$\frac{1}{n} ∑_i X_i^k$ obeys a weak LLN.  Then the Method of Moments
estimator of $θ$ is consistent.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../asymptotics"
%%% End: 
