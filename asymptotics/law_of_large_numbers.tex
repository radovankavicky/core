% Copyright © 2013, authors of the "Econometrics Core" textbook; a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Law of large numbers}%
\addcontentsline{toc}{part}{Law of large numbers}

\section{Convergence of point estimator}

\subsection{Introduction to asymptotics}

\begin{itemize}
\item We've talked about point estimation
\begin{itemize}
\item For unbiased estimators, low variance is better
\item Can calculate the distribution via transformation theorem
\end{itemize}
\item Everything relies on knowing the finite sample distributions
\begin{itemize}
\item Not realistic
\end{itemize}
\item Moreover, there is a common structure in a lot of the estimators
       we've derived
\begin{itemize}
\item Method of moments estimator:
  $\θh = g^{-1}(\frac{1}{n}∑_i X_i,..., \frac{1}{n} ∑_i X_i^k)$
\begin{itemize}
\item estimator is a function of averages
\end{itemize}
\item MLE: often FOC are $\frac{1}{n} ∑_i \frac{d}{dθ} f(x_i; θ) = 0$
\begin{itemize}
\item estimator may inherit properties of averages
\end{itemize}
\end{itemize}
\item Suggests that we maybe should try to study behavior of averages
\begin{itemize}
\item If there is a systematic behavior exhibited by averages, then
         we don't necessarily need to know the finite sample
         distribution to have an idea about the behavior of these estimators.
\end{itemize}
\end{itemize}

\paragraph{Outline of results}
\begin{itemize}
\item Two crucial (families of) results:
\begin{itemize}
\item LLN
\begin{itemize}
\item Generic version: Under some conditions (weak dependence, few
           extreme observations), 
           $\frac{1}{n} ∑_{i=1}^n X_i → \frac{1}{n} ∑_{i=1}^n \E X_i$
           in some sense
\item Weak LLN (an example): If $X₁,X₂,... ∼ i.i.d. (μ, σ²)$
  then $\Xb → μ$ in probability.
\begin{itemize}
\item We'll discuss convergence in probability soon.
\end{itemize}
\item Strong LLN (an example): If $X₁,X₂,... ∼ i.i.d. (μ, σ²)$ then
  $\Xb → μ$ almost surely.
\begin{itemize}
\item We'll discuss almost sure convergence later.
\item Almost sure convergence implies convergence in
             probability, but not vice versa (hence the names)
\end{itemize}
\end{itemize}
\item CLT
\begin{itemize}
\item Generic version: Under some conditions (weak dependence, few
  extreme observations), $\frac{1}{\sqrt{n}} ∑_{i=1}^n X_i → N$ in
  some sense.
\item An example: Suppose that $X₁, X₂, ... ∼ i.i.d. (μ, σ²)$ then
  $\sqrt{n}(\Xb - μ) → N(0, σ²)$ in distribution.
\begin{itemize}
\item We'll discuss what ``convergence in distribution'' means
\end{itemize}
\end{itemize}
\item All of these assumptions can be weakened considerably.
\item That's what I mean by ``family of theorems''
\end{itemize}
\end{itemize}

\paragraph{Illustrative simulations for LLN}
\begin{itemize}
\item LLNs suggest that $\Xb$ should converge in some sense to the mean
\item Some R code illustrates that point
\begin{itemize}
\item \texttt{hist(replicate(1000, mean(runif(10, -1, 1))), 150, xlim = c(-1,1))}
\begin{itemize}
\item redo for 100, 1000, 10000
\end{itemize}
\item i.i.d. uniform clearly satisfies the conditions listed above
\item Other dists do too
\begin{itemize}
\item \texttt{hist(replicate(1000, mean(rexp(1000))), 150, xlim = c(-1,1))}
\item \texttt{hist(replicate(1000, mean(rt(1000, df = 5))), 150, xlim = c(-1,1))}
\end{itemize}
\item What happens for dis that doesn't?
\begin{itemize}
\item \texttt{hist(replicate(1000, mean(rt(1000, df=1))), 150, xlim = c(-1,1))}
\end{itemize}
\end{itemize}
\end{itemize}

\paragraph{Illustrative simulations for CLT}
\begin{itemize}
\item CLTs suggest that $\Xb$ should be asymp. normal, even though variance vanishes
\item Some R code illustrates that point
\begin{itemize}
\item \texttt{hist(replicate(10000, mean(runif(10, -1, 1))), 150)}
\begin{itemize}
\item redo for 100, 1000, 10000
\end{itemize}
\item \texttt{hist(replicate(10000, mean(rexp(1000))), 150)}
\end{itemize}
\item We can see where the $\sqrt{n}$ factor comes from:
\begin{itemize}
\item $\var(\sqrt{n} \Xb) = \frac{n}{n²} ∑_{i=1}^n \var(X_i) = σ²$
\item scaling by any less makes the variance still go to zero
\item scaling by any more and the variance blows up
\item \texttt{hist(replicate(10000, sqrt(100) * mean(rexp(100))), 150)}
\item \texttt{hist(replicate(10000, sqrt(1000) * mean(rexp(1000))), 150)}
\end{itemize}
\end{itemize}

\paragraph{Interpretation}
\begin{itemize}
\item As long as the DGP belongs to a large class (finite mean and
        variance, weak dependence), the behavior of $\Xb$ doesn't
        really depend on the details of the DGP
\end{itemize}

\subsection{$L_p$ convergence}
\begin{itemize}
\item A stronger form of convergence than convergence in probability,
        but easier to conceptualize.
\item Suppose we have $n$ iid $(μ, σ²)$ random variables,
  $X₁,...,X_n$.  Consider the sample mean, $\Xb ≡ n^{-1} ∑_{i=1}^n$.
  What happens when $n$ gets large?
\item what happens to the mean?  Well,
  \begin{align*}
    \E \Xb &= n^{-1} ∑_{i=1}^n \E X_i \\
    &= n^{-1} ∑ μ \\
    &= μ
  \end{align*}
  so the mean doesn't depend on $n$ at all.
\item What happens to the variance?
  \begin{align*}
    \var \Xb &= n^{-2} \var ∑_{i=1}^n X_i \\
    &= n^{-2} \E(∑_i X_i - μ)² \\
    &= n^{-2} \E\left(∑_{i,j} (X_i - μ)(X_j - μ)\right) \\
    &= n^{-2} \E ∑_{i=2}n (X_i - μ)² + 2 n^{-2} \E ∑_{i = 1}^n
            ∑_{j = 1}^{i-1} (X_i - μ) (X_j - μ) \\
    &= n^{-2} ∑_{i} \E(X_i - μ)² +  2 n^{-2} ∑_{i = 2}^n
            ∑_{j = 1}^{i-1} \E (X_i - μ) (X_j - μ) \\
    &= n^{-2}∑_i σ² \\
    &= σ²/n
  \end{align*}
\item What happens to the distribution? (draw it).
\item So, this seems like a reasonable form of convergence;
\begin{itemize}
\item Note that $\E \Xb → μ$ and $\var(\Xb) → 0$ iff
          $\E(\Xb - μ)² → 0$
\begin{itemize}
\item which happens iff $(\E(\Xb - μ)²)^{1/2} → 0$
\item This last formulation is convenient since
            $(\E(X-Y)²)^{1/2}$ defines a measure of the distance
            between the random variables $X$ and $Y$
\item Analagous to Euclidean distance in $\RR^k$.
\end{itemize}
\end{itemize}
\end{itemize}

\paragraph{Definition}
\begin{itemize}
\item Define the $L_p$ norm $(p ≥ 1)$ of a random variable $X$ as $\|X\|_p = (\E X^p)^{1/p}$
\item Let $Y₁, Y₂, ...$ be a sequence of random variables and let
        $Y$ be another r.v.  If $\|Y_n - Y\|_p → 0$ then $Y_n$
        converges in $L_p$ to $Z$.
\begin{itemize}
\item This can be written $Y_n →^{L_p} Y$
\end{itemize}
\item If $Y₁,Y₂,...$ is a sequence of random vectors and $Y$ is
        another random vector, $Y_n →^{L_p} Y$ if each element of
        $Y_n$ converges in $L_p$ to the corresponding element of $Y$.
\end{itemize}

\paragraph{Code to plot on the screen}
\begin{itemize}
\item Know from motivating example that $\Xb → μ$ in $L₂$ if
        $X$ is i.i.d with finite mean and variance.
\item Going to look at behavior of $\Xb$ when $X$ is uniform(-1,1), so $μ = 0$
\item To type into R for behavior of rvs:
\begin{itemize}
\item \texttt{hist(replicate(1000, mean(runif(n = 100, -1, -1))), 70, xlim = c(-1,1))}
\item \texttt{hist(replicate(1000, mean(runif(n = 500, -1, -1))), 70, xlim = c(-1,1))}
\item \texttt{hist(replicate(1000, mean(runif(n =1500, -1, -1))), 70, xlim = c(-1,1))}
\end{itemize}
\item Another view:
\begin{itemize}
\item \texttt{n <- seq(from = 10, to = 10000, length = 100)}
\item \texttt{xbar <- do.call(rbind, lapply(n, function(n) \{}
\begin{itemize}
\item \texttt{xbars <- replicate(1000, mean(runif(n, -1, -1)))}
\item \texttt{cbind(y = xbars, n = n)\}))}
\end{itemize}
\item \texttt{jitter <- runif(nrow(xbar), -10, 10)}
\item \texttt{plot(xbar[,1] ~ xbar[,2] + jitter, col = rgb(0,0,0,0.2))}
\end{itemize}
\item To approximate $\|\Xb - μ\|₂$:
\begin{itemize}
\item \texttt{L2 <- rep(NA, 100)}
\item \texttt{n <- seq(from = 10, to = 10000, length = 100)}
\item \texttt{for (i in 1:100)              L2[i] <- mean(replicate(1000, mean(runif(n = n[i], -1, -1)))\^2)\^0.5}
\item \texttt{plot(L2 ~ n, type = "l")}
\end{itemize}
\end{itemize}

\paragraph{Remarks}
\begin{itemize}
\item If $\{Y_n\}$ is a sequence of random variables that converges in
        $L_p$ to some constant $c$ then it converges in $L_r$ to $c$ for
        all $1 ≤ r < p$.
\item also called ``convergence in $p$ th mean'' (or, for $p = 2$,
        convergence in quadratic mean).
\item Convergence in $L_p$ requires that the $p$ th moment exists.
        This is not always guaranteed.
\item doesn't cover every case that we'd think of as ``convergence''
\begin{description}
\item[An example] Let $W_n = n$ with probability $1/n$ and $W_n = 0$
          with probability $1 - 1/n$.
\begin{itemize}
\item Draw pictures
\item Probability $W_n = 0$ converges to 1 as $n → ∞$.
\item But the expected value of $W_n = 1$, and the value of $\lVert
            W_n \rVert₁ = \E | W_n | = 0 P[W_n = 0] + n P[W_n = n] = 1$
            as well; so we don't have convergence in $L₁$.
\end{itemize}
\item[Code] We can see the failure of convergence pretty easily:
\begin{itemize}
\item Looking at the plots of the series
\begin{itemize}
\item \texttt{Wn <- do.call(rbind, lapply(n, function(n) \{}
\begin{itemize}
\item \texttt{Ws <- replicate(1000, n * rbinom(1, 1, 1/n))}
\item \texttt{cbind(y = Ws, n = n)\}))}
\end{itemize}
\item \texttt{jitter <- runif(nrow(Wn), -10, 10)}
\item \texttt{plot(Wn[,1] ~ Wn[,2] + jitter, col = rgb(0,0,0,0.2))}
\end{itemize}
\item Looking at $L₂$ convergence:
\begin{itemize}
\item \texttt{L2 <- rep(NA, 100)}
\item \texttt{n <- seq(from = 10, to = 10000, length = 100)}
\item \texttt{for (i in 1:100)                  L2[i] <- mean(replicate(10000, n * rbinom(1, 1, 1/n))\^2)\^0.5}
\item \texttt{plot(L2 ~ n, type = "l")}
\end{itemize}
\end{itemize}
\end{description}
\end{itemize}

\subsection{Convergence in probability}
\begin{itemize}
\item Convergence in probability (which we invoked for WLLN without
       defining it) is a weaker form of convergence than $L_p$
       convergence.
\item A sequence of random variables $Y_n$ converges in probability to
       $Y$ if \[ \Pr[\lvert Y_n - Y \rvert > \ep] → 0 \] for
       any positive constant $\ep$.
\item Note that convergence in probability is implied by convergence
       in $L_p$ (students should prove this for homework)
\item Take $W_n$ as before.  Does $W_n$ converge to zero in probability?
\begin{itemize}
\item $\Pr[\lvert W_n \rvert > \ep] ≤ \Pr[W_n = n] = 1/n → 0$ as $n → ∞$.
\end{itemize}
\end{itemize}

\paragraph{Discussion}
\begin{enumerate}
\item Notation. arrow and $\plim$.
\item if $X_n$ converges in $L_p$ for $p ≥ 1$, then $X_n$
          converges in probability.\sidenote{You should prove this on your own for homework.}
\item Obviously, the converse doesn't hold (we've given a
          counterexample).  However, you might have notice that the
          counterexample involves a small part of the pmf ``escaping'' off
          to infinity.  If we impose conditions that prevent that from
          happening, then convergence in probability implies convergence
          in $L_p$.  This additional condition is called ``uniform
          integrability'', and we won't cover it in the course.
\item To prove convergence in probability, we usually just prove that
          the mean and variance converge.  This in turn implies
          convergence in $L₂$.
\item A useful result holds for continuous functions: suppose that
          $X_n$ converges to $c$ in probability and that $g$ is a
          function and is continuous at $c$.  Then $g(X_n)$ converges in
          probability to $g(c)$.
\begin{itemize}
\item remember the definition of a continuous function.  $g$ is
  continuous at $c$ if for every $\ep >0$, there exists a $δ > 0$ such
  that $|g(c + h) - g(c)| < \ep$ for all $-δ ≤ h ≤ δ$.
\item We want to prove that $\Pr[|g(X_n) - g(c)| > γ] → 0$ for all
  $γ > 0$.
\item So, take $γ$ as given.  Then define $δ$ so that
  $|g(c + h) - g(c)| ≤ γ$ for all $-δ ≤ h ≤ δ$.
\begin{itemize}
\item so $|X_n - c| < δ$ implies $|g(X_n) - g(c)| ≤ γ$
\item $\Pr[|X_n - c| < δ] ≤ \Pr[|g(X_n) - g(c)| ≤ γ]$
\item $\Pr[|g(X_n) - g(c)| > γ] ≤ \Pr[|X_n - c| ≥ δ] → 0$ since 
  $X_n →^p c$.
\end{itemize}
\item Since $X_n$ converges to $c$, there exists an $N$
  such that $P[|X_n - c| ≥ δ] < β$ for all $n ≥ N$
\item this last step completes the proof.
\end{itemize}
\item Theorem D.14 in Greene gives some important implications of
            this result.
\begin{description}
\item[One important case] suppose you show that the sequence of
                 matrices $W_n$ converges in probability to $σ$.
                 How would you show that $W_n^{-1}$ converges in
                 probability to $σ^{-1}$
\end{description}
\item More generally, have $g(\plim X_n) = \plim g(X_n)$ even if
  $\plim X_n$ is stochastic.\sidenote{Try to prove this on your own
    for homework.}
\end{enumerate}

\paragraph{Weak law of large numbers}
\begin{itemize}
\item As I said earlier, the Weak LLN is a family of results that can
       be basically stated: Suppose $\{X_n\}$ is a sequence of random
       variables with mean $μ$.  Under some conditions, $n^{-1}
       ∑_{i=1}^n X_i → μ$ in probability.
\item Khinchine's WLLN :: If $X₁,...,X_n$ is a iid sequence of
          random variables with mean $μ$ then $\plim \Xb = μ$.
\begin{itemize}
\item Here we don't assume that second moments exist.  So the earlier
         proof doesn't work
\end{itemize}
\item Chebychev's WLLN :: Suppose that $X₁,...,X_n$ is an
  independent sequence of r.v. with $\E X_i = μ_i$ and $\var X_i = σ²_i$
  and that $n^{-2} ∑_i σ²_i → 0$ as $n → ∞$.  Then $\plim \Xb - \μb = 0$.
\begin{itemize}
\item discuss intuition with a picture.
\item Example where variance condition fails; suppose $σ²_i = i$, $μ_i = 0$.
\begin{itemize}
\item Then $\|\Xb_n\|₂ = (\var(\Xb))^{1/2}$
\begin{itemize}
\item $= \frac{1}{n²} ∑_i \var(X_i)²$
\item $= \frac{1}{n²} ∑_i i$
\item $= \frac12 + \frac{1}{2n} → \frac{1}{2}$
\end{itemize}
\item Convergence in probability fails.
\end{itemize}
\end{itemize}
\end{itemize}

\subsection{Big-O, little-o notation}
\begin{itemize}
\item This notation makes it easier to carry along remainder terms in
  proofs.\sidenote{For homework:
\begin{itemize}
\item prove that $O_p(1) + o_p(1) = O_p(1)$
\item prove that $n O_p(n) = O_p(n²)$
\item come up with other interesting facts and prove them.
\end{itemize}}
\item Little o:
\begin{itemize}
\item A sequence of random variables $X_n$ is $o_p(1)$ as $n → ∞$ if
  $X_n →^p 0$
\item Written as $X_n = o_p(1)$
\item $X_n = o_p(n)$ as $n → ∞$ if $X_n/n = o_p(1)$
\end{itemize}
\item Big O:
\begin{itemize}
\item $X_n$ is $O_p(1)$ as $n → ∞$ if, for each $ε > 0$, there are
  constants $c$ and $N$ such that $\Pr[|X_n| > c] < ε$ for every $n > N$.
\begin{itemize}
\item Written as $X_n = O_p(1)$
\end{itemize}
\item $X_n = O_p(n)$ if $X_n/n = O_p(1)$
\item Captures the idea of being bounded for a random variable
         (i.e. ``bounded in probability'')
\end{itemize}
\item Be careful, even though it's written as an equality, the direction matters:
\begin{itemize}
\item $o_p(1) = O_p(1)$ but $O_p(1) ≠ o_p(1)$
\item First statement means, ``if a sequence converges to zero in
         probability, it is bounded in probability.''
\item A proof: let $X_n$ be an $o_p(1)$ sequence.
\begin{itemize}
\item Then, $X_n → 0$ in probability, so, for each positive $ε$ and
  $δ$, there is a value of $N$ such that
  \[\Pr[|X_n| > δ] < ε\] for every $n > N$.
\item So the big O condition holds with $c = δ$
\end{itemize}
\item A counterexample to the second ``claim'' that $O_p(1) = o_p(1)$
\begin{itemize}
\item means ``if a sequence is bounded in probability, it converges to zero in probability''
\item let $X_n$ be i.i.d. uniform(0,1)
\item then $\Pr[|X_n| > c]  = 0$ for any $c > 1$
\item but $X_n$ does not converge to zero in probability.
\end{itemize}
\end{itemize}
\end{itemize}

\subsection{Almost sure convergence}
\begin{itemize}
\item This is a somewhat unintuitive convergence concept that is stronger
        than convergence in probability.
\item Remember that a random variable is a function from a sample space
        to the real line;
\begin{enumerate}
\item so if we have the probability space $(Ω, \Fs, P)$ and $ω$ is an
  element of $Ω$, we know that $X_j(ω)$ is equal to some number.
\item $X_j(ω')$ might be equal to a different number.
\item So we can talk about different sequences of numbers that
  depend on $ω$
\begin{itemize}
\item $X₁(ω), X₂(ω), ..., X_k(ω)$.
\item $X₁(ω'), X₂(ω'), ..., X_k(ω')$.
\end{itemize}
\item For a given $ω$, the sequence of outcomes might converge, for a
  different $ω$, the sequence might not.
\item If the probability of drawing a value of $ω$ that gives a
           convergent sequence of real numbers is one, this means that
           the sequence of random variables also converges, and in a very
           ``strong'' way.
\end{enumerate}
\item Definition: The sequence of random variables $Y_n$ converges
        \emph{almost surely} to the random variable $Y$ if, for every
        $\ep > 0$, \[ \Pr[\lim_{n → ∞} | Y_n - Y | < \ep] = 1 \]
\item notation -- arrow with a.s. over it
\item almost sure convergence implies convergence in probability, but
        not vice versa.
\end{itemize}

\paragraph{Example of almost sure convergence (from SLLN)}
\begin{itemize}
\item Remember that the SLLN implies that $\Xb_n → μ$ a.s.
\item \texttt{library(lattice)}
\item \texttt{d <- do.call(rbind, lapply(1:30, function(i) \{}
\begin{itemize}
\item \texttt{xs <- runif(300, -1, 1)}
\item \texttt{cbind(i = i, n = 1:300, xbar = cumsum(xs)/(1:300))\}))}
\end{itemize}
\item Plotting individual sequences:
\begin{itemize}
\item \texttt{xyplot(xbar ~ n, data = as.data.frame(d), group = i, col = "black", type = "l", subset = i==1)}
\item \texttt{xyplot(xbar ~ n, data = as.data.frame(d), group = i, col = "black", type = "l", subset = i==2)}
\item \texttt{xyplot(xbar ~ n, data = as.data.frame(d), group = i, col = "black", type = "l", subset = i==3)}
\end{itemize}
\item Plotting them all:
\begin{itemize}
\item \texttt{xyplot(xbar ~ n, data = as.data.frame(d), group = i, col = rgb(0,0,0,.4), type = "l")}
\end{itemize}
\end{itemize}

\paragraph{Example of convergence i.p. but not a.s.}
\begin{itemize}
\item a sequence of r.v. that converges to zero in probability, but
        not almost surely.
\item Let $U$ be a r.v. from the uniform(0,1) distribution.
\begin{itemize}
\item Define the sequence: (draw this sequence)
\begin{itemize}
\item $X₁(U) = 1$
\item $X₂(U) = \1\{U ≤ 1/2\}$,
\item $X₃(U) = \1\{U > 1/2\}$,
\item $X₄(U) = \1\{U ≤ 1/3\}$
\item $X₅(U) = \1\{1/3 < U ≤ 2/3\}$
\item $X_6(U) = \1\{U > 2/3\}$
\item etc.
\end{itemize}
\item Obviously, each $X_n$ is either zero or one, and the
          probability it is one decreases with $n$.
\item But, for any value of $U$, $X_n(U)$ does not converge to zero
          ---there are always later values that are equal to one.
\end{itemize}
\item to plot them:
\begin{itemize}
\item function to generate $X₁,...,X_n$
\begin{itemize}
\item \texttt{Xseq <- function(U, n) \{}
\begin{itemize}
\item \texttt{den <- 1}
\item \texttt{num <- 1}
\item \texttt{X <- rep(NA, n)}
\item \texttt{for (i in 1:n) \{}
  \begin{description}
  \item[.] \texttt{X[i] <- (num-1)/den < U \&\& U < = num/den}
  \item[.] \texttt{if (num==den) \{}
    \begin{description}
    \item[.] \texttt{num = 1}
    \item[.] \texttt{den = den + 1}
    \end{description}
  \item[.] \texttt{\} else \{}
    \begin{description}
    \item[.] \texttt{num = num+1}
    \end{description}
  \item[.] \texttt{\}\}}
  \end{description}
\item \texttt{X\}}
\end{itemize}
\end{itemize}
\item To plot them (repeat the plot several times)
\begin{itemize}
\item \texttt{plot(X(runif(1), 500), type = "l")}
\end{itemize}
\item For plotting several at once:
\begin{itemize}
\item \texttt{d <- do.call(rbind, lapply(1:100, function(i)) \{}
\begin{itemize}
\item \texttt{U <- runif(1)}
\item \texttt{cbind(i = i, U = U, X = Xseq(U, 500), n = 1:500)\})}
\end{itemize}
\item \texttt{d <- as.data.frame(d)}
\item Marginals (to see convergence in probability)
\begin{itemize}
\item \texttt{d\$jitter <- runif(nrow(d), -5, 5)}
\item \texttt{xyplot(X ~ n + jitter, data = d, col = rgb(0,0,0,.2), subset = n \%\% 100 = 1)}
\end{itemize}
\item Sample paths
\begin{itemize}
\item \texttt{xyplot(X ~ n, d, group = i, col = rgb(0,0,0,.3), type = "l", subset = i < 10)}
\end{itemize}
\end{itemize}
\end{itemize}
\end{itemize}

\paragraph{Strong law of large numbers}
\begin{itemize}
\item Similar to weak law of large numbers, but with almost sure
       convergence.  I'll give an example
\item Kolmogorov's SLLN :: If $\{X_n\}$ is a sequence of iid random
  variables with mean $μ$ then $\Xb → μ$ almost surely.
\end{itemize}

\subsection{Consistency}
     Consistency is an asymptotic version of unbiasedness

\paragraph{Definition}
an estimator $\θh$ is a consistent estimator of the parameter $θ$ if
$\θh →^d{p} θ$ for any value of $θ$.

\paragraph{Basic result for consistency \textbf{:hw:}}
\begin{itemize}
\item Statement of the result
\begin{itemize}
\item Let $\θh_n$ be a sequence of estimators of $θ$ such
          that, for every $θ$,
\begin{enumerate}
\item $\var_θ(\θh_n) → 0$
\item $\E_θ (\θh_n) → 0$
\end{enumerate}
\item then $W_n$ is consistent for $θ$
\end{itemize}
\item Proof:
\begin{itemize}
\item Follows immediately from Cauchy-Schwarz
\item Want to show $\Pr[|\θh_n - θ| > ε] → 0$ for all $ε > 0$
  \begin{align*}
    \Pr[|\θh_n - θ| > ε] 
    &≤ \dfrac{\E(\θh_n - θ)²}{ε²} \\
    &= \dfrac{\E((\θh_n - E\θh_n)+ (\E\θh_n - θ))²}{ε²} \\
    &= \dfrac{\E(\θh_n - E\θh_n)²}{ε²}
      + 2 \dfrac{\E((\θh_n - E\θh_n)(E\θh_n - θ))}{ε²} + \dfrac{(E\θh_n - θ)²}{ε²} \\
    &= o(1) + 0 + o(1)
  \end{align*}
\end{itemize}
\end{itemize}

\paragraph{Consistency of MLE (\citealp[Theorem 10.1.6]{CB02} and
  \citealp[Section 14.4]{Gre12})}
\begin{itemize}
\item We're just going to mention the result
\begin{itemize}
\item you'll see it in more detail next semester
\item we won't mention the assumptions
\item But, this is a big justification for using MLE as a general method
\end{itemize}
\item Suppose that $X₁,...,X_n ∼ i.i.d.\ f(x; θ)$ where
        $f$ satisfies some technical regularity conditions and suppose
        $\θh_n$ is the MLE of $θ$.  Then $\θh_n →^p θ$ as $n → ∞$.
\begin{itemize}
\item Note, doesn't always hold, but usually holds.
\item A good homework exercise---look up the regularity conditions
          in \citet{CB02} or \citet{Gre12} and see if common families (gamma,
          exponential, uniform, etc) satisfy them; then try to prove consistency
          directly.
\item There are weaker conditions than listed in \citet{CB02} (in particular,
          we can relax i.i.d)
\end{itemize}
\end{itemize}

\paragraph{Consistency of method of moments estimators}

Suppose that $X₁,...,X_n ∼ f(·; θ)$ where $θ$ is a $k$-vector, $(\E
X_i,...,\E X_i^k) = g(θ₁,...,θ_k)$, $g^{-1}$ is continuous, and that
$\frac{1}{n} ∑_i X_i^k$ obeys a weak LLN.  Then the Method of Moments
estimator of $θ$ is consistent.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../asymptotics"
%%% End: 
