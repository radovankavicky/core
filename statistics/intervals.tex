% Copyright (c) 2013-2014, Gray Calhoun.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\section{Confidence intervals}

\subsection{Introduction to interval estimation}

\begin{itemize}[leftmargin=0pt]  

\item Note that our discussion of the sampling distribution lets us
  think rigorously about the precision associated with a particular
  estimator of an unknown parameter $\theta$.  But it hasn't helped us
  model uncertainty about the parameter value itself.  We'll see in
  this section that confidence intervals are one way to think
  rigorously about uncertainty of the parameter value, and we'll see
  how to generate confidence intervals from point estimators.

\item Let's introduce some definitions
  \begin{defn}
    An \emph{interval estimator} of a parameter $\theta$ is any pair of
    functions $L(X)$ and $U(X)$ s.t. $L(x) \leq U(x)$ for all $x$ and,
    when we observe $x$ we make the inference $\theta \in [L(x), U(x)]$.
  \end{defn}

  \begin{defn}
    The \emph{coverage probability} of an interval estimator
    $[L(X), U(X)]$ is the probability $\Pr_\theta[\theta \in [L(X), U(X)]]$
  \end{defn}

  \begin{defn}
    The \emph{confidence coefficient} is $\inf_\theta P_\theta[\theta \in [L(X), U(X)]]$
  \end{defn}

  Generally, the idea behind an interval estimator is that we want to
  estimate $\theta$ in a way that accounts for uncertainty.  So the
  researcher ``agrees'' not to distinguish between values in the
  interval.  These intervals can be one-sided or two-sided.

\item The idea of a confidence interval can be extended to
  \emph{confidence regions} or \emph{confidence sets} when the
  parameter has more than one element.  The extension is obvious.

\item If we have a statistic with known sampling distribution, we can
  often use that distribution to generate a confidence interval.  This
  is the idea: suppose you have a statistic $\thetah$ and know that its
  distribution function, $F(t; \theta)$, is a monotone function of $\theta$.  We
  can define a $1-\alpha$ confidence interval of the form $[\theta_L(\thetah),
  \theta_U(\thetah)]$ by solving the following equations for $\theta_L, \theta_U$:

  \begin{itemize}
  \item If $F(t; \theta)$ is decreasing in $\theta$:
    \begin{align*}
      F(\thetah; \theta_U) &= \alpha/2 & F(\thetah; \theta_L) &= 1 - \alpha/2
    \end{align*}
  \item If $F(t; \theta)$ is increasing in $\theta$:
    \begin{align*}
      F(\thetah; \theta_L) &= \alpha/2 & F(\thetah; \theta_U) &= 1 - \alpha/2
    \end{align*}
  \item Note that the $L$ and the $U$ are switched in the two formula.
  \end{itemize}

  \begin{ex} Let $X_1,...,X_n$ be iid $N(\theta, 1)$ and we want to
    construct a two-sided 95\% interval for $\theta$.  Let be $\Xb$, which
    is $N(\theta, 1/n)$.  Then
    \begin{equation*}
      \Pr_\theta[\Xb \leq t] = \Phi(\sqrt{n} (t - \theta)) = F(t; \theta)
    \end{equation*}
    which is decreasing in $\theta$.

    To construct an interval, we need to solve
    \begin{align*}
      \Phi(\sqrt{n} (\Xb - \theta_U)) &= 0.025, &
      \Phi(\sqrt{n} (\Xb - \theta_L)) &= 0.975
    \end{align*}
    which becomes
    \begin{align*}
      \Xb - \theta_U &= \Phi^{-1}(0.025) / \sqrt{n}, &
      \Xb - \theta_L &= \Phi^{-1}(0.975) / \sqrt{n}
    \end{align*}
    which then becomes
    \begin{align*}
      \Xb - \Phi^{-1}(0.025) / \sqrt{n} &= \theta_U, &
      \Xb - \Phi^{-1}(0.975) / \sqrt{n} &= \theta_L.
    \end{align*}
    Since $\Phi^{-1}(0.025) = -1.96$ and $\Phi^{-1}(0.975) = 1.96$, this
    finally becomes
    \begin{align*}
      \theta_U &= \Xb + 1.96 / \sqrt{n}, &
      \theta_L &= \Xb - 1.96 / \sqrt{n}
    \end{align*}
  \end{ex}

\end{itemize}

\subsection{Interval optimality}

\begin{itemize}[leftmargin=0pt]

\item Generally, as you might assume from above, if you have several
  estimators to use, you should choose the estimator with the tighter
  sampling distribution---the more efficient estimator.

\item This is going to lead to a principle better for asymptotic
  intervals.

\item A second question, for a two sided interval, is how to spit the
  mass between the tails.  The following theorem can help (this
  version is taken from \citealp{CB02}):

  \begin{thm}
    Let $f(x)$ be a unimodal pdf with mode $x^*$.  If $[a,b]$
    satisfies
    \begin{enumerate}
    \item $\int_a^b f(x) dx = 1 - \alpha$,
    \item $f(a) = f(b) > 0$, and
    \item $a \leq x^* \leq b$
    \end{enumerate}
    then $[a,b]$ is the shortest interval with coverage $1-\alpha$.
  \end{thm}

  The proof works by showing that any shorter interval has coverage
  strictly less than $1-\alpha$.  It's pretty easy to see from pictures,
  which I need to draw.

  \begin{itemize}
  \item Suppose $b'-a' < b-a$ and $a', b' \leq a$
  \item Suppose $b'-a' < b-a$ and $a' \leq a < b'$ (so then $b' < b$).
  \item Suppose $b'-a' < b-a$ and $a < a'$ and $b' < b$.
  \item Suppose $b'-a' < b-a$ and $a < a'$ and $b \leq b'$.
  \end{itemize}

\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../core_econometrics"
%%% End:
