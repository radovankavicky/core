% Copyright (c) 2013-2014, Gray Calhoun.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\section{Bootstrap}
\tryinput{statistics/bootstrap_macros.tex}
\providecommand{\bootquantiles}{?}
\providecommand{\bootleft}{?}
\providecommand{\bootright}{?}
\providecommand{\medv}{?}
\providecommand{\powv}{?}
\providecommand{\ymedian}{?}
\providecommand{\bootinference}{?}

\subsection{Introduction to bootstrap material}

\begin{itemize}[leftmargin=0pt]

\item There are two approaches one could take, just based on what
  we've done so far.  One is to use the asymptotic distribution (i.e.,
  assume that the OLS estimator is normal).  The second approach is to
  assume a particular finite-sample distribution for the errors, and
  then work out the finite-sample distribution of the OLS estimator
  under that error structure.  If $n$ is large, this will be more or
  less the same as the asymptotic distribution, but there may be
  differences.  This is the approach we advocated when we suggested
  using $t$ or $F$ critical values instead of normal or chi-squared.

  Assuming the errors are normal makes it easy to get explicit numbers
  for the critical values, but it's not necessary.  We could assume
  that the errors are $\uniform(-1,1)$ and generate critical values by
  simulation.

  Here are some examples.\footnote{This section's analysis is done in
    \emph{Julia} \citep{BKS12} and uses the \emph{fancyvrb} LaTeX
    package \citep{Zan98} to embed source code in the document.}

  First, define a simple function that will generate \verb|nsims|
  averages (default 5000), each of \verb|nobs| draws from the
  distribution specified in \verb|dist|, as follows
  \renewcommand*\FancyVerbStartString{## block 2}
  \VerbatimInput{statistics/bootstrap.jl}
  For example, to draw sample means of two uniform random variables,
  we would use
  \begin{Verbatim}[gobble=4]
## block 2
    rmeans(Uniform(), 2)
  \end{Verbatim}

  We'll generate averages from the uniform distribution first; see
  Figure~\ref{fig:bootstrap1}. Observe that the normal approximation
  for the uniform mean is reasonable even when $n$ is as small as 5.
  Plots for the exponential distribution are in the same figure and we
  can see that the asymmetry has an effect even when $n$ is 50.

  \begin{figure}\centering
    \begin{tabular}{cc}
      \subfloat[Sampling distribution of the mean of two
      $\uniform(0,1)$
      r.v.s.]{\tryincludegraphics{statistics/bootstrap_u1.pdf}} &
      \subfloat[Sampling distribution of the mean of two exponential
      r.v.s.]{\tryincludegraphics{statistics/bootstrap_ex1.pdf}} \\

      \subfloat[Sampling distribution of the mean of five
      $\uniform(0,1)$
      r.v.s.]{\tryincludegraphics{statistics/bootstrap_u2.pdf}} &
      \subfloat[Sampling distribution of the mean of five exponential
      r.v.s.]{\tryincludegraphics{statistics/bootstrap_ex2.pdf}} \\

      \subfloat[Sampling distribution of the mean of 50 $\uniform(0,1)$
      r.v.s.]{\tryincludegraphics{statistics/bootstrap_u3.pdf}} &
      \subfloat[Sampling distribution of the mean of 50 exponential
      r.v.s.]{\tryincludegraphics{statistics/bootstrap_ex3.pdf}}
    \end{tabular}
    \caption{Finite-sample distribution of the mean of uniform and
      exponential random variables, based on 5000 simulations.}
    \label{fig:bootstrap1}
  \end{figure}

  Note that the histograms can be made arbitrarily close to the true
  \emph{finite sample} densities just by increasing the number of
  simulations.

\item Notice that, by and large, it does not matter if we know the
  exact distribution of the errors---using that information would not
  affect our critical values for the statistic.

\item Of course, another approach would be to use the \emph{best}
  finite sample distribution---one with potential optimality properties.
  If we knew the true distribution of the errors, then we would have a
  slightly more accurate distribution for the OLS estimators.  Since
  we don't know the true distribution, we can try estimating it.  This
  is the basic idea behind the bootstrap.

  Suppose we observe the data $y_1,...,y_n$ (univariate for now), and
  let $Y^*$ be a random variable s.t.
  \begin{equation*}
    Y^* =
    \begin{cases}
      y_1 & \text{with probability } 1/n \\
      y_2 & \text{with probability } 1/n \\
      \vdots \\
      y_n & \text{with probability } 1/n.
    \end{cases}
  \end{equation*}
  The corresponding distribution function is called the empirical cdf,
  \begin{equation*}
    \Fh(y) = (1/n) \sum_{i=1}^n \ind\{ Y_i \leq y \}.
  \end{equation*}
  Now we can estimate the sampling distribution of $\Yb$ by
  calculating it's finite sample distribution under the (incorrect)
  assumption that each $Y_i \sim \iid\ \Fh$.  We'll do that through
  simulation.

  \renewcommand*\FancyVerbStartString{## block 1}
  \VerbatimInput{statistics/bootstrap.jl}

  We can mimic a testing scenario as follows.
  \renewcommand*\FancyVerbStartString{## block 3}
  \VerbatimInput{statistics/bootstrap.jl}
  The endpoints for 5\% one-sided intervals based on the bootstrap
  distribution are given by \verb|quantile(boots, [0.05, 0.95])| and
  equal \bootquantiles.  They would have true coverage of \bootleft\
  and \bootright.

\end{itemize}

\subsection{Estimating the variance}

\begin{itemize}[leftmargin=0pt]

\item One potential use of the bootstrap is to estimate the variance
  of an estimator.  To do this, calculate the empirical distribution
  function, $\Fh$, of the observed data, then repeat the following $B$
  times:
  \begin{itemize}
  \item Draw a random sample of size $n$ from $\Fh$ with replacement
    and estimate the statistic on that sample; call it $\thetah^*_b$.
  \end{itemize}
  This gives $\thetah^*_1,...,\thetah^*_B$ replications of the estimator.
  Calculate their variance, and this is an estimator of the variance
  of $\thetah$.

  We can illustrate this for the median with the command
  \renewcommand*\FancyVerbStartString{## block 4}
  \VerbatimInput{statistics/bootstrap.jl}
  which equals \medv.

\item When the variance is easy to calculate, this isn't a big
  advantage.  But if we (say) want to calculate the variance of the
  median raised to the 1.2 power, our code becomes
  \renewcommand*\FancyVerbStartString{## block 5}
  \VerbatimInput{statistics/bootstrap.jl}
  which is a trivial code change (the variance estimate is now \powv).
  Now, the bootstrap doesn't always work (you need conditions like you
  need for delta method to work).

\item Another advantage is that the bootstrap approximation is usually
  more accurate than other approximations (see \citealp{Hal91}, for
  details).

\end{itemize}

\subsection{Inference}

\begin{itemize}[leftmargin=0pt]

\item A complication shows up when you want to use the bootstrap for
  testing: resampling the data will typically not impose the null
  hypothesis, so you need to manually impose it.  Suppose you want to
  test the null hypotheses that $\E Y = 0$ so you use the bootstrap to
  generate a critical value for the median.  To do this, you need to
  resample from $Y_1 - m(Y),...,Y_n - m(Y)$.
  \renewcommand*\FancyVerbStartString{## block 6}
  \VerbatimInput{statistics/bootstrap.jl}
  Here the median is \ymedian\ and the 95\% bootstrap quantile is
  \bootinference, so we would reject the null that the median is zero.

\item This works better if you use studentized statistics.
  
\item The main thing to keep in mind is the analogy: population is to
  sample as sample is to bootstrap.  The bootstrap builds a
  ``bootstrap world'' that we can analyze exactly.  This ``bootstrap
  world'' is an approximation to the real world.

\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../core_econometrics"
%%% End:
