% Copyright (c) 2013-2014, Gray Calhoun.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\section{Point estimators}

\subsection{Summarizing data sets}

\begin{itemize}[leftmargin=0pt]

\item We'll start with a few very basic definitions.
  \begin{defn}
    The random variables $X_1,...,X_n$ are a \emph{random sample} if
    they form an i.i.d. draw from some distribution $F$.
  \end{defn}

  \begin{defn}
    If $T$ is a function from the sample space of $(X_1,...,X_n)$ to
    $\RR^k$ then the random variable $T(X_1,...,X_n)$ is a
    \emph{statistic}.
  \end{defn}

  Note that statistics are random variables and have distributions.  A
  statistic is anything that can be calculated from the data.  It can
  not depend on unknown parameters of the distribution.

\item It's worth spending a little while thinking about ``reasonable''
  ways to summarize datasets.  For now, we're only going to worry
  about mathematical characterizations of ``summarizing'' a dataset,
  we're not going to get into statistical graphics, etc.  One natural
  way to think about a data summary is through the idea of
  ``sufficient'' statistics.

  \begin{defn}
    If $X_1,...,X_n \sim f(\cdot; \theta)$ is a draw from a known family of
    distributions with unknown parameter $\theta$, a statistic $T(X)$ is a
    \emph{sufficient statistic for $\theta$} if the conditional
    density/distribution $f(\cdot \mid T(X))$ does not depend on $\theta$.
  \end{defn}

  The order statistics are always going to be a sufficient statistic
  for i.i.d. random variables, but there are often better (as in
  smaller) options, and we'll present a result soon that can help in
  constructing sufficient statistics.

  \begin{defn}
    $T(X)$ is a \emph{minimal sufficient statistic} if it is a
    function of any other sufficient statistic.
  \end{defn}

  First, though, there's the related concept of ancillarity.

  \begin{defn}
    If $X_1,...,X_n \sim f(\cdot; \theta)$ is a draw from a known family of
    distributions with unknown parameter $\theta$, a statistic $T(X)$ is an
    \emph{ancillary statistic for $\theta$} if the sampling distribution of
    $T(X)$ does not depend on $\theta$.
  \end{defn}

  Ancillary statistics can be informative about the precision of an
  estimate: for example, if $X_1,\ldots,X_n \sim \uniform(\theta,\theta+1)$, we can
  see that $\range(X_i) = \max_i X_i - \min_i X_i$ is ancillary:
  \begin{align*}
    \max_i X_i - \min_i X_i
    &= \max_i (X_i - \theta) - \min_i (X_i - \theta) \\
    &=^d \max_i U_i - \min_i U_i
  \end{align*}
  where $U_i \sim \uniform(0, 1)$.  But, if
  \begin{equation*}
    \thetah = (1/2) \min_i X_i + (1/2) \max_i (X_i - 1)
  \end{equation*}
  this estimator will clearly be more precise if the range is close to
  one than if it is close to zero.  So samples with a large range are
  more informative about $\theta$ than those with a small range.  This
  observation can sometimes make inference \emph{conditional on
  ancillary statistics} attractive, but we won't cover that topic in
  this document.  See \citet[Chapter 10]{LR:05} for further discussion.

\item The ``factorization theorem'' is useful in determining whether
  or not a given statistic is sufficient for a parameter.
  \begin{thm}
    If $f(x; \theta)$ is the joint pdf of $(X_1,...,X_n)$, $T(X)$ is a
    sufficient statistic for $\theta$ if and only if
    \begin{equation*}
      f(x; \theta) = g(T(x); \theta) h(x)
    \end{equation*}
    for some functions $g$ and $h$.
  \end{thm}
  The important thing to note is that $T(x)$ and $\theta$ are both in $g$
  and $\theta$ is not in $h$.  The other thing to note is the ``if and only
  if'' part of the proof.

  It is straightforward to prove that sufficiency implies the
  factorization holds: let $h(x)$ be the joint density of $x$ given
  $T(X)$.  It is less straightforward, but more important in practice,
  to prove that existence of the factorization implies sufficiency.
  The result still follows from the conditional densities (as it seems
  that it must).
  \begin{align*}
    f_X(x \mid T(X) = T(x); \theta)
    &= f(x, T(x); \theta) / f_T(T(x); \theta) \\
    &= f_X(x; \theta) / f_T(T(x); \theta) \\
    &= g(T(x); \theta) h(x) / f_T(T(x); \theta)
  \end{align*}
  where the second equality holds because $T(x)$ is redundant when we
  know $x$, and the third equality holds because we're assuming that
  this factorization exists.

  Now we can write the denominator as
  \begin{align*}
    f_T(T(x); \theta)
    &= \int_A f(T(y) \mid X = y; \theta) f_X(y) \dy \\
    &= \int_A f_X(y; \theta) \dy \\
    &= \int_A g(T(x); \theta) h(y) \dy \\
    &= g(T(x); \theta) \int_A h(y) \dy
  \end{align*}
  where $A$ is the set of points s.t. $T(y) = T(x)$ for all $y \in A$
  and the equalities hold for the same reasons as before.

  Now just merge the two equations to get
  \begin{equation*}
    f_X(x \mid T(X) = T(x); \theta)
    = \frac{g(T(x); \theta) h(x)}{g(T(x); \theta) \int_A h(y) \dy}
    = \frac{h(x)}{\int_A h(y) \dy}.
  \end{equation*}
  This last quantity does not depend on $\theta$, so we're done.

  We can use the factorization theorem to prove that $\min_i X_i$ and
  $\max_i X_i$ are sufficient statistics for the $\uniform(a,b)$.
  \begin{ex}
    The joint pdf of $X_1,...,X_n$ is
    \begin{align*}
      f(x_1,...,x_n; a, b)
      &= \prod_{i=1}^n \tfrac{1}{b-a} \ind\{x_i \in [a,b]\} \\
      &= \Big(\tfrac{1}{b-a}\Big)^n \ind\{a \leq \min_i x_i
         \text{ and } \max_i x_i \leq b\},
    \end{align*}
    so we can define $h(x) = 1$ and
    \begin{equation*}
      g(T(x); a, b) = \Big(\tfrac{1}{b-a}\Big)^n
        \ind\{a \leq \min_i x_i \text{ and } \max_i x_i \leq b\}.
    \end{equation*}
  \end{ex}

\item We can also think of the density function itself as giving a
  summary of the dataset.\footnote{Note that we're implicitly assuming
    that we know the density function, but that's not really true.}
  Earlier, though, we viewed the densities as functions of the
  possible values of the random variables, $x$, given particular
  parameter values.  Now we're going to treat the random variables as
  known/observed and view the densities as functions of the possible
  parameterizations.  When we do that, we call them \emph{likelihoods}
  and will typically write them as $L(\theta; x)$.

  [PLOT PICTURE OF UNIFORM LIKELIHOOD.]

  Regions where the likelihood is high are to some degree more
  plausible than regions where it is low.  If we look at the uniform
  dist; when the likelihood is higher, its corresponding density is
  more concentrated around the observed data.

\end{itemize}

\subsection{The method of moments}

\begin{itemize}[leftmargin=0pt]
\item Suppose we have $X_1,...,X_n \sim \iid\ f(x; \theta)$ where $f$ is known
  and $\theta$ is an unknown $p$-vector that we want to estimate.  The
  \emph{method of moments} estimator is based on a straightforward
  idea often called the ``analogy principle''---replace population
  expectations with averages to derive an estimator.  A good estimator
  of $\Pr[X_i \leq c]$ is often $(1/n) \sum_i \ind\{X_i \leq c\}$, a good
  estimator of $\E X_i$ is often $(1/n) \sum_i X_i$, etc.

  To estimate $\theta$, we first relate it to the first $p$ moments of
  $X_i$:
  \begin{equation}\label{f1}
    (\mu_1,...,\mu_p)' = (g_1(\theta),...,g_p(\theta))'
  \end{equation}
  where $\mu_\ell = \E X_i^\ell$ and $g$ depends on the density $f$.  Then the
  estimator of $\theta$ is the population vector that solves~\eqref{f1},
  so
  \begin{equation}\label{f2}
    \big((1/n) \sum_i X_i,...,(1/n) \sum_i X_i^p\big)
    = (g_1(\thetah),...,g_p(\thetah))'
  \end{equation}
  or
  \begin{equation*}
    \thetah = g^{-1}\big((1/n) \sum_i X_i,...,(1/n) \sum_i X_i^p\big).
  \end{equation*}
  Obviously $g$ has to be inevitable for this to be a feasible
  estimator.  For this to be a reasonable estimator, we want each
  $\mu_\ell$ to be close (in some as of yet unspecified way) to $\mu_\ell$ and
  $g^{-1}$ to be continuous so that $\thetah$ is correspondingly close to
  $\theta$.

\item If it is difficult or impossible to get a closed form solution
  for $\thetah$ you can solve~\eqref{f2} numerically.

\item We will see later that averages are easy to analyze with
  asymptotics (as the number of observations gets arbitrarily large)
  and, if $g^{-1}$ is smooth, $\thetah$ will also be easy to analyze.

\item It might help to do a few examples.

\item %
  \begin{ex}
    Suppose that $X_i \sim \iid\ \N(\mu, \sigma^2)$ where both $\mu$ and $\sigma^2$ are
    unknown.  The first two moments are
    \begin{equation*}
      (\E X_i, \E X_i^2) = (\mu, \mu^2 + \sigma^2)
    \end{equation*}
    and so the method of moments estimator of $(\mu, \sigma^2)$ is given by
    \begin{align*}
      (\muh, \sigmah^2)
      &= (\muh_1, \muh_2 - \muh_1^2) \\
      &= \big( (1/n) \sum_i X_i, (1/n) \sum_i (X_i - \Xb)^2 \big)
    \end{align*}
  \end{ex}

\item %
  \begin{ex}
    Suppose that $X_i \sim \iid\ \uniform(a,b)$ where the parameters $a$
    and $b$ are both unknown.  We can again calculate the first two
    moments of $X_i$,
    \begin{align*}
      \E X_i
      &= (1/(b-a)) \int_a^b x \dx \\
      &= (b+a) / 2
    \intertext{and}
      \E X_i^2
      &= (1/(b-a)) \int_a^b x^2 \dx \\
      &= \frac{b^3 - a^3}{3(b - a)} \\
      &= (1/3) \times (b^2 + ab + a^2).
    \end{align*}
    These can be solved for $\ah$ and $\bh$:
    \begin{equation*}
      \ah = \bh - 2 \muh_1
    \end{equation*}
    and
    \begin{equation*}
      \bh^2 + (\bh - 2 \muh_1) \bh + (\bh - 2 \muh_1)^2 - 3 \muh_2 = 0
    \end{equation*}
    so
    \begin{equation*}
      (\ah, \bh) = (\muh_1 - \sh \sqrt{3}, \muh_1 + \sh \sqrt{3})
    \end{equation*}
    where $\sh = \sqrt{(1/n) \sum_i (X_i - \Xb)^2}$.
  \end{ex}

  This is probably not a good estimator of $a$ and $b$.

\item %
  Suppose now that $(Y_i, X_i) \sim \iid$ where the distribution of $X_i$
  is unspecified and
  \begin{equation*}
    Y_i \mid X_i \sim N(\beta_0 + \beta_1 X_i, \sigma^2).
  \end{equation*}
  We want to estimate $\beta_0$ and $\beta_1$ and will treat
  $\sigma^2$ as a known constant for now.

  The method of moments estimator is based on $\E Y_i$ and $\E X_i
  Y_i$:
  \begin{equation*}
    \begin{pmatrix} \E Y_i \\ \E X_i Y_i  \end{pmatrix}
    =
    \begin{pmatrix}
      \beta_0 + \beta_1 \E X_i \\ \beta_0 \E X_1 + \beta_1 \E X_1^2
    \end{pmatrix}
    =
    \begin{pmatrix}
      1 & \E X_i \\ \E X_i & \E X_i^2
    \end{pmatrix}
    \begin{pmatrix} \beta_0 \\ \beta_1 \end{pmatrix}
  \end{equation*}
  So
  \begin{equation*}
    \begin{pmatrix} \beta_0 \\ \beta_1 \end{pmatrix}
    =
    \begin{pmatrix} 1 & \E X_i \\ \E X_i & \E X_i^2 \end{pmatrix}^{-1}
    \begin{pmatrix} \E Y_i \\ \E X_i Y_i  \end{pmatrix}
  \end{equation*}
  Assuming invertability of the matrix.  The estimator is (again,
  assuming invertability)
  \begin{equation*}
    \begin{pmatrix} \betah_0 \\ \betah_1 \end{pmatrix}
    = \begin{pmatrix} 1 & (1/n) \sum_i X_i \\ 
      (1/n) \sum_i X_i & (1/n) \sum_i X_i^2 \end{pmatrix}^{-1}
    \begin{pmatrix} (1/n) \sum_i Y_i \\ (1/n) \sum_i X_i Y_i  \end{pmatrix}
  \end{equation*}

\item The method of moments estimator has some advantages: it is
  usually a viable way to get an estimator and the estimator is
  usually easy to analyze, at least asymptotically.

\item The estimator has some disadvantages too.  It is usually going
  to be inefficient.  It should be clear that the estimator is
  reliable only when the first few moments have a lot of information
  about the distribution.  This is true for the Normal distribution
  (in the univariate case as well as the linear regression case), but
  not for the uniform.

  The estimators are not even guaranteed to satisfy distributional
  constraints: we could easily estimate values of $\ah$ and $\bh$ that
  are smaller than the smallest and largest observed values.

\item There is a variation of method of moments, \emph{Generalized
    Method of Moments} (usually abbreviated to GMM) \citep[derived
  by][as part of his Ph.D. dissertation]{Han:82}.  Many models in
  economics give implication that the period $t$ conditional
  expectation satisfies 
  \begin{equation*}
    \E_t g(X_t, \theta) = 0
  \end{equation*}
  where $g(x_t, \theta)$ comes from agent rationality---essentially,
  $g(X_t,\theta)$ measures the agent's surprise, and if agents choose
  rationally in period $t$, then the difference between their
  prediction and the actual outcome must be unforecastable (if it
  weren't, they would have already acted).

  The LIE tells us that
  \begin{equation*}
    \E g(X_t, \theta) = 0
  \end{equation*}
  unconditionally as well, so $\theta$ can be estimated by solving
  \begin{equation*}
    (1/n) \sum_t g(X_t, \theta) = 0
  \end{equation*}
  for $\theta$.  If there are more equations than parameters, $\theta$ can be
  estimated using a weighting scheme, and the additional equations can
  be used to test the adequacy model.  The key difference between GMM
  and the method of moments is that the equations here are generally
  derived from economic theory and not from distributional assumptions
  on the random variables.  Of course, the setup is very general, so
  it turns out that many estimators are special cases of GMM and can
  be analyzed using the same theory.\footnote{\citet{Hay:00} gives a
    treatment of much of the material we cover, using GMM as the
    underlying analysis.}

\end{itemize}

\subsection{Maximum Likelihood Estimators}

\begin{itemize}[leftmargin=0pt]

\item We discussed the likelihood function earlier as a method of
  summarizing a data set.  Notice that parameter values corresponding
  to higher values of the likelihood function also correspond to
  densities that are more tightly concentrated around the observed
  data.  This suggests that a good estimate might be the parameter
  value associated with the tightest density, i.e. the largest value
  of the likelihood function.

  \begin{ex} Suppose that $X_1,...,X_n \sim \uniform(0, b)$.  Then the
    likelihood function (as before) is
    \begin{equation*}
      L(b; x_1,...,x_n) = \prod_i (1/b) \ind\{0 \leq x_i \leq b\}
    \end{equation*}
    [Draw the likelihood and several densities]

    The maximum is clearly $b = \max x_i$.
  \end{ex}

\item The definition is unsurprising:
  \begin{defn}
    Suppose that $X_1,...,X_n \sim f(x_1,...,x_n; \theta)$.  The \emph{Maximum
      Likelihood Estimator} (MLE) of $\theta$ is
    \begin{equation*}
      \thetah = \argmax_\theta L(\theta; x_1,...,x_n).
    \end{equation*}
  \end{defn}

\item We can see that the usual OLS estimator is the MLE.
  \begin{ex} Suppose that $(Y_i,X_i)$ are \iid\ where $X_i$ is a $k \times
    1$ random vector with unspecified density $f$ and
    \begin{equation*}
      Y_i \mid X_i \sim N(X_i'\beta, \sigma^2),
    \end{equation*}
    $\beta$ and $\sigma^2$ are the parameters of interest.  The maximization
    problem will be easier of we take logs (remember, log is
    monotonic, so the maximizer of the log likelihood will be the same
    as that of the likelihood.  The log likelihood is
    \begin{equation*}
      \log L(\beta,\sigma^2; x, y) = const - n\log (\sqrt{\sigma^2}) -
      (1/2\sigma^2) \sum_i (y_i - x_i'\beta)^2 + \sum_i f(x_i).
    \end{equation*}
    
    The first order conditions for $\beta$ are
    \begin{equation*}
      0 = (\partial/\partial\beta) \log L(\beta, \sigma^2; x, y) = (1/\sigma^2) \sum_i x_i (y_i - x_i'\beta)
    \end{equation*}
    which gives
    \begin{equation*}
      \betah = \big(\sum_i x_i x_i'\big)^{-1} \sum_i x_i y_i.
    \end{equation*}
    The first order conditions for $\sigma^2$ are
    \begin{equation*}
      0 = (\partial/\partial\sigma^2) \log L(\beta, \sigma^2; x, y) = -(n/2\sigma^2) + (1/2\sigma^4) \sum_i (y_i - x_i'\beta)^2
    \end{equation*}
    which gives
    \begin{equation*}
      \sigmah^2 = (1/n) \sum_i (y_i - x_i'\betah)^2.
    \end{equation*}

    You should verify that this is a maximum using the second order
    conditions.
  \end{ex}

\item The derivative of the log likelihood shows up often and is
  called the \emph{score}.

\item Unlike method of moments, where we connect our parameters to the
  mean, variance, etc. regardless of the distribution; here we look at
  the features of the data that the distribution tells us are the most
  relevant.  For the normal, these features \emph{are} the mean and
  variance, so MLE and MoM give us the same statistics.  For the
  uniform, and others, this is \emph{not} the mean and variance.

\item The MLE has a nice invariance property: say you're not
  interested in the parameters per se, but care about a transformation
  of the parameters $T(\theta)$.  If $\thetah$ is the maximum likelihood
  estimator of $\theta$, then $T(\thetah)$ is the MLE of $T(\theta)$.
\item We'll see later that you can use MLE to get an estimator, even
  if you don't believe that the distribution is true.  This estimator
  is called the \emph{quasi-maximum likelihood} estimator and will be
  (asymptotically) valid as long as the quasi MLE first order
  conditions are true for the correct distribution.

\end{itemize}

\subsection{Optimality focusing on mean and variance}

\begin{itemize}[leftmargin=0pt]

\item In this section, we're going to view estimators as algorithms
  and evaluate them based on how well they typically perform.  This
  means that we're going to evaluate estimators on the basis of their
  distributions, called their \emph{sampling distribution}.  

\item Typically, the class of possible point estimators is too large
  to make general statements about optimality.  But we can make
  statements about a restricted class of estimators.  A natural
  restriction to impose is that the estimator must be correct ``on
  average,'' i.e. that it should not make systemic and predictable
  errors.  Such an estimator is called a ``unbiased'' estimator.

  \begin{defn}
    If $\thetah$ is an estimator of $\theta_0$, the \emph{bias} of $\thetah$ is the
    quantity $\E \thetah - \theta_0$.  An estimator is \emph{unbiased} if its
    bias equals zero for every value of $\theta_0$
  \end{defn}

  It can make sense to compare unbiased estimators through their
  dispersion: a more disperse estimator is probably worse than one
  that's more tightly concentrated around the true parameter value
  $\theta_0$.

  \begin{defn}
    If $\thetah_1$ and $\thetah_2$ are two unbiased estimators of a parameter
    $\theta$, $\thetah_1$ is \emph{more efficient} than $\thetah_2$ if the variance
    of $\thetah_1$ is less than that of $\thetah_2$.
  \end{defn}

  An estimator $\thetah$ of $\theta$ is said to be the \emph{best unbiased
  estimator of $\theta$} if it is unbiased and has smaller variance than
  any other unbiased estimator

\item For parameter vectors, we need the variance of $\gamma'\thetah_1$ to be
  smaller than that of $\gamma'\thetah_2$ for all nonzero $\gamma$.  This is the same
  as requiring that $\var(\thetah_2) - \var(\thetah_1)$ is positive definite
  (you should verify this on your own).

\item A particular result comes up in many different contexts
  \begin{thm}
    Suppose $\E \thetah_1 = \theta$; $\thetah_1$ is the best unbiased estimator of
    $\theta$ iff $\thetah_1$ and $\thetah_1 - \thetah_2$ are uncorrelated for any other
    unbiased estimator $\thetah_2$ of $\theta$.
  \end{thm}
  This also holds when restricted to other classes.

  The first part of the proof is easy.  Assume that $\thetah_1$ and $\thetah_1 -
  \thetah_2$ are uncorrelated.  Then
  \begin{align*}
    \var(\thetah_2) &= \var(\thetah_2 - \thetah_1 + \thetah_1) \\
    &= \var(\thetah_2 - \thetah_1) + \var(\thetah_1) \\
    &\geq \var(\thetah_1)
  \end{align*}
  as required.

  For the second part of the proof, assume $\thetah_1$ is the best unbiased
  estimator and consider the variance of the unbiased estimator $a
  \thetah_1 + (1 - a) \thetah_2$.  Then
  \begin{align*}
    \var(a \thetah_1 + (1 - a) \thetah_2)
    &= \var(\thetah_1 + (1 - a) (\thetah_2 - \thetah_1)) \\
    &= \var(\thetah_1) + (1 - a)^2 \var(\thetah_2 - \thetah_1)
    + 2 (1 - a) \cov(\thetah_1, \thetah_2 - \thetah_1).
  \end{align*}

  We can choose $a$ to minimize this variance and find that it is
  smallest for
  \begin{equation*}
    a = 1 + \frac{\cov(\thetah_1, \thetah_2 - \thetah_1)}{\var(\thetah_2 - \thetah_1)}.
  \end{equation*}
  Plugging in above gives
  \begin{equation*}
    \var(a \thetah_1 + (1 - a) \thetah_2)
    = \var(\thetah_1) - \frac{(\cov(\thetah_1, \thetah_2 - \thetah_1))^2}{\var(\thetah_2 - \thetah_1)}.
  \end{equation*}
  This last quantity is less than $\var(\thetah_1)$ (a contradiction)
  unless $\cov(\thetah_1, \thetah_2 - \thetah_1) = 0$, completing the proof.

\item It can also be useful to know the smallest possible variance
  that an unbiased estimator (or estimator in general) could have in a
  particular problem.  Then, if we have an unbiased estimator that has
  that variance, we know that it is ``the best'' estimator possible in
  a certain sense and can stop looking for better estimators.

  The Cramer-Rao lower bound gives us such a bound.
  \begin{thm}
    Suppose $X_1,...,X_n$ have a joint density with likelihood $L(\theta;
    X)$ and let $\thetah$ be an estimator of $\theta$ with finite variance and
    \begin{equation*}
      (d/d\theta) \int_\Xc \thetah(x) L(\theta; x) \dx = \int_\Xc \thetah(x) (d/d\theta) L(\theta; x) \dx
    \end{equation*}
    where $\Xc$ is the support of $X_1,...,X_n$.
    Then 
    \begin{equation*}
      \var(\thetah) \geq  \frac{((d/d\theta) \E \thetah(X))^2}{\E ((d/d\theta) \log L(\theta; X))^2}.
    \end{equation*}
  \end{thm}
  If the data are \iid\ then this result can be simplified.

  The proof is relatively straightforward and follows from the
  Cauchy-Schwarz inequality.  For any random variables $Y$ and $Z$,
  the CS inequality implies that
  \begin{equation*}
    \cov(Y, Z)^2 \leq \var(Y) \var(Z).
  \end{equation*}
  If we let $Y = \thetah$ and $Z = (d/d\theta) \log L(\theta; X)$, we get
  \begin{equation*}
    \var(\thetah) \geq
    \frac{\cov(\thetah, (d/d\theta) \log L(\theta; X))^2}
         {\var((d/d\theta) \log L(\theta; X))}
  \end{equation*}
  (as long as $\var((d/d\theta) \log L(\theta; X)) > 0$), and the result holds
  after we show that
  \begin{equation*}
    \cov(\thetah, (d/d\theta) \log L(\theta; X)) = (d/d\theta) \E \thetah
  \end{equation*}
  and
  \begin{equation*}
    \E (d/d\theta) \log L(\theta; X)) = 0,
  \end{equation*}
  which I need to fill in in the future.

  \begin{ex}
    Suppose that $X_1,...,X_n \sim \iid N(\mu, \sigma^2)$ with $\sigma^2$ known.  We can
    use the CR lower bound to find the best-case variance of an
    unbiased estimator.  Since
    \begin{equation*}
      d/d\mu \log L(\mu, \sigma^2; X) = \sum_i (x_i - \mu)/\sigma^2,
    \end{equation*}
    we have
    \begin{equation*}
      \E(d/d\mu \log L(\mu, \sigma^2; X))^2 = n/\sigma^2,
    \end{equation*}
    and the CR lower bound for any unbiased estimator of $\mu$ is
    $\sigma^2/n$.  The sample mean is unbiased and has this variance, so
    it's the best unbiased estimator.
  \end{ex}

\item It is possible that no estimator, even the best one, achieves
  the CR lower bound.

\end{itemize}

\subsection{Loss-function based optimality}

\begin{itemize}[leftmargin=0pt]

\item We can also consider other measures of optimality.  One is to
  consider the average loss associated with an estimator.  A
  \emph{loss function} $L$ is a convex function s.t. $L(0) = 0$ that
  measures the cost associated with misestimating $\theta$: the cost
  associated with the error $\thetah - \theta$ is $L(\thetah - \theta)$.  An estimator
  $\thetah_1$ has lower risk than $\thetah_2$ if
  \begin{equation*}
    \E L(\thetah_1 - \theta) \leq \E L(\thetah_2 - \theta)
  \end{equation*}
  
  A standard loss criterion is MSE, where $L(x) = x^2$.  Notice that
  MSE weights (squared) bias and variance equally:
  \begin{align*}
    \E (\thetah - \theta)^2 &= \E((\thetah - \E \thetah) + (\E \thetah - \theta))^2 \\
    &= \E(\thetah - \E \thetah)^2 + 2 \E (\thetah - \E \thetah) (\E \thetah - \theta) + (\E \thetah - \theta)^2 \\
    &= \var(\thetah) + \bias(\thetah)^2
  \end{align*}
  since the middle term in the second to last equation is zero.  Loss
  functions can also be derived directly from economic criteria like
  utility functions.

\item Loss-function based criteria are rarely decisive, in the sense
  that the best estimator can depend on the parameter values, as shown
  in the next example.
  \begin{ex}
    Suppose that $X_1,...,X_n \sim \binomial(p)$ and consider two
    estimators of $p$, $\Xb$ and $1/2$.  Then
    \begin{equation*}
      \mse(\Xb) = \bias(\Xb)^2 + \var(\Xb) = p(1-p)/n
    \end{equation*}
    and
    \begin{equation*}
      \mse(1/2) = \bias(1/2)^2 + \var(1/2) = (1/2 - p)^2.
    \end{equation*}
    When $p$ is near $1/2$, the second estimator has lower MSE.  When
    $p$ is not near $1/2$, the first estimator will typically have
    lower MSE.
  \end{ex}

  For loss-function based optimality to be useful, the researcher
  needs to weight the parameter space by importance; if $W(\theta)$ is a
  weighting function for the parameter space, we can choose the
  estimator $\thetah$ associated with the lowest average risk:
  \begin{equation*}
    \int \E L(\thetah - \theta) W(\theta) \dtheta.
  \end{equation*}

\end{itemize}

\subsection{Consistency of point estimators}

\begin{itemize}[leftmargin=0pt]

\item Consistency is an asymptotic version of unbiasedness
  \begin{defn}
    An estimator $\thetah$ is a \emph{consistent} estimator of the
    parameter $\theta$ if $\thetah \to^p \theta$ for any value of $\theta$.
  \end{defn}

  Obviously, if $\E \thetah \to \theta$ and $\var(\thetah) \to 0$ then $\thetah$ is
  consistent.

\item It is straightforward to show that Method of Moments estimators
  are consistent as well.  Suppose that
  \begin{enumerate}
  \item $X_1,...,X_n$ are independent $p$ vectors
  \item $(\E X_i,...,\E X_i^k) = g(\theta_1,...,\theta_k)$ and $g^{-1}$ is
    continuous
  \item $(1/n) \sum_{i=1}^n X_i^p$ obeys an LLN for $p = 1,...,k$,
  \end{enumerate}
  then
  \begin{equation*}
    g^{-1}\Big( (1/n) \sum_{i=1}^n X_i,..., (1/n) \sum_{i=1}^n X_i^k \Big)
    \to^p (\theta_1,...,\theta_k),
  \end{equation*}
  so the Method of Moments estimator is consistent.

\item It is less straightforward to show this, but MLEs are typically
  consistent as well under technical regularity conditions on the
  distributions of the underlying random variables.  This is one
  justification for using MLE.

\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../core_econometrics"
%%% End: 

%  LocalWords:  datasets dataset ancillarity parameterizations GMM
