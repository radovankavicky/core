% Copyright © 2013, authors of the "Econometrics Core" textbook; a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Motivation for the Likelihood Ratio Test}%
\addcontentsline{toc}{part}{Motivation for the Likelihood Ratio Test}

\begin{itemize}
\item If we know the distribution of the random sample, and want to
  test a hypothesis about the unknown parameters, we can use the
  likelihood ratio test to get a test statistic.
\item test statistic is given by
  \begin{equation*}
    Λ(x₁,…,x_n) =
    \frac{\sup_{θ ∈ Θ₀} L(θ; x₁,…,x_n)}{\sup_{θ ∈ Θ₀ ∪ Θ_a} L(θ; x₁,…,x_n)}
  \end{equation*}
\begin{itemize}
\item $L(θ; x₁,...,x_n)$ is the likelihood function
\item reject if $Λ(x₁,...,x_n) ≤ c$ where $c$ is chosen
        so that the test has the correct prespecified size.
\end{itemize}
\item you can draw some pictures connecting this to the maximum
      likelihood estimator
\end{itemize}

\subsection{Uniform LRT}

\begin{itemize}
\item suppose that $X₁,...,X_n$ is iid $uniform(0,b)$ and we want
      to test the null hypothesis $b ≥ 1$ vs $b < 1$.
\begin{itemize}
\item use pictures to motivate a lot of this
\item get the statistic
\begin{itemize}
\item calculate the likelihood: $L(b; x₁,...,x_n) = b^{-n}$ if
  $b ≥ \max_i X_i$, zero otherwise.
\begin{itemize}
\item introduce the order statistics:
\begin{description}
\item[.] $x_{(1)} = \min_i x_i$
\item[.] $x_{(2)}$ is the 2nd smallest
\item[.] through $x_{(n)} = \max_i x_i$
\end{description}
\end{itemize}
\item The denominator of the test statistic is just
  $\sup_{b > 0} L(b; x₁,...,x_n) = x_{(n)}^{-n}$
\item The numerator is also $x_{(n)}^{-n}$ as long as $x_{(n)} ≥ 1$.
  Otherwise, the numerator is 1
\begin{itemize}
\item $\sup_{b ≥ 1} L(b; x₁,...,x_n)$
\end{itemize}
\item so the likelihood ratio test statistic is
\begin{itemize}
\item 1 if $x_{(n)} ≥ 1$
\item $x_{(n)}^{-n}$ if $x_{(n)} < 1$
\end{itemize}
\item we're going to reject if $x_{(n)} ≤ c$ for some value of
          $c$ that we're going to calculate
\begin{itemize}
\item this makes intuitive sense -- if the maximum value we
            see is much less than 1, it is unlikely that $b ≥
            1$.
\item if $x_{(n)} ≥ 1$ we know that the alternative can't be true
\end{itemize}
\end{itemize}
\item get the critical value from the distribution of $x_{(n)}$
\begin{itemize}
\item we're going to reject if $x_{(n)}$ is small
\item since we know $X₁,...,X_n$ is uniform, we know the
          distribution of $X_{(n)}$
\item want to find $c$ so that $\sup_b \Pr_b[X_{(n)} ≤ c] = α$
  \begin{align*}
    \sup_{b ≥ 1} \Pr_b[X_{(n)} ≤ c]
    &= \sup_{b≥ 1} \Pr_b[X_{(n)}/b ≤ c/b] \\
    &= \sup_{b ≥ 1} \Pr_b[X₁/b ≤ c/b, ..., X_n/b ≤ c/b] \\
    &= \sup_{b ≥ 1} \Pr_b[X₁/b ≤ c/b] ⋯ \Pr_b[X_n/b ≤ c/b] \\
    &= \sup_{b ≥ 1} cⁿ/bⁿ \\
    &= cⁿ
  \end{align*}
  where we're using the fact that $X_i/b ∼ uniform(0,1)$.  So
  $c = α^{1/n}$
\end{itemize}
\item plugging in specific numbers
\begin{itemize}
\item code:
\begin{itemize}
\item n <- 10
\item crit <- 0.05\^(1/n)
\item b <- 1
\item stats <- replicate(10000, max(runif(n, 0, b)))
\item hist(stats, 150)
\item mean(stats <= crit)
\item repeat for n = 100, n = 1000, and discuss different critical values
\end{itemize}
\item suppose $n = 10$ and $α = 0.05$
\begin{itemize}
\item gives critical value of $0.05^{0.10} = 0.74$
\end{itemize}
\item calculate the maximum of the 10 numbers we draw and
          compare it to $0.74$
\begin{itemize}
\item if it is less than 0.74, we reject the null hypothesis
            that $b ≥ 1$
\item otherwise, we don't reject
\end{itemize}
\item if $n = 100$, the critical value becomes 0.97
\item as we get more observations, the rejection region grows
          and the test becomes more powerful while preserving the
          correct size.
\end{itemize}
\end{itemize}
\end{itemize}

\subsection{Linear regression LRT \textbf{:hw:}}

\section{Neyman-Pearson Lemma}

\begin{itemize}
\item Helps us think about the best possible power
\item Def of UMP: Let C be a class of tests for testing $θ ∈ Θ_0$
  against $θ ∈ Θ_0^c$.  A test in C with power function $β(θ)$ is a
  \emph{uniformly most powerful} (UMP) class C test if $β(θ) ≥ β'(θ)$
  for all $θ ∈ Θ^c$ and all $β'$ that are power functions of tests in
  C.
\item Usefulness really shows up in asymptotics
\item Often there is no UMP class C test
\begin{itemize}
\item sometimes we can get a UMP test by restricting C further
\end{itemize}
\end{itemize}

\subsection{Statement of Lemma}

\begin{itemize}
\item Consider testing $θ = θ_0$ vs $θ = θ₁$ where the pdf for each
  parameter value is $f(x; θ_i)$, $i = 1,2$, using a test with
  rejection region $R$ such that, for some $k ≥ 0$,
\begin{itemize}
\item $x ∈ R$ if $f(x; θ₁) > k f(x; θ_0)$
\item $x ∉ R$ if $f(x; θ₁) < k f(x; θ_0)$
\end{itemize}
\item Then
\begin{enumerate}
\item Any size $α$ test with such a rejection region is a UMP level $α$-test
\item If there is a size $α$ test with such a rejection region for $k
  > 0$, then every UMP level $α$ test has size $α$ and every UMP level
  $α$ test has this rejection region almost surely.
\end{enumerate}
\item Notes
\begin{itemize}
\item both are strict inequalities to allow for discrete random
         variables
\item justifies the LRT (at least for simple tests)
\item most UMP results for more complicated settings are extensions
         of this lemma
\item more or less how we think of power in econometrics
\end{itemize}
\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../finitesample"
%%% End:
