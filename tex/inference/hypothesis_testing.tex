% Copyright © 2013, authors of the "Econometrics Core" textbook; a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Introduction to hypothesis testing}%
\addcontentsline{toc}{part}{Introduction to hypothesis testing}

\section{Basic setup for testing}

\begin{itemize}

\item Often we don't care about the exact value of a particular
  unknown parameter, but only care about whether or not it falls in a
  particular range.  For example, we may only care whether a
  particular program is cost effective or not.  This is similar
  conceptually to confidence intervals, and we will see that it is a
  nearly identical problem mathematically, but the emphasis is
  somewhat different.  When constructing confidence intervals, the
  idea is to avoid unnecessarily precise (and wrong) estimates, and
  the end points are determined by the data.  Here, in hypothesis
  testing, we know the points we care about in advance, and then look
  at the data to see whether it is consistent with those parameter
  values.

\item So the setup is that we have a parameter $θ$ and are interested
  in knowing which of two scenarios is true: if $θ ∈ Θ₀$ or if $θ ∈
  Θ_a$.  The first is the \emph{null hypothesis}, which is what we
  would believe (and act on) in the absence of any data, and the
  second is the \emph{alternative hypothesis}, which is a candidate
  belief that might be true.  A \emph{hypothesis test} is a statistic
  with two possible outcomes: (1) reject the null hypothesis or (2)
  fail to reject/accept the null hypothesis.  It makes the math much
  easier if we let the test statistic take on the values 0 or 1, where
  1 indicates rejection.

  There are four possible things that can happen when we conduct a
  hypothesis test
  \begin{enumerate}
  \item We reject the null hypothesis, but it is really true.  This is
    an error.
  \item We fail to reject the null hypothesis, but it is false.  This
    is another error.
  \item We correctly reject the null hypothesis; it really is false.
  \item We correctly fail to reject the null hypothesis; it really is true.
  \end{enumerate}
  The convention is that the first error, rejecting a true hypothesis,
  is the worst possible outcome and that failing to reject a false
  hypothesis is a much less serious mistake.  When I say that this is
  the convention, it means that you need to set up your problem to
  conform with this prioritization.  If the consequences of both types
  of mistake are roughly equal, a hypothesis test may not be
  appropriate.

  These are called (unhelpfully) \emph{Type I error} and \emph{Type II
  error}.  One way to remember the difference is to let the Roman
  numeral ``I'' in Type I error stand for ``I'm screwed,'' because it
  is the worse one.

\item Some more terminology:
  \begin{defn}
    A test's \emph{power function} is the probability that the test
    will reject for each possible value of the true parameter, $θ$,
    i.e., if $δ$ is a test statistic, then its power function $β$
    satisfies
    \begin{equation*}
      β(θ) = \Pr_θ[δ = 1].
    \end{equation*}
    
    The test has \emph{size $α$} if
    \begin{equation*}
      \sup_{θ ∈ Θ₀} β(θ) = α
    \end{equation*}
    and \emph{level $α$} if
    \begin{equation*}
      \sup_{θ ∈ Θ₀} β(θ) ≤ α.
    \end{equation*}

    A test is \emph{unbiased} if $\inf_{θ ∈ Θ_a} β(θ) ≥ α$
  \end{defn}

  Note that $α$ should be set in advance of the experiment.  By
  convention, hypothesis tests that reject with size or level 0.05 are
  considered \emph{significant} rejections.  A test is \emph{valid} if
  it actually has the correct size.

\item An example may be helpful.
  \begin{ex}
    Suppose $X₁,...,X_n ∼ \iid N(θ,1)$ and we want to test the
    hypothesis $θ = 0$ against $θ = 1$, so $Θ₀ = \{0\}$ and $Θ_a =
    \{1\}$.  We can use the  standard one-sided test statistic that
    you've seen in undergrad:
    \begin{equation*}
      δ(X) = \1\{\sqrt{n} \Xb > c\}
    \end{equation*}
    where $c$ is a critical value chosen to ensure that $δ$ has size
    $α$ (which we haven't set yet).

    We can find $c$ for a given value of $α$, say $α = 0.05$.  Then
    $c$ must satisfy
    \begin{equation*}
      \Pr_θ[\sqrt{n} \Xb > c] = α
    \end{equation*}
    Since $\Pr_θ[\sqrt{n} \Xb > c] = 1 - Φ(c)$, this becomes
    \begin{equation*}
      c = Φ^{-1}(1 - α) ≈ 1.64
    \end{equation*}
    and so $δ(X) = \1\{\sqrt{n} \Xb > 1.64\}$ which is probably
    something you've seen before.

    We can now find the power function, $β(θ)$.  For any $θ ∈ \RR$,
    \begin{align*}
      β(θ) &= \Pr_θ[\sqrt{n} \Xb > 1.64] \\
      &= \Pr_θ[\sqrt{n} (\Xb - θ) > 1.64 - \sqrt{n} θ] \\
      &= 1 - Φ(1.64 - \sqrt{n} θ).
    \end{align*}
    Note that as $θ$ grows towards $∞$, $Φ(1.64 - \sqrt{n} θ)$
    converges to 0 and the power converges to 1; as $θ$ goes towards
    $-∞$, $Φ(1.64 - \sqrt{n} θ)$ converges to 1 and the power
    converges to 0.  Monotonicity implies that this test is unbiased
    (the probability that it rejects if $θ = 1$ is greater than $α$.
  \end{ex}

\item A brief aside on \pvalue s.  These are useful when you want to
  present results to an audience which may have people who have
  different ideas about the correct size of the test.  A \pvalue\
  tells you the largest size for which the test would reject.

\item There is a fundamental relationship between testing hypotheses
  and constructing confidence intervals.  Given any hypothesis test, we
  can construct a corresponding confidence interval as
  follows \citep[As described in][Section 9.2]{CB02}:

  For each $θ₀$, let $A(θ₀)$ be the acceptance region of a level $α$
  test of $H₀: θ = θ₀$.  For each sample $x$, define a set $C(x)$ in
  the parameter space by \[C(x) = \{θ₀: x ∈ A(θ₀)\}.\]  Then the
  random set $C(X)$ is a $1-α$ confidence set.\sidenote{You should
    prove this on your own for homework.  The proof follows
    immediately from the definition of each set, but it can take some
    thought and work to realize it.  Start by writing out the
    definition of a $1-α$ confidence set and then verify by set
    relationships that $C(X)$ satisfies the definition.}

  \begin{ex}
    Suppose that $X₁,…,X_n ∼ N(μ,σ²)$ and we want to construct a
    two-sided $1-α$ confidence interval for $μ$; $σ²$ is known.  We
    can start with the family of tests for the null hypothesis
    \begin{equation*}
      H₀:\ μ = μ₀ \text{\quad vs \quad} μ ≠ μ₀
    \end{equation*}
    for which we want to use the statistic
    \begin{equation*}
      δ(X) = \1\{\sqrt{n} |\Xb - μ₀| / σ > c_α\}
    \end{equation*}
    where $c_α$ is a critical value chosen so that the test has
    correct size (see the previous example).

    Then
    \begin{equation*}
      A(μ₀) = \{X : \sqrt{n} |\Xb - μ₀| / σ ≤ c_α\}
    \end{equation*}
    and
    \begin{equation*}
      C(X) = \{μ₀ : \sqrt{n} |\Xb - μ₀| / σ ≤ c_α\}.
    \end{equation*}
  \end{ex}

  Notice that
  \begin{itemize}
  \item $A(μ₀)$ has samples as its elements and is different for
    different parameter values.
  \item $C(x)$ has parameter values as its elements and is different
    for different samples.
  \end{itemize}

\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../inference"
%%% End:
