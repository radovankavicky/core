% Copyright (c) 2013-2014, Gray Calhoun.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts. A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\section{Additional exercises}

\begin{hw}
  Suppose that you are interested in the model $y_i = \beta_0 +
  x_i\beta_1 + u_i$ where $x_i$ is a random scalar, $\E(u_i \mid X) =
  0$ and the observations are i.i.d. But suppose that we do not
  observe $x_i$ directly but instead we observe $w_i = x_i + u_i$,
  where $u_i$ is another error term.
  \begin{enumerate}
  \item Prove that, if $u_i$ is perfectly correlated with $x_i$, OLS
    is essentially consistent in the following sense: let $\dot x_i =
    (x_i - \E x_i) / sd(x_i)$ and let $\dot w_i = (w_i - \E w_i) /
    sd(w_i)$. Then the OLS coefficient in the regression of $y_t$ on
    $\dot x_i$ is the same as that of the regression of $y_i$ on $\dot
    w_i$.
  \item Prove that, if $u_i$ is independent of $x_i$ and $u_i$, OLS is
    inconsistent even after standardizing the variables.
  \item Derive the MLE of $\beta_1$ under the assumption that $u_i$ and
    $u_i$ are independent mean-zero normal and determine whether it is
    consistent.
  \item Suppose now that $u_i$ and $u_i$ are independent, mean zero
    normal random variables, but now assume that we have another
    regressor $z_i = x_i + v_i$, where $v_i$ is another mean-zero,
    normal error term that's independent of $u_i$ and $u_i$. Derive
    the MLE of $\beta_1$ and determine whether it is consistent. For bonus
    points and well-deserved pride: is it asymptotically normal?
  \end{enumerate}
\end{hw}


\begin{hw}
  Consider this situation: you are interested in the effect of a
  variable $X_1$ (say, education level) on $Y$ (say, earnings), so you
  want to test whether the coefficient $\beta_1$ is significant in the
  equation
  \begin{equation*}
    Y_{i} = \beta_0 + \beta_1 X_{i1} + \vep_i.
  \end{equation*}
  But, you don't know whether or not you should include a second
  regressor, $X_2$ (maybe some measure of ability, like a test score).

  Three of the approaches we've discussed are:
  \begin{enumerate}
  \item regress $Y$ on $X_1$ and test for significance.
  \item regress $Y$ on $X_1$ and $X_2$ and test $\beta_1$ for
    significance.
  \item regress $Y$ on $X_1$ and $X_2$ and test $\beta_2$ for
    significance.
    \begin{itemize}
    \item If $\beta_2$ is significant, test $\beta_1$ for
      significance.
    \item If $\beta_2$ is \emph{not} significant, regress $Y$ on only
      $X_1$ and test $\beta_1$ for significance in that model.
    \end{itemize}
  \end{enumerate}

  Please design a Monte Carlo experiment in R that would let you study
  how well each of these approaches would work.
\end{hw}

\begin{hw}
  Suppose that we're interested in the effect of a binary treatment on
  some outcome of interest. Let $x_i$ denote the (univariate)
  treatment variable for individual $i$ and $y_i$ the outcome.
  Suppose that the DGP is
  \begin{equation}\label{eq:1}
    y_i = \beta_0 + \beta_1 x_i + \varepsilon_i
  \end{equation}
  but $\varepsilon_i$ and $x_i$ are potentially correlated.  For a
  concrete example, imagine that $x_i$ indicates whether individual
  $i$ smokes and $y_i$ is a measure of the individual's overall
  health.

  Fortunately, we have run an experiment that assigned individuals to
  either be in a treatment or a control group: let $x_i^*$ be 1 when
  the $i$th individual is in the treatment group and 0 otherwise and
  assume that
  \begin{equation}
    \Pr[x_i^* = 0] = \Pr[x_i^* = 1] = 1/2
  \end{equation}
  and $x_i^*$ is set independently of $\varepsilon_i$.

  \paragraph{The complication:} Individuals do not always comply with
  treatment. In particular,
  \begin{equation}
    \Pr[x_i = 1 \mid x_i^* = 1] = \Pr[x_i = 0 \mid x_i^* = 0] = 0.90
  \end{equation}
  and (just to complete the algebra for you)
  \begin{equation}
    \Pr[x_i = 1 \mid x_i^* = 0] = \Pr[x_i = 0 \mid x_i^* = 1] = 0.10.
  \end{equation}
  (Maintain this assumption in answering the questions below. You
  probably wouldn't know this information in real research, but there
  are times that you would.)

  \begin{enumerate}
  \item Suppose that $x_i^*$ is observed but $x_i$ is not.  This is
    the common situation where the experimenter knows whether or not
    an individual was treated but not whether or not the individual
    complied with treatment.  Derive the probability limit and
    asymptotic distribution for the OLS regression of $y_i$ on $x_i^*$
    and show that it is inconsistent for $\beta_1$.

  \item Based on your answer to the first part, how could you
    construct (asymptotic) 95\% confidence intervals for $\beta_1$?
    {\bfseries You do not necessarily need a consistent estimator of
      $\beta_1$ to construct these intervals.}  We know (by fiat) that
    $x_i^*$ and $x_i$ will different in 10\% of the sample on average,
    so you can work out the worst-case misclassification to derive
    your answer.

    If you need to impose additional assumptions to derive an answer,
    feel free to. (It's much better than no answer at all.)

  \item Now suppose that you observe $x_i$ but not $x_i^*$.  Repeat
    your steps for the previous two parts, but using $x_i$.  Show that
    the OLS regression of $y$ on $x_i$ is inconsistent for $\beta_1$
    and give a procedure for constructing 95\% confidence intervals
    for $\beta_1$ using only data on $y_i$ and $x_i$.

  \item Now suppose that both $x_i$ and $x_i^*$ are observed. Prove
    that $x_i^*$ can be used as an instrument for $x_i$ and show that
    the two-stage least squares estimator can be used to consistently
    estimate $\beta_1$ in~\eqref{eq:1}.

  \end{enumerate}
\end{hw}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../core_econometrics"
%%% End: 
