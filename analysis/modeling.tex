% Copyright (c) 2013-2014, Gray Calhoun.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\section{Modeling with linear regression}
\label{ch_modeling}
\tryinput{analysis/modeling_macros.tex}
\providecommand{\exampleratio}{?}

\subsection{What happens when the model is wrong}

\begin{itemize}[leftmargin=0pt]

\item You've heard of \emph{omitted variable bias} but we need to
  think about what that means.  Suppose that we have a single
  regressor, the DGP is
  \begin{equation*}
    y_i = \alpha_0 + \alpha_1 x_i + \alpha_2 x^2_i + \vep_i
  \end{equation*}
  where (for argument's sake) $(x_i, \vep_i) \sim \iid\ (0,I)$ with
  $\vep_i$ and $x_i$ independent.  But now suppose we estimate the
  model
  \begin{equation*}
    y_i = \betah_0 + \betah_1 x_i
  \end{equation*}
  with OLS.  What are the properties of $\betah_0$ and $\betah_1$?  We know
  that
  \begin{equation*}
    \betah =
    \begin{pmatrix}
      1 & (1/n) \sum_{i=1}^n x_i \\
      (1/n) \sum_{i=1}^n x_i & (1/n) \sum_{i=1}^n x^2_i
    \end{pmatrix}^{-1}
    \begin{pmatrix}
      (1/n) \sum_{i=1}^n (\alpha_0 + \alpha_1 x_i + \alpha_2 x_i^2 + \vep_i ) \\
      (1/n) \sum_{i=1}^n x_i (\alpha_0 + \alpha_1 x_i + \alpha_2 x_i^2 + \vep_i)
    \end{pmatrix}.
  \end{equation*}
  Since we've assumed finite variance (not really an important part of
  this example) we can apply LLNs to everything, so
  \begin{gather*}
     (1/n) \sum_{i=1}^n x_i \to^p 0 \\
     (1/n) \sum_{i=1}^n x^2_i \to^p 1 \\
     (1/n) \sum_{i=1}^n (\alpha_0 + \alpha_1 x_i + \alpha_2 x_i^2 + \vep_i ) \to^p  \alpha_0 + \alpha_1 \\
     (1/n) \sum_{i=1}^n x_i (\alpha_0 + \alpha_1 x_i + \alpha_2 x_i^2 + \vep_i) \to^p
     \alpha_1 + \alpha_2 \E x_i^3
  \end{gather*}
  so
  \begin{equation*}
    \betah \to^p
    \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}^{-1}
    \begin{pmatrix} \alpha_0 + \alpha_2 \\ \alpha_1 + \alpha_2 \end{pmatrix}
    = \begin{pmatrix} \alpha_0 + \alpha_2 \\ \alpha_1 + \alpha_2 \end{pmatrix}
  \end{equation*}

  Usually we don't care about the intercept, but we almost always care
  about the slope, and the OLS estimator for the slope is
  asymptotically biased.

\item Now, how do we interpret those coefficients?  They minimize the
  SSR among the class of models linear in $x_i$.  We can show that the
  first order conditions for
  \begin{equation*}
    \argmin_{\beta_0, \beta_1} \E (y_i - \beta_0 - \beta_1 x_i)^2 
  \end{equation*}
  are satisfied by the above $\plim$s:
  \begin{align*}
    0
    &= (\partial /\partial \beta) \E (y_i - \beta_0 - \beta_1 x_i)^2 \\
    &= - 2
    \begin{pmatrix}
      \E (y_i - \beta_0 - \beta_1 x_i) \\
      \E (y_i - \beta_0 - \beta_1 x_i) x_i
    \end{pmatrix} \\
    &= - 2
    \begin{pmatrix}
      \E (\alpha_0 + \alpha_1 x_i + \alpha_2 x_i^2 + \vep_i - \beta_0 - \beta_1 x_i) \\
      \E (\alpha_0 + \alpha_1 x_i + \alpha_2 x_i^2 + \vep_i - \beta_0 - \beta_1 x_i) x_i
    \end{pmatrix} \\
    &= - 2
    \begin{pmatrix}
      \alpha_0 + \alpha_2 - \beta_0 \\
      \alpha_1 + \alpha_2 \E x_i^3 - \beta_1
    \end{pmatrix}
  \end{align*}
  and this is satisfied only when $\beta_0 = \alpha_0 + \alpha_2$ and
  $\beta_1 = \alpha_1 + \alpha_2 \E x_i^3$.

\item Now, is this what you want?  Sometimes (for prediction) this
  isn't too bad on average (especially if $\alpha_2$ is small).  But
  usually this isn't what we want.

  But the thing to notice is that the moment and weak dependence
  conditions ensure that the OLS estimator converges to
  \emph{something} and is asymptotically normal around that parameter
  value, even if it's not what we want to estimate.

\item But we can also look at the variance of the OLS estimator here
  (where $X$ is just a column of ones and the column of $x_i$'s)
  \begin{align*}
    \E(\betah \mid X)
    &= ((1/n) X'X)^{-1}
    \begin{pmatrix}
      (1/n) \sum_{i=1}^n \E(\alpha_0 + \alpha_1 x_i + \alpha_2 x_i^2 + \vep_i \mid X) \\
      (1/n) \sum_{i=1}^n \E(x_i (\alpha_0 + \alpha_1 x_i + \alpha_2 x_i^2 + \vep_i) \mid X)
    \end{pmatrix} \\
    &= ((1/n) X'X)^{-1}
    \begin{pmatrix}
      (1/n) \sum_{i=1}^n (\alpha_0 + \alpha_1 x_i + \alpha_2 x_i^2) \\
      (1/n) \sum_{i=1}^n (\alpha_0 x_i + \alpha_1 x_i^2 + \alpha_2 x_i^3)
    \end{pmatrix}
  \end{align*}
  and so
  \begin{align*}
    \var(\betah \mid X)
    &= ((1/n) X'X)^{-1}
      (1/n^2) \sum_{i=1}^n \E(\vep_i^2 \mid X)
      \begin{pmatrix} 1 & x_i \\ x_i & x_i^2 \end{pmatrix}
      ((1/n) X'X)^{-1} \\
    &= \sigma^2 (X'X)^{-1}
  \end{align*}
  under conditional homoskedasticity.  The natural question is how
  this variance compares to that of the full model.

  To simplify comparisons, impose that $\beta_0$ is known to be zero.  Then
  the restricted variance just becomes $\sigma^2 / \sum_{i=1}^n x_i^2$ and the full
  model's variance-covariance matrix is
  \begin{equation*} \sigma^2
    \begin{pmatrix}
      \sum_{i=1}^n x_i^2 & \sum_{i=1}^n x_i^3 \\
      \sum_{i=1}^n x_i^3 & \sum_{i=1}^n x_i^4
    \end{pmatrix}^{-1}
    = \frac{\sigma^2}{\sum_{i=1}^n x_i^2 \sum_{i=1}^n x_i^4 - \big( \sum_{i=1}^n x_i^3 \big)^2}
    \begin{pmatrix}
      \sum_{i=1}^n x_i^4 & -\sum_{i=1}^n x_i^3 \\
      -\sum_{i=1}^n x_i^3 & \sum_{i=1}^n x_i^3
    \end{pmatrix}.
  \end{equation*}
  Now, we're just interested in the top left element of that matrix,
  which is
  \begin{equation*}
    \frac{\sigma^2 \sum_{i=1}^n x_i^4}%
    {\sum_{i=1}^n x_i^2 \sum_{i=1}^n x_i^4 - \big( \sum_{i=1}^n x_i^3 \big)^2}.
  \end{equation*}

  We can compare their reciprocals easily; the reciprocal of the full
  model's variance is
  \begin{equation*}
    \sum_{i=1}^n x_i^2 / \sigma^2 -
    \big( \sum_{i=1}^n x_i^3 \big)^2 \Big/ \sigma^2 \sum_{i=1}^n x_i^4
  \end{equation*}
  which is clearly less than $\sum_{i=1}^n x_i^2 / \sigma^2$, the
  reciprocal of the variance of the restricted estimator, so the
  restricted estimator has smaller conditional variance.

\item Don't go overboard.  We can come up with examples where the
  unrestricted model has lower variance too.
  \begin{hw}[Regression model with omitted exogenous variables]
    Suppose that $(Y_i, X_i, Z_i)$ are \iid\ such that
    \begin{equation*}
      \E(Y_i \mid X_i, Z_i) = \alpha' X_i + \beta' Z_i
    \end{equation*}
    and assume that $X_i$ and $Z_i$ have finite second moments and
    that $\E(Z_i \mid X_i) = 0$.  This second condition makes the OLS
    estimator of $Y$ on $X$ unbiased for $\alpha$.  Find parameter
    values for $\beta$ and $\alpha$ and $\var(Y_i \mid X_i, Z_i)$ so
    that the OLS estimator of the regression of $Y$ on $X$ has higher
    variance than the OLS estimator from the regression on $X$ and
    $Z$.

    Then verify your results by writing a short program to conduct a
    Monte Carlo study comparing the variance of each estimator.
  \end{hw}

\item There's nothing special about the original variables we're
  given, and often they need to be transformed.  We can use OLS
  whenever we have models of the form
  \begin{equation*}
    y_i = \beta_0 + \beta_1 g_1(z_i) + \cdots + \beta_k g_k(z_i) + \vep_i
  \end{equation*}
  as long as the moment and dependence conditions hold.  And, of
  course, the coefficients estimated are going to be the ones that
  minimize the expected SSR, regardless of the true DGP.

\item An extreme example of this is for categorical data.  You will
  often see this in marketing-type surveys where the numbers 1--5
  represent ``strongly agree'' through ``strongly disagree.''

  \begin{ex}[Categorical regressor]
    Suppose that $y_i$ is income for individual $i$ and $x_i$
    represents highest level of education achieved by individual $i$,
    where
    \begin{equation*}
      x_i =
      \begin{cases}
        0 & \text{if individual $i$ has not completed high school} \\
        1 & \text{completed high school but no higher degree} \\
        2 & \text{completed college but no higher degree} \\
        3 & \text{completed a master's degree or entered a PhD
          program, but no higher degree} \\
        4 & \text{completed a PhD program}.
      \end{cases}
    \end{equation*}

    It would be a mistake to regress $y_i$ on the raw values of $x_i$.
    The numbers in $x_i$ \emph{don't mean anything}, but that
    regression would impose an assumption that the difference in
    expected income between someone with no degree ($x_i = 0$) and
    someone with a high school diploma ($x_i = 1$) is exactly half the
    difference between someone with a college diploma and someone with
    a PhD.  So, obviously, estimating that sort of model is basically
    useless for interpretation, etc.  You could still interpret the
    model as a best linear predictor, just like we did earlier, but
    why?

    Instead, for this sort of model you want to create separate
    indicator variables:
    \begin{align*}
      D_{i1} &= \ind\{x_i = 1\} &
      D_{i2} &= \ind\{x_i = 2\} \\
      D_{i3} &= \ind\{x_i = 3\} &
      D_{i4} &= \ind\{x_i = 4\}
    \end{align*}
    and then estimate the model
    \begin{equation*}
      y_i = \beta_0 + \sum_{j=1}^4 \beta_j D_{ij} + \vep_i.
    \end{equation*}
    
    Now each $\beta_j$ can be interpreted as the conditional
    expectation given each educational level, and the difference in
    the coefficients gives the difference in conditional expectations.
  \end{ex}

  NB: even if you estimate the correct model (with indicator
  variables) it's likely that you'll only be able to estimate the mean
  of the population of people with college degrees, etc.  It's
  unlikely that you're going to be able to estimate the expected
  change in someone's earnings, etc., that would result from a change
  in their education level.
  
\end{itemize}

\subsection{Model selection}

[need to add material on graphics, residual plots, etc.]

\begin{itemize}[leftmargin=0pt]

\item There are lots of tools for choosing a model, ranging from
  informal tools, like eyeballing scatter plots, to formal tools that
  try to give the researcher a precise measure of ``model fit.''  Some
  are very bad, but most of them can be helpful in discovering gross
  features of a dataset.  Generally speaking, the informal tools
  (graphing, plotting, etc.) work better but take longer, so they
  might be prohibitively time consuming for datasets with many
  potential regressors.

\item As we'll see, pretty much all of the model selection tools
  perform alarmingly badly at discovering detailed features of a
  dataset.  This is easiest to show for the formal tools, obviously,
  but there's no reason to think that informal tools do any better.
  It's just hard to prove results about them.

  We'll talk about this issue later in the section, but if you are
  impatient or want more details, see the review article by
  \citet{LP05}.

\item The first model-selection statistic that anyone sees is
  $R$-square, so let's get it out of the way.

  \begin{defn}[Centered $R$-square] $R$-square measures the highest
    fraction of variance in the dependent variable that can be removed
    by a linear combination of the regressors:
    \begin{equation*}
      R^2 = 1 - SSR/SST = 1 - \veph'\veph/y'M_0y
    \end{equation*}
    with $M_0 = I - \iota \iota' / n$.
  \end{defn}

  This measure has the obvious shortcoming that adding any regressor
  will allow no less of the variance of the dependent variable to be
  removed, so $R^2$ will go up (weakly) if you add more regressors.

\item One response, then, is to use $R^2$ but add an additional
  penalty term to offset the increase that one would expect if adding
  an irrelevant regressor.  This gives \emph{adjusted $R$-square},
  $\Rb^2$, which is defined as
  \begin{equation*}
    \Rb^2 = 1 - \frac{n-1}{n-K}(1-R^2).
  \end{equation*}
  Now if we add a regressor, $K$ increases too and $\Rb^2$ may go
  down.

\item We can also increase the penalty even more, giving the AIC and
  the BIC:
  \begin{equation*}
    AIC = s^2_y (1 - R^2) e^{2K/n}
  \end{equation*}
  and
  \begin{equation*}
    BIC = s^2_y (1 - R^2) n^{2K/n}
  \end{equation*}
  where $s_y^2 = \sum_{i=1}^n (y_i - \yb)^2 / n$.\footnote{The penalty
    is higher for the BIC, so it will typically select a smaller model
    than the AIC.}
  For these measures, smaller values mean the model is a better fit.

  Usually these are presented in logs, so the AIC is often reported as
  $\ln(\veph'\veph/n) + 2K/n$ and the BIC is often reported as
  $\ln(\veph'\veph/n) + (K/n) \ln n$.

  The imagined workflow is that the researcher calculates the AIC (or
  BIC) for all of the models under consideration, then chooses the one
  that minimizes the criterion.

\item There is a problem with all of this, though.  Imagine the
  setting where you're choosing between two models and the only
  difference between the two is that the larger model has an
  additional regressor---for a specific example, imagine that your
  model selection method is to do a \ttest\ on its coefficient and
  choose the smaller model if the test fails to reject.  Step back for
  a second and remember: you always have the option of just using the
  larger model.  If the coefficient on the additional regressor is not
  zero, this model will have lower bias and may have lower variance as
  well.

  So the pre-test can only be helpful when the coefficients are small
  (but should be harmless for very large coefficients).
  Unfortunately, there is enough noise in the coefficient estimates
  that the \ttest s do extremely badly here.  The next example is
  heavily stylized but illustrates this point.

  \begin{ex}
    Consider the simplest possible setting: $X_1,\dots,X_n \sim \iid
    N(\mu,\sigma^2)$ where $\mu$ is unknown and the two models are
    $\Xb$ and
    \begin{equation*}
      \muh =
      \begin{cases}
        0   & |\Xb| \leq n^{-1/4} \\
        \Xb & |\Xb| > n^{-1/4}.
      \end{cases}
    \end{equation*}
    (this is ``Hodges's estimator''\footnote{Need to track down the
      real citation, but see \citet{LP08}.})

    The first question is to look at the asymptotic variance of each
    estimator.  Obviously, $\var(\sqrt{n} \Xb) = \sigma^2$.  But the
    asymptotic variance of $\muh$ depends on $mu$.  If $\mu \neq 0$
    then
    \begin{equation*}
      \Pr[ \Xb > n^{-1/4} ] \to 1
    \end{equation*}
    so $\muh = \Xb + o_p/\sqrt{n}$ and $\var(\sqrt{n} \muh) \to
    \sigma^2$.  On the other hand, if $\mu = 0$ then
    \begin{equation*}
      \Pr[ \Xb > n^{-1/4} ] = \Pr[ \sqrt{n} \Xb > n^{1/4} ] \to 0
    \end{equation*}
    and so $\var(\sqrt{n} \muh) \to 0$.  Combined, we have
    \begin{equation*}
      \var(\sqrt{n} \muh) \to
      \begin{cases}
        0        & \mu = 0 \\
        \sigma^2 & \mu \neq 0,
      \end{cases}
    \end{equation*}
    which looks like a clear improvement: the asymptotic variance is
    the same everywhere except at $\mu = 0$ and the estimator is much
    more precise there.

    More sophisticated mathematical arguments show that this
    conclusion is wrong.  But we'll just get some intuition from
    simulations.

    The Julia code \citep{BKS12} for the second estimator is
    straightforward:
    \renewcommand*\FancyVerbStartString{## block 1}
    \VerbatimInput{analysis/modeling.jl}
    We can also write a simple function to estimate the relative
    variance of the two estimators for any $n$ and $\mu$ through
    simulation:
    \renewcommand*\FancyVerbStartString{## block 2}
    \VerbatimInput{analysis/modeling.jl}
    So, for example, calling
    \renewcommand*\FancyVerbStartString{## block 3}
    \VerbatimInput{analysis/modeling.jl}
    returns~\exampleratio, which is the simulated estimate of $\var(\muh)
    / \var(\Xb)$ when $\mu$ is 0.1 and $n$ is 100.

    We can repeat this call for $\mu$ between $-1$ and $1$:
    \renewcommand*\FancyVerbStartString{## block 4}
    \VerbatimInput{analysis/modeling.jl}
    and the results are plotted in Figure~\ref{fig:m1} (a).  For $\mu$
    very near zero the selection model is more efficient, and for
    $\mu$ far from zero the models are essentially equivalent (the
    ratio is about 1).  But for $\mu$ in a neighborhood of 1 the
    selection model is much, much worse than the naive sample average.

    Figure~\ref{fig:m1} (b) plots the same graph for 1000 observations,
    called by
    \renewcommand*\FancyVerbStartString{## block 5}
    \VerbatimInput{analysis/modeling.jl}
    and notice that the results are qualitatively the same.  But the
    neighborhood is smaller and the relative inefficiency of the
    selection model is larger.
  \end{ex}

  \begin{figure}[t]\centering
    \begin{tabular}{c}
      \subfloat[Relative variance of sample mean and threshold estimator
      for different values of $\mu$, samples of 100 observations.]
      {\tryincludegraphics{analysis/modeling_fig1.pdf}} \\
      \subfloat[Relative variance of sample mean and threshold estimator
      for different values of $\mu$, samples of 1000 observations.]
      {\tryincludegraphics{analysis/modeling_fig2.pdf}} 
    \end{tabular}
    \caption{Relative variance of model-selection based estimation to
      the sample mean.}
    \label{fig:m1}
  \end{figure}

\item To discuss further: do we care about relative or absolute
  efficiency; show that this applies even to a realistic selection
  model.
  
\end{itemize}

\subsection{Shrinkage}
\begin{itemize}[leftmargin=0pt]
\item Why is having irrelevant regressors bad?
\begin{itemize}
\item increases the variance of our estimators
\end{itemize}
\item One approach: drop regressors
\begin{itemize}
\item sets the coefficients on some of the regressors equal to zero
\item introduces bias
\end{itemize}
\item Another approach
\begin{itemize}
\item shift the coefficients on all of the regressors closer to zero
\item decreases the variance on the coefficients
\item introduces bias as well
\end{itemize}
\end{itemize}

\paragraph{intuition}
\begin{itemize}
\item For OLS:
\begin{itemize}
\item $\betah = (X'X)^{-1}X'Y$
\item $\E(\betah \mid X) = (X'X)^{-1}X'X\beta$
\item $\var(\betah \mid X) = \sigma^2(X'X)^{-1} X'X (X'X)^{-1}$
\end{itemize}
\item Another possibility
\begin{itemize}
\item $\betah_2 = (X'X + V)^{-1}X'Y$
\begin{itemize}
\item $V$ is some positive definite matrix
\item Assume $V$ is a function of $X$
\item Say $V = X'X$ (for example); then $\betah_2 = \betah / 2$
\end{itemize}
\item $\E(\betah_2 \mid X) = (X'X + V)^{-1}X'X\beta$
\begin{itemize}
\item So the estimator is biased
\item in our example, $\E(\betah \mid X) = \beta/2$
\end{itemize}
\item $\var(\betah_2 \mid X) = \sigma^2 (X'X + V)^{-1} X'X (X'X + V)^{-1}$
\begin{itemize}
\item in our example, $\var(\betah_2 \mid X) = (\sigma^2/4) (X'X)^{-1}$
\end{itemize}
\end{itemize}
\end{itemize}

\paragraph{variance reduction of this estimator}
\begin{itemize}
\item We can see that $\var(\betah_2 \mid X) \leq \var(\betah \mid X)$:
\begin{itemize}
\item Take inverses and look at difference:
  \begin{align*}
    (\var(\betah_2 \mid X)^{-1} - (\var(\betah \mid X))^{-1}
    &= \sigma^2((X'X + V) (X'X)^{-1} (X'X + V) - X'X) ] \\
    &= \sigma^2 (X'X + V + V(X'X)^{-1} (X'X + V)) \\
    &= \sigma^2 (X'X + 2 V + V(X'X)^{-1}V) 
  \end{align*}
\item This last term is positive definite
\end{itemize}
\end{itemize}

\paragraph{The ridge regression estimator}
\begin{itemize}
\item To get a convenient expression for our estimator, we should center and scale the regressors
\begin{description}
\item[$\Xt$] centered and scaled $X$
\begin{itemize}
\item $X = [1, X_1, X_2, ..., X_{k}]$
\item $\Xt = [1, (X_1 - \i \Xb_1) / \sd(X_1), ...]$
\end{itemize}
\end{description}
\item The ridge regression estimator is
  \[ \betah_{ridge} = (\Xt'\Xt + \l I)^{-1} \Xt'Y\]
\begin{itemize}
\item For uncentered/unscaled regressors, either
\begin{itemize}
\item adjust the formula
\item transform the scaled/centered coefficients
\end{itemize}
\end{itemize}
\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../core_econometrics"
%%% End:
