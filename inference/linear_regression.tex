% Copyright © 2013, authors of the "Core Econometrics Textbook;" a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Inference on linear regression}%
\addcontentsline{toc}{part}{Inference on linear regression}

\section{Estimation of linear regression under restrictions}

\begin{itemize}

\item Let's start with a simple example (one that uses economic
  variables instead of letters; coincedentally, this is the same
  example used in \citealp[p. 81]{Gre12}).  Suppose we have a simple
  model of investment,
  \begin{equation}\label{eq:1}
    \log I_t = β₁ + β₂ i_t + β₃ Δp_t + β₄ \log Y_t + β_5 t + \ep_t
  \end{equation}
  where
  \begin{itemize}
  \item $I_t$ is investment in period $t$
  \item $i_t$ is the nominal interest rate
  \item $Δp_t$ is the rate of inflation
  \item $Y_t$ is real output
  \item $t$ is a time trend.
  \end{itemize}
  We might think that the inflation rate and the nominal interest rate
  don't matter individually, but that the real interest rate is a key
  driver of investment decisions.  That would imply that the
  investment equation should be (to a rough approximation),
  \begin{equation}\label{eq:2}
    \log I_t = β₁ + β₂ (i_t - Δp_t) + β₄ \log Y_t + β_5 t + \ep_t.
  \end{equation}
  
  We can estimate this new model by using $i_t - Δp_t$ as a regressor
  instead of $i_t$ and $Δp_t$, but we can also estimate the model by
  estimating~\eqref{eq:1} under the constraint $β₂ = - β₃$.

\item In general, this amounts to solving an optimization problem
  under a constraint.  We'll focus on linear constraints here for
  simplicity, so the estimator becomes
  \begin{equation}\label{eq:4}
    \βh = \argmin_β ∑_{i=1}^n (y_i - x_i'β)² = \argmin_β (Y - Xβ)(Y-Xβ)
  \end{equation}
  subject to the constraint
  \begin{equation}\label{eq:5}
    R β = q
  \end{equation}
  where $R$ and $q$ are an arbitrary $j × k$ matrix ($j ≤ k$) and $j ×
  1$ vector respectively that are known and set by the researcher and
  $R$ is assumed to have rank $j$.

  Most economics graduate students will have solved dozens of
  constrained optimization problems by the time they read this
  passage, so we'll just do a sketch of the
  solution.\footnote{\citet{SB94} is a reasonably comprehensive
    resource for results like these and it is typically required
    reading by graduate economics programs.}  Set up the Lagrangian
  \begin{equation}\label{eq:6}
    (Y - Xβ)'(Y - Xβ) + (Rβ - q)' λ
  \end{equation}
  and take derivatives with respect to $β$ to get the first order
  conditions
  \begin{equation}\label{eq:3}
    0 = 2X'X β^* - 2 X'Y + R'λ^*
  \end{equation}
  and the original constraint~\eqref{eq:5}, where the star indicates
  that the variable solves the constrained optimization problem.
  
  We can rewrite Equation~\eqref{eq:3} as
  \begin{equation}\label{eq:8}
    β^* = \βh - (1/2) (X'X)^{-1} R'λ^*,
  \end{equation}
  where $\βh$ is the usual OLS estimator, and premultiplying by $R$
  gives
  \begin{equation}\label{eq:7}
    Rβ^* = R \βh - (1/2) R (X'X)^{-1} R'λ^*.
  \end{equation}
  Since $Rβ^* = q$,~\eqref{eq:7} determines $λ^*$:
  \begin{equation}\label{eq:9}
    (1/2) λ^* = (R (X'X)^{-1} R')^{-1} (R \βh - q)
  \end{equation}
  and substituting~\eqref{eq:9} into~\eqref{eq:8} gives the solution,
  \begin{equation}\label{eq:10}
    β^* = \βh - (X'X)^{-1} R' (R (X'X)^{-1} R')^{-1} (R \βh - q)
  \end{equation}

\item Notice that $Xβ^*$ can be interpreted as a projection onto a
  subspace of the columns of $X$.  If we define $Z = X (X'X)^{-1} R'$
  then
  \begin{equation*}
    X β^* = (X(X'X)^{-1}X' - Z(Z'Z)Z') Y ≡ (P_X - P_Z) Y,
  \end{equation*}
  and notice that $P_X P_Z = P_Z$.  On reflection this should not be
  suprising.  If the restriction amounts to imposing that some of the
  coefficients are zero, it's obvious.  Otherwise, there's a rotation
  of the design matrix $X$ such that the restriction is equivalent to
  imposing zero on some of the coefficients in the rotated design
  matrix.

\item The restricted estimator of $σ²$ under this restriction is going
  to be $(1/(n - k + \rank(R))) ∑_i (y_i - x_i'β^*)²$

\end{itemize}

\section{Finite-sample hypothesis testing in linear regression}

\begin{itemize}

\item Suppose that we want to test an arbitrary linear hypothesis about
  $β$; ie \[R β = q\] against the alternative \[R β ≠ q\]
\begin{itemize}
\item ie $R = (1, 0, 0, ...)$ and $q=0$ gives us a test that $β₀=0$
\item $R = I$ and $q = (0,0,...,0)$ gives us a test that all of the
         coefficients are equal to zero.
\end{itemize}
\item for now, assume we have normal errors
\end{itemize}

\paragraph{change in SSR under the null}
      we can look at the change in the SSR when we impose the null
        hypothesis
\begin{itemize}
\item ie ${SSR_R - SSR \over SSR}$
\begin{itemize}
\item $SSR = ∑_i \eph²_i$
\item don't need to present this, but can be written as \[
  {(R² - R²_R) / J \over (1-R²) / (n-k-1)} \]
\end{itemize}
\item Our test is actually a scaled version of that:
  \[ F = {(SSR_R - SSR)/J \over SSR / (n-k-1)} \]
\begin{itemize}
\item $J$ is the number of restrictions (ie dimension of $q$)
\end{itemize}
\end{itemize}

\paragraph{Distribution of F under null}
      now, suppose the null is true, what is the distribution of $F$?

\paragraph{distribution of numerator}
\begin{itemize}
\item reexpress the numerator
  \begin{align*}
    SSR_R - SSR
    &= (Y - X\βh_R)'(Y - X\βh_R) - (Y - X\βh)'(Y - X\βh) \\
    &= (\βh - \βh_R)'X'X(\βh - \βh_R)
  \end{align*}
  (you're proving this for homework).  Remember that
  \begin{align*}
    \βh_R &= \βh + (X'X)^{-1} R'(R(X'X)^{-1}R')(q - R\βh) \\
    &= (q - R\βh)' (R(X'X)^{-1}R')^{-1} R(X'X)^{-1}X'X(X'X)^{-1}R' (R(X'X)^{-1}R')^{-1} (q - R\βh) \\
    &= (q - R\βh)' (R(X'X)^{-1}R')^{-1} R(X'X)^{-1} R'(R(X'X)^{-1}R')^{-1} (q - R\βh) \\
    &= (q - R\βh)' (R(X'X)^{-1}R')^{-1} (q - R\βh)
  \end{align*}
\item distribution of numerator
\begin{itemize}
\item $q - R\βh$ is normal with mean $q - Rβ$ and variance $σ²
  R(X'X)^{-1}R'$
\item under the null, this mean is zero.
\item so we have a normal divided by its variance covariance matrix...
\item so $(q - R\βh)'(R(X'X)^{-1}R')^{-1}(q - R\βh)$ equals $σ²$
  chi-square r.v. with $J$ degrees of freedom
\end{itemize}
\end{itemize}

\paragraph{distribution of denominator}
\begin{itemize}
\item just like earlier, we know that $SSR = (n-k-1) s²$ and $s²$
         and $\βh$ are independent given $X$.
\item denominator is $σ²$ times a chi-square with $n-k$
         degrees of freedom and indpendent of the numerator.
\end{itemize}

\paragraph{distribution of statistic}
       distribution of $F$
\begin{itemize}
\item $F = {σ² χ²_J / J \over σ² χ²_{n-k-1} / (n-k-1)}$ in
  distribution.
\item numerator and denominator are independent
\item so this has an $F_{J, n-k-1}$ distribution under the null.
\end{itemize}

\paragraph{Distribution of F under alternative}
\begin{itemize}
\item Denominator is not affected
\item Numerator is
\begin{itemize}
\item for $R\βh - q$ to have mean zero, we need the null to
          be true
\item otherwise the numerator will get larger
\end{itemize}
\end{itemize}

\subsection{t-test}

\begin{itemize}

\item Suppose you wanted to test a single restriction, say $β_i = b$
  for some known $b$.
\item We know that ${\βh_i - β_i \over \sqrt{s² q_i}}$ is the ratio of
  a standard normal r.v. and a chi-square/(n-k-1) random variable
\item so it is $t$ with (n-k-1) degrees of freedom
\item under the null, we know $β_i = b$, so we also have ${\βh_i - b
    \over \sqrt{s² q_i}}$
\item we can use this as a test statistic:
  \begin{itemize}
  \item calculate the value of the r.v.
  \item get the appropriate critical values from the t-distribution
    table (or the computer)
  \item reject if the statistic is farther from zero than the critical
    value
  \end{itemize}
\item If we're testing an equality, this will give us exactly the same
  test as the F-test with 1 and $n-k-1$ degrees of freedom
\item if we're testing an inequality, this test can be a little easier
  to work with.
\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../inference"
%%% End:
