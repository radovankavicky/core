% Copyright © 2013, authors of the "Econometrics Core" textbook; a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Inference on linear regression}%
\addcontentsline{toc}{part}{Inference on linear regression}

\section{Estimation of linear regression under restrictions}

\subsection{introduction}

We've been looking at the model $Y = Xβ + \ep$ or (equation by
equation) \[ y_i = x_{i,1} β₁ + ⋯ + x_{i,2} β₂ + \ep_i \]
\begin{itemize}
\item what if you thought that (say) $β₁ = β₂$
\item an example \citep[p. 81]{Gre12}; have a model of investment
  \[ \log I_t = β₁ + β₂ i_t + β₃ Δp_t + β₄ \log Y_t + β_5 t + \ep_t \]
\begin{itemize}
\item $I_t$ is investment in period $t$
\item $i_t$ is the nominal interest rate
\item $Δp_t$ is the rate of inflation
\item $Y_t$ is real output
\item time trend
\end{itemize}
\item so suppose you think that inflation and the nominal interest rate
      don't matter on their own, but combined they do matter as the
      \emph{real} interest rate
\begin{itemize}
\item model becomes
  \[ \log I_t = β₁ + β₂ (i_t - Δp_t) + β₄ \log Y_t + β_5 t + \ep_t \]
\item ie you want to estimate the first model under the restriction
  that $β₂ = - β₃$.
\item we will rewrite the restriction as $β₂ + β₃ = 0$.
\end{itemize}
\item So, how would you estimate the first equation under this
      restriction?
\end{itemize}

\subsection{LM restrictions for a simple example}

\begin{itemize}
\item to derive the OLS estimators, we minimized $∑_i (y_i - x_i'β)²$
\begin{itemize}
\item this is equivalent to MLE under normality
\end{itemize}
\item If we want to estimate the restricted model, we can just minimize
  $∑_i (y_i - x_i'β)²$ subject to the constraint that (in the previous
  example) $β₂ + β₃ = 0$.
\begin{itemize}
\item set up lagrangian: $∑_i (y_i - x_i'β)² + (β₂ + β₃)λ$
\item gives foc:
\begin{itemize}
\item $(0, 1, 1, 0) · β = 0$
\begin{itemize}
\item rewrite this as $Rβ = 0$
\end{itemize}
\item $0 = - 2 ∑_i x_i (y_i - x_i'β) + (0, 1, 1, 0)' λ$
\begin{itemize}
\item can be written $0 = 2 X'Xβ - 2 X'Y + R' λ$
\end{itemize}
\end{itemize}
\item we can start to solve:
\begin{itemize}
\item $\βh_R = \βh - 1/2 (X'X)^{-1} R' \λh_R$
\item $R \βh_R = 0$ (scalar)
\begin{itemize}
\item $= R \βh - 1/2 R (X'X)^{-1} R' \λh_R$
\end{itemize}
\end{itemize}
\item so $- \λh_R/2 = (R (X'X)^{-1} R')^{-1} R\βh$
\item $\βh_R = \βh - (X'X)^{-1} R' [R(X'X)^{-1}R']^{-1} R\βh$
\item For intuition, look at the fitted values
\end{itemize}
\end{itemize}

\paragraph{diagram that should give intuition:}

\subsection{LM restrictions on a general example}

\begin{itemize}
\item suppose now that we have the restriction $Rβ = q$
\item take first order conditions for the lagrangian
  \[ (Y - Xβ)'(Y - Xβ) + (Rβ - q)'λ \] (now $λ$ is a vector)
\begin{itemize}
\item $Rβ = q$
\item $0 = 2 X'X - 2 X'Y + R' λ$
\end{itemize}
\item mimic the same steps as before
\begin{itemize}
\item $\βh_R = \βh - \frac12 (X'X)^{-1} R'\λh_R$
\item $-\frac12 \λh_R = (R(X'X)^{-1}R')^{-1}(q - R\βh)$
\item $\βh_R = \βh + (X'X)^{-1} R'(R(X'X)^{-1}R')^{-1} (q - R\βh)$
\end{itemize}
\end{itemize}

\subsection{remarks}

\begin{itemize}
\item this is also the \textbf{restricted MLE} under normality
\item estimator of $σ²$ under this restriction is going to be
  $\ov{n - k - 1 + rank(R)} ∑_i (y_i - x_i'\βh_R)²$
\end{itemize}

\section{Hypothesis testing in finite samples}

\begin{itemize}
\item Reading for this material: \citet{Gre12} 4.7, 5.1-5.3
\end{itemize}

\subsection{introduction}

\begin{itemize}
\item Suppose that we want to test an arbitrary linear hypothesis about
  $β$; ie \[R β = q\] against the alternative \[R β ≠ q\]
\begin{itemize}
\item ie $R = (1, 0, 0, ...)$ and $q=0$ gives us a test that $β₀=0$
\item $R = I$ and $q = (0,0,...,0)$ gives us a test that all of the
         coefficients are equal to zero.
\end{itemize}
\item for now, assume we have normal errors
\end{itemize}

\subsection{a convenient test}

\paragraph{change in SSR under the null}
      we can look at the change in the SSR when we impose the null
        hypothesis
\begin{itemize}
\item ie ${SSR_R - SSR \over SSR}$
\begin{itemize}
\item $SSR = ∑_i \eph²_i$
\item don't need to present this, but can be written as \[
  {(R² - R²_R) / J \over (1-R²) / (n-k-1)} \]
\end{itemize}
\item Our test is actually a scaled version of that:
  \[ F = {(SSR_R - SSR)/J \over SSR / (n-k-1)} \]
\begin{itemize}
\item $J$ is the number of restrictions (ie dimension of $q$)
\end{itemize}
\end{itemize}

\paragraph{Distribution of F under null}
      now, suppose the null is true, what is the distribution of $F$?

\paragraph{distribution of numerator}
\begin{itemize}
\item reexpress the numerator
  \begin{align*}
    SSR_R - SSR
    &= (Y - X\βh_R)'(Y - X\βh_R) - (Y - X\βh)'(Y - X\βh) \\
    &= (\βh - \βh_R)'X'X(\βh - \βh_R)
  \end{align*}
  (you're proving this for homework).  Remember that
  \begin{align*}
    \βh_R &= \βh + (X'X)^{-1} R'(R(X'X)^{-1}R')(q - R\βh) \\
    &= (q - R\βh)' (R(X'X)^{-1}R')^{-1} R(X'X)^{-1}X'X(X'X)^{-1}R' (R(X'X)^{-1}R')^{-1} (q - R\βh) \\
    &= (q - R\βh)' (R(X'X)^{-1}R')^{-1} R(X'X)^{-1} R'(R(X'X)^{-1}R')^{-1} (q - R\βh) \\
    &= (q - R\βh)' (R(X'X)^{-1}R')^{-1} (q - R\βh)
  \end{align*}
\item distribution of numerator
\begin{itemize}
\item $q - R\βh$ is normal with mean $q - Rβ$ and variance $σ²
  R(X'X)^{-1}R'$
\item under the null, this mean is zero.
\item so we have a normal divided by its variance covariance matrix...
\item so $(q - R\βh)'(R(X'X)^{-1}R')^{-1}(q - R\βh)$ equals $σ²$
  chi-square r.v. with $J$ degrees of freedom
\end{itemize}
\end{itemize}

\paragraph{distribution of denominator}
\begin{itemize}
\item just like earlier, we know that $SSR = (n-k-1) s²$ and $s²$
         and $\βh$ are independent given $X$.
\item denominator is $σ²$ times a chi-square with $n-k-1$
         degrees of freedom and indpendent of the numerator.
\end{itemize}

\paragraph{distribution of statistic}
       distribution of $F$
\begin{itemize}
\item $F = {σ² χ²_J / J \over σ² χ²_{n-k-1} / (n-k-1)}$ in
  distribution.
\item numerator and denominator are independent
\item so this has an $F_{J, n-k-1}$ distribution under the null.
\end{itemize}

\paragraph{Distribution of F under alternative}
\begin{itemize}
\item Denominator is not affected
\item Numerator is
\begin{itemize}
\item for $R\βh - q$ to have mean zero, we need the null to
          be true
\item otherwise the numerator will get larger
\end{itemize}
\end{itemize}

\subsection{t-test}

\begin{itemize}

\item Suppose you wanted to test a single restriction, say $β_i = b$
  for some known $b$.
\item We know that ${\βh_i - β_i \over \sqrt{s² q_i}}$ is the ratio of
  a standard normal r.v. and a chi-square/(n-k-1) random variable
\item so it is $t$ with (n-k-1) degrees of freedom
\item under the null, we know $β_i = b$, so we also have ${\βh_i - b
    \over \sqrt{s² q_i}}$
\item we can use this as a test statistic:
  \begin{itemize}
  \item calculate the value of the r.v.
  \item get the appropriate critical values from the t-distribution
    table (or the computer)
  \item reject if the statistic is farther from zero than the critical
    value
  \end{itemize}
\item If we're testing an equality, this will give us exactly the same
  test as the F-test with 1 and $n-k-1$ degrees of freedom
\item if we're testing an inequality, this test can be a little easier
  to work with.
\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../inference"
%%% End:
