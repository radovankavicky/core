% Copyright (c) 2013, authors of "Core Econometrics;" a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Properties of random variables}%
\addcontentsline{toc}{part}{Properties of random variables}

\section{Expectation}

\begin{itemize}
\item There are a few ways to think about the \emph{expectation} of a
  random variable.  One of the more natural is to think about many
  independent repeated draws, $X₁, X₂,…,X_n$, from the same density
  function $f$—think of them as repeated measurements of some physical
  quantity, like a weight or a height.  Under some often plausible
  assumptions (that will be discussed later) these measurements
  will cluster around a particular point $μ$ in the following sense:
  as the number of observations grows ($n → ∞$), the average of the
  observations will settle down and converge to $μ$.  If the
  measurements are unbiased, then $μ$ will be the true quantity being
  measured; if they are unbiased, $μ$ will reflect both the true
  quantity and the systematic measurement error.  In either case, $μ$
  is the \emph{expected value} of the density $f$.

  Formally, write the expected value of the density $f$ as $\E X$
  where $X ∼ f$ is a representative draw from the distribution, and
  define the expected value as
  \begin{equation}\label{eq:3}
    \E X = ∫ x f(x) \dx.
  \end{equation}
  For a random vector $X = (X₁,…,X_k)$, the expected value is just the
  vector of each element's expected value:
  $\E X = (\E X₁,… \E X_k)'$

  Now, the expected value does not necessarily exist for a given
  random variable: if the density's tails are too fat, the integral
  in~\eqref{eq:3} can be infinite.  Generally, if $\E |X| = ∞$ then
  the mean of $X$ is said to not exist.

\item The expected value of a function of $X$, $\E g(X)$, is
  surprisingly straightforward and does not require the density of
  $g(X)$ to be calculated:
  \begin{equation*}
    \E g(X) = ∫ g(x) f(x) \d x.
  \end{equation*}
  This can be verified in special cases from the formula for the
  density of $g(X)$.  For the general case, this result follows from
  taking the limit of the expectation of step functions (it can be
  shown that the expectation operator is closed under certain limits,
  but we won't cover those properties here).

\item Note that the expectation of an indicator function is just the
  probability of the event represented by that indicator function:
  \begin{equation*}
    \E \1\{ X ∈ A \} = \Pr[X ∈ A]
  \end{equation*}

  Also notice a fundamental and important property of the expectation
  operator, linearity.  For any constants $a$ and $b$, we have
  \begin{equation*}
    \E(a X + b) = a \E X + b
  \end{equation*}

\item For a random vector, we define the expecation as the vector of
  expectations of the individual elements:
  \begin{equation*}
    \E X = (\E X₁,...,\E X_k)
  \end{equation*}

\item It is straightforward to see that if $X$ and $Y$ are
  independent then $\E XY = \E X \E Y$.  For proof:
  \begin{align*}
    \E XY
    &= ∫∫ x y f_{X,Y}(x,y) \dx \dy \\
    &= ∫∫ xy f_X(x) f_Y(y) \dx \dy \\
    &= ∫ x f_X(x) \dx ∫ y f_Y(y) \dy \\
    &= \E X \E Y.
  \end{align*}

\item An especially useful inequality based on the expectation is
  \emph{Jensen's inequality}.
  \begin{thm}
    For any random variable $X$, $\E g(X) ≥ g(\E X)$ for any convex
    $g$.
  \end{thm}

  Remember the definition of a convex function:
  \begin{defn}
    A function $g(x)$ is convex if 
    \begin{equation*}
      g( λ x + (1-λ) y ) ≤ λ g(x) + (1-λ) g(y) 
    \end{equation*}
    for all $λ ∈ [0,1]$ and any $x$ and $y$.
  \end{defn}

  [Add a picture.]

\end{itemize}

\section{Conditional expectation}

\begin{itemize}

\item We can also (obviously) take the expectation of conditional
  densities.  If $X$ and $Y$ are two random vectors, the conditional
  expectation of $Y$ given $X$ is
  \begin{equation*}
    \E(Y ∣ x) = ∫ y f_Y(y ∣ x) \dy
  \end{equation*}
  where $f_Y$ is the density of $Y$.\sidenote{There are other, more
  fundamental ways to derive conditional expectations that we're not
  going to worry about.}  When we don't want to focus on a specific
  value of $X$, we just write $\E(Y ∣ X)$ (note that this is now a
  function of the random variable $X$).

\item The conditional expectation has some important properties (these
  properties actually make a lot more sense if you view the
  conditional expectation as a projection):
  \begin{enumerate}
  \item If $Y = g(X)$ for some function $g$ then $\E(Y ∣ X) = Y$.
    This occurs because the conditional density of $Y$ given $X$ is
    degenerate.
  \item Similarly, If $X = (X₁, X₂)$ then we have the relationships
    \begin{equation*}
      \E(\E(Y ∣ X) ∣ X₁) = \E(Y ∣ X₁) \quad\text{a.s.}
    \end{equation*}
    and
    \begin{equation*}
      \E(\E(Y ∣ X₁) ∣ X) = \E(Y ∣ X₁) \quad\text{a.s.}
    \end{equation*}
    Here it's important to remember the connection between random
    variables and information sets.  The r.v. $X₁$ contains less
    information than $X$, so when we first condition on $X₁$ to get
    $\E(Y ∣ X₁)$, that step removes (integrates out) the information
    in $X₂$.  A second conditioning step can not restore that
    information.
  \item $\E(X₁ Y ∣ X) = X₁ \E(Y ∣ X)$; again, given the information in
    $X$, we can view $X₁$ as a known constant.  The result then
    follows from linearity of the expectation.
  \item The Law of Iterated Expectations
    \begin{thm}
      Suppose that $X$ and $Y$ are random variables and $Y$ has finite
      mean; then $\E \E(Y ∣ X) = \E Y$.
    \end{thm}
    The proof (assuming densities, etc) is pretty straightforward
    \begin{align*}
      \E Y &= ∫ y f_Y(y) \dy \\
      &= ∫ ∫ y f_{X,Y}(x,y) \dx \dy \\
      &= ∫ ∫ y f_Y(y ∣ X = x) f_X(x) \dx \dy \\
      &= ∫ ∫ y f_Y(y ∣ X = x) \dy f_X(x) \dx \\
      &= ∫ \E(Y ∣ X = x) f_X(x) \dx \\
      &= \E \E(Y ∣ X)
    \end{align*}
  \end{enumerate}

\end{itemize}

\section{General measures of the center of a density}

\begin{itemize}

\item The mean has lots of advantages; it is mathematically convenient
  to work with and shows up often because of the Law of Large Numbers.
  But it has disadvantages too; it is extremely sensitive to events in
  the tails of the distribution.  So sometimes other measures of
  location are useful

\item %
  \begin{defn}
    The \emph{median} of a distribution $F$ is the value $c$ such that
    $\Pr[X ≤ c] ≥ 1/2$ and $\Pr[X ≥ c] ≥ 1/2$.
  \end{defn}
  Note that this definition means that the median may not be unique if
  $X$ is discrete.

\item %
  \begin{defn}
    The \emph{mode} of a distribution is the value that maximizes
    the density function $f_X$.
  \end{defn}
  The mode is not necessarily unique either.

\end{itemize}

\section{Measures of dispersion and other moments}

\begin{itemize}

\item The variance measures a random variable's dispersion about its
  mean, and is defined as
  \begin{equation*}
    \var(X) = \E(X - \E X)².
  \end{equation*}
  An equivalent formula is $\var(X) = \E(X²) - (\E X)²$, which can be
  easier to use at times.  Obviously the variance is nonnegative (it
  will be zero if $X$ is a constant).

  The standard deviation is the square root of the variance, which has
  the advantage that it's in the same units as $X$.

\item We can generalize the variance to measure the linear
  relationship between two variables.  Define the \emph{covariance} as
  \begin{equation*}
    \cov(X,Y) = \E(X - \E X) (Y - \E Y) = \E XY - \E X \E Y
  \end{equation*}
  and the \emph{correlation} as
  \begin{equation*}
    \corr(X, Y) = \cov(X,Y) / \sqrt{\var(X)} \sqrt{\var(Y)}.
  \end{equation*}

  The Cauchy-Schwarz inequality ensures that the correlation is
  between $-1$ and 1.
  \begin{thm}[Cauchy-Schwarz inequality]
    Let $X$ and $Y$ be random variables.  Then 
    \begin{equation}\label{eq:5}
      \E (|X Y|) ≤ \sqrt{\E X²} \sqrt{\E Y²}
    \end{equation}
  \end{thm}
  To prove this result, we're going to show that it is equivalent to
  something like $0 ≤ something²$;~\eqref{eq:5} is equivalent to
  \begin{equation*}
    0 ≤ \E X² - (\E |X Y|)² / \E Y²
  \end{equation*}
  as long as $Y$ is not identically zero (if $Y$ is identically zero
  then the result holds trivially).  Then
  \begin{align*}
    \E X² - (\E |X Y|)² / \E Y²
    &= \E X² - 2 (\E |XY|)²/ \E Y² + (\E |XY|)²/ \E Y² \\
    &= \E X² - 2(\E |XY| / \E Y²) \E |XY| + (\E |XY|/\E Y²)² \E Y² \\
    &= \E(X² - 2(\E |XY| / \E Y²) |X| |Y| + (\E |XY|/\E Y²)² Y²) \\
    &= \E(|X| - (\E |XY|/\E Y²) |Y|)²
  \end{align*}
  and this last quantity is clearly nonnegative.

\item The variance obeys a relationship similar to the Law of the
  Iterated Expectation:
  \begin{equation*}
    \var(X) = \var(\E(X|Y)) + \E(\var(X|Y)).
  \end{equation*}
  There are two implications of this formula: first, that a
  conditional expectation has lower variance than its original random
  variable; and second, that conditioning decreases the variance on
  average, but not almost surely.

\item We can define the variance for vectors as well: $\var(X) = \E(X
  - \E X) (X - \E X)'$.  For vectors, $\var(X)$ is called the
  \emph{variance-covariance matrix}.  Observe that if $X$ has
  variance-covariance matrix $Σ$ and is $A$ a deterministic $j × k$
  matrix then $A X$ has variance-covariance matrix $A Σ A'$.

\item We can verify that the variance-covariance matrix is always
  positive semi-definite, i.e. that $α'\var(X)α ≥ 0$ for any nonzero
  $α$:
  \begin{align*}
    α'\var(X)α &= α' \E((X - \E X)(X - \E X)') α \\
    &= \E α'(X - \E X)(X - \E X)'α \\
    &= \E (α'X - \E(α'X))² \\
    &= \var(α'X) ≥ 0
  \end{align*}

\item In general, we can refer to the $k$th moment or central moment.
  The $k$th moment of a random variable $X$ is just $\E X^k$ and it is
  said to exist if $\E |X|^k$ is finite.  The $k$th central moment is
  defined as $\E |X - \E X|^k$.

\item As a consequence of Jensen's inequality, we have the following
  result: if the $p$th moment of a random variable is finite, so are
  all moments between 0 and $p$.

  For a proof, let $q$ be between 0 and $p$ and observe that $g(x) ≡
  x^{p/q}$ is convex.  So, for any $Y$, Jensen's inequality implies
  that $\E g(Y) ≥ g(\E Y)$.  Now, let $Y = |X|^q$, so
  \begin{equation}\label{eq:4}
    \E g(|X|^q) ≥ g(\E |X|^q).
  \end{equation}
  But $\E g(|X|^q) = \E |X|^p$ and $g(\E |X|^q) = (\E |X|^q)^{p/q}$.
  Raising both sides of Equation~\eqref{eq:4} to the $q/p$ power
  completes the proof.

\item The first four moments can be interpreted to some degree; we
  have already discussed the first two.  The random variable $X$'s
  \emph{skewness} is its normalized third moment, $\E(X - \E X)³ /
  σ³$, but you will sometimes see slightly different definitions.
  Skewness measures the asymmetry of the density function: if the
  density is perfectly symmetric, it is zero; if there is a long right
  tail, the skewness is positive, etc.

  $X$'s \emph{kurtosis} is its normalized fourth moment, $\E(X - \E
  X)⁴ / σ⁴$, which measures to some extent the thickness of the tails
  (higher kurtosis implies thicker tails).  It is sometimes useful to
  discuss the \emph{excess kurtosis}, which is $\E(X - \E X)⁴ / σ⁴ -
  3$; ``3'' is the kurtosis of the normal distribution.

\end{itemize}

\section{General transformations of random variables}

\begin{itemize}

\item Notice that, if $g$ is a Borel-measurable function from $\RR$ to
  $\RR$, then $g(X)$ is a random variable as well.  For any $B ∈ \BB$,
  we know that $g^{-1}(B) ∈ \BB$ by measurability of $g$, and so
  $X^{-1}(g^{-1}(B)) ∈ \Fs$ by measurability of $X$.

  This can be useful when $g$ is smooth and when $X$ has a known
  density function.  If we start with the distribution functions, let
  $H$ be the CDF of $g(X)$ and $F$ the CDF of $X$.  Then, if $g$ is
  invertible and monotone increasing,
  \begin{equation*}
    H(c) = \Pr[g(X) ≤ c] = \Pr[X ≤ g^{-1} c] = F(g^{-1}(c)).
  \end{equation*}
  Now, assuming all of these operations go through, we can write the
  density of $g(X)$ as
  \begin{align*}
    h(z) &= (d/dz) H(z) \\
    &= (d/dz) F(g^{-1}(z)) \\
    &= f(g^{-1}(z)) (d/dz) g^{-1}(z).
  \end{align*}

  If $g$ is monotone decreasing, you can do almost exactly the same
  operations but need to account for the different sign.  More
  generally, it might be necessary to partition the domain of $g$ into
  segments where $g$ is monotone and work with the individual pieces.

  A reasonably formal but concise statement of this result is:
  \begin{thm}
    Suppose that $Y = g(X)$, where $X$ is a random variable with density
    $f_X$ and $g$ is a continuous, monotone function with continuously
    differentiable inverse.  The density of $Y$, $f_Y$, is given by the
    equation
    \begin{equation}
      f_Y(y) = f_X(g^{-1}(y)) | (d/dy) g^{-1}(y) |
    \end{equation}
    for $y$ in the range of $g$ and zero elsewhere.
  \end{thm}

\item \emph{Location-scale families} are a useful special case of the
  transformation formula.  If $X$ is a random variable with density
  $f_X$ and $f_Y$ is the density of $Y = σ X + μ$ for some constant
  $μ$ and positive constant $σ$, then
  \begin{equation*}
    f_Y(x) = \tfrac{1}{σ} f_X\big( \tfrac{x - μ}{σ} \big)
  \end{equation*}
  for all $x$.\footnote{Verify on your own that this follows from the
  general transformation theorem.}

  This family of pdfs is called the ``location-scale family with
  standard pdf $f_X(x)$.''
  \begin{itemize}
  \item $μ$ is the \emph{location parameter}
  \item $σ$ is the \emph{scale parameter}.
  \end{itemize}
  If only $μ$ or $σ$ varies then we have either a \emph{location
  family} or a \emph{scale family}.

\item The math also becomes more painful if we are dealing with random
  vectors.  A general theory is very painful to work with because of
  dimension changes in particular, but working with one-to-one
  transformations from $\RRᵏ$ to $\RRᵏ$ is reasonably straightforward.
  First we need a definition:
  
  \begin{defn}
    Let $g: \RRᵏ → \RRᵏ$ be a differentiable function.  The Jacobian
    of this function is the matrix of partial derivatives
    \begin{equation*}
      J(x) =
      \begin{pmatrix} ∂g₁(x)/∂x₁ & ∂g₁(x)/∂x₂ & ⋯ & ∂g₁/∂x_k \\
      ⋮ \\
      ∂g_k/∂x₁ & ⋯ & ⋯ & ∂g_k/∂x_k
      \end{pmatrix}.
    \end{equation*}
  \end{defn}
      
  Then the result is (note that this is the version presented by
  \citealp[B.7.7]{Gre12})
  \begin{thm}
    Suppose that $X$ is a random vector in $\RR^k$ with joint density
    $f_X$ and that $g: \RRᵏ → \RRᵏ$ is one-to-one from the support of
    $X$ to its image and let $Y = g(X)$.  Then the joint density of $Y$ is
    \begin{equation}
      f_Y(y) = f_X(g^{-1}(y)) \abs(\det( J(y) ))
    \end{equation}
    (where $J(y)$ is the Jacobian of $g^{-1}$) in the image of the support
    of $X$ and is zero elsewhere.
  \end{thm}

\item For transformations from a higher dimension to a lower
  dimension, one can sometimes create an artificial transformation
  that's easy to work out, then integrate out the other dimension.

\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../probability"
%%% End: 
%  LocalWords:  nonnegative kth skewness
