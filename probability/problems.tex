% Copyright (c) 2013â€“2014, authors of "Core Econometrics;" a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts. A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\chapter{Problems}

\begin{hw}
  When $X$ has a continuous distribution $F$ and $U \sim$
  uniform(0,1), $F(X) =^d U$ and, consequently, $X =^d F^{-1}(U)$ (see
  \citealt[Theorem 2.1.10]{CB02} for a proof). Use this result to
  write an R function called \texttt{rtan} that generates $n$ random
  variables from the location-scale family $\mu + \sigma X$, where $X
  \sim F$ and
  \begin{equation*}
    F(x) = 0.5 + \tan^{-1}(x)/\pi.
  \end{equation*}
  The arguments of the function should include $n$, $\mu$, and $\sigma$.
\end{hw}

\begin{hw}
  You can also generate random variables indirectly. If $f_X$ has
  support $(0,1)$ and $c = \max_x f_X(x)$, then the following
  algorithm will draw $X$ from the density $f_X$ \citep[see][Section
  5.6.2]{CB02}
  \begin{enumerate}
  \item Draw $U$ and $V$ as independent uniform(0,1).
  \item If $U \leq f_X(V) / c$, set $X = V$. Otherwise draw $U$ and $V$
    again.
  \item One case where we might need to use this algorithm is if we
    wanted to draw $X$ and $Y$ that satisfy
    \begin{align*}
      \binom{X}{Y} &\sim N(\mu,\Sigma) & \text{s.t.}&& X^2 + Y^2 &= r^2,
    \end{align*}
    where $r$ is some known scalar. Note that the probability that
    the constraint holds is zero, so we can't just simulate $(X,Y)$
    from the normal distribution and discard pairs that don't satisfy
    it. Use the accept-reject algorithm to write an $r$ function
    \texttt{rpair} that simulates $n$ observations from this
    conditional distribution ($\mu$, $\Sigma$ and $r$ should all be arguments
    of the function).
  \end{enumerate}
\end{hw}

\begin{hw}
  Let $X$ and $Y$ be two random variables with the joint density
  function
  \begin{equation*}
    f(x, y) = \begin{cases}
      \min(6 y, 6 - 6y, 6 x, 6 - 6 x) & \text{if } (x, y) \in [0,1]^2 \\
      0 & \text{otherwise.}
    \end{cases}
  \end{equation*}
  (this looks like a pyramid centered at (1/2, 1/2)).
  \begin{enumerate}
  \item Derive the joint distribution of $X$ and $Y$.
  \item Derive the marginal distribution and density of $X$
  \item Derive the conditional distribution and density of $Y$ given
    $X$.
  \item We're going to write two different R functions to generate
    draws from this density function. The first is going to use the
    analytic work you've done in the previous few questions, and the
    second uses a trick. For both, please write a function that takes
    the number of observations to generate as its only argument and
    generate a contour plot of 10000 draws from the function.
    \begin{enumerate}
    \item Invert the distribution function of $X$ and the conditional
      distribution function of $Y$ given $X$. You can then generate
      two independent uniform random variables $U$ and $V$ and and
      pass them to the inverted distribution functions. Probably the
      best way to do this is to create a function \texttt{rX} that
      generates $X$ and another function \texttt{rY} that takes $X$ as
      an argument and generates a draw from the conditional
      distribution of $Y$.
    \item You can also generate random variables indirectly using the
      accept-reject algorithm \citep[Section 5.6.2]{CB02}. Let $c =
      \max_{x,y} f(x,y)$ and do the following
      \begin{enumerate}
      \item Draw $U$, $V$ and $W$ as independent uniform(0,1).
      \item If $U \leq f(V,W) / c$, set $X=V$ and $Y=W$. Otherwise draw
        $U$, $V$, and $W$ again.
      \item Repeat until you accept candidate values of $X$ and
        $Y$. That is a single draw from the target density.
      \end{enumerate}
    \end{enumerate}

    The first approach will execute faster, but can be impractical (or
    even impossible) when the density is complicated. The second
    approach is sometimes slow but can usually be applied (and when it
    can't, there are other similar algorithms that can be applied more
    generally).
  \end{enumerate}
\end{hw}

\begin{hw}
  Suppose that $X$ and $Y$ are random variables with finite second
  moments such that $\E(Y \mid X) = \alpha + \beta X$. Prove that
  \begin{align}
    \alpha &= \E Y - \beta \E X &\text{and}&& \beta &= \frac{\cov(X, Y)}{\var(X)}.
  \end{align}
\end{hw}

\begin{hw}
  Suppose that $X_1,\dots,X_n$ are an i.i.d. sample from the continuous
  distribution $F$. What is the distribution of $\max_i X_i$? Derive
  the joint distribution of $\min_i X_i$ and $\max_i X_i$.
\end{hw}

\begin{hw}
  Let $X$ and $Y$ be random variables. Prove that 
  \begin{equation}
    \var(Y) = \var(\E(Y \mid X)) + \E \var(Y \mid X)
  \end{equation}
\end{hw}

\begin{hw}
  Suppose that $X_1$ and $X_2$ are independent univariate Normal
  random variables. Prove that $X_1 + X_2$ is Normal and find its mean
  and variance.
\end{hw}

\begin{hw}
  Let $X \sim N(0,I_n)$ and let $P$ be a symmetric $n \times n$ matrix.
  Prove that $X'PX$ is chi-square with $k$ degrees of freedom if and
  only if $P$ is idempotent with rank $k$ \citep{SL03}.
\end{hw}

\begin{hw}
  Suppose that $X_1 \sim$ beta$(\alpha, \beta)$ and $X_2 \sim$ beta$(\gamma, \delta)$ and
  they are independent. Derive the density of $X_1 \cdot X_2$.
\end{hw}

\begin{hw}
  Let $\{e_t\}$ be a sequence of i.i.d. $(0,1)$ random variables
  and let $X_t = e_t + 0.5\, e_{t-1}$. Such a process is called a
  Moving Average of order 1, abbreviated as MA(1). What is the
  probability limit of $n^{-1} \sum_{t=1}^n X_t$ as $n \to \infty$?
\end{hw}

\begin{hw}
  Let $\{e_t\}$ be a sequence of i.i.d. $(0,1)$ random variables
  and let $X_t = e_t + 0.5\, e_{t-1}$. Such a process is called a
  Moving Average of order 1, abbreviated as MA(1). What is the
  probability limit of $n^{-1} \sum_{t=1}^n X_t$ as $n \to \infty$?
\end{hw}

\begin{hw}
  Suppose that $Z$ is a random variable with mean $\mu$ and variance
  $\sigma$ and let $e_1,e_2,...$ be a sequence of i.i.d. random
  variables with mean zero and variance 1 that are independent of $Z$.
  Show that the sequence $X_i = Z + e_i$ obeys the following weak law
  of large numbers: as $n \to \infty$, $n^{-1} \sum_{i=1}^n ( X_i -
  \E(X_i \mid Z) ) \to 0$ in probability. Note that $X_1,X_2,...$ is
  \emph{not} an i.i.d.\ sequence, but is an example of an
  \emph{exchangeable} sequence of random variables.
\end{hw}

\begin{hw}
  Suppose that $X_n \to^p X$ and that $g$ is a continuous function.
  Prove that $g(X_n) \to^p g(X)$.
\end{hw}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../core_econometrics"
%%% End: 