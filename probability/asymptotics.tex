% Copyright (c) 2013â€“2014, authors of "Core Econometrics;" a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\chapter{Convergence concepts for sequences of random variables}

\section{Convergence in $L_p$}

\begin{itemize}[leftmargin=0pt]
\item Averages have interesting properties as the number of
  observations increases.  For many DGPs, the sample average collapses
  to a point, the population mean, and the difference between the
  statistic and its limit has an approximately known distribution when
  it is rescaled to offset the degeneracy---the normal distribution.
  These results are known as the \emph{Law of Large Numbers} (LLN) and
  the \emph{Central Limit Theorem} (CLT) respectively but that
  terminology is a little misleading since ``the LLN'' refers to any
  one of a family of theorems that show the sample average converges
  to the population mean and same for ``the CLT.''

  For either of these results to make sense, we need to define what it
  means for a sequence of random variables $\{\Xb_n\}_{n=1}^\infty$
  to converge (where $\Xb_n = (1/n) \sum_{i=1}^n X_i$ and
  $\{X_i\}_{i=1}^\infty$ is another sequence of random variables).

\item The simplest version of convergence is to say that $\Xb_n$
  converges to some quantity $\mu$ if $\E \Xb_n$ converges to $\mu$ and
  $\var(\Xb_n)$ converges to zero as $n \to \infty$.  These conditions are
  equivalent to $\E(\Xb_n - \mu)^2$ converging to zero as $n \to \infty$, which
  is called ``convergence in mean square'' or ``convergence in $L_2$''
  for reasons that will become immediately clear.

\item Notice that $(\E(X - Y)^p)^{1/p} \equiv \lVert X - Y \rVert_p$ can be
  viewed as a distance between the random variables $X$ and $Y$.  The
  operation $\lVert \cdot \rVert_p$ denotes the \emph{$L_p$-norm} of its
  argument for $p \geq 1$.\footnote{Remember that a \emph{norm} is an
    abstraction of the idea of length.  A norm is a function $\rho$ (on
    a vector space) that satisfies
    \begin{enumerate}
    \item $\rho(a v) = |a| \rho(v)$ for any scalar $a$,
    \item $\rho(u + v) \leq \rho(u) + \rho(v)$ (i.e. the triangle inequality),
    \item $\rho(0) = 0$.
    \end{enumerate}
    Clearly $\rho(\cdot) = \lVert \cdot \rVert$ satisfies conditions 1 and 3; it
    can be shown to satisfy the triangle inequality by applying
    Holder's inequality.}
  We can define \emph{convergence in $L_p$} in the obvious way.

  \begin{defn}
    Let $\{X_n\}$ be a sequence of random variables and let $X$ be
    another r.v.  $X_n$ converges to $X$ in $L_p$ as $n \to \infty$ (or $X_n
    \to^{L_p} X$) if $\lVert X_n - X \rVert_p \to 0$ as $n \to \infty$.
  \end{defn}
  The sequence $\{\lVert X_n - X \rVert_p\}_n$ is just a sequence of
  numbers, so convergence of this quantity to zero is conceptually
  straightforward.

  Convergence of random vectors holds if their individual elements
  converge.

\item We can also show that if $X_n \to X$ in $L_p$, then $X_n \to X$ in
  $L_q$ for any $q \in (0, p)$.  To see this, we can write
  \begin{align*}
    \E | X_n - X |^p
    &= \E | X_n - X |^{q \cdot (p/q)} \\
    &\geq (\E | X_n - X |^{q})^{p/q}
  \end{align*}
  since $p/q$ is a convex function (Jensen's inequality).  Raising
  both sides to the $1/p$ power completes the argument.

\item If $X_1, X_2,...$ are an \iid$(\mu, \sigma^2)$ sequence, we can easily
  verify that $\Xb_n \to \mu$ in $L_2$ as $n \to \infty$.  First, we have
  \begin{equation*}
    \E \Xb_n = n^{-1} \sum_{i=1}^n \E X_i = \mu.
  \end{equation*}
  Second,
  \begin{align*}
    \var \Xb &= (1/n^2) \var\Big( \sum_{i=1}^n X_i \Big)\\
    &= (1/n^2) \sum_{i=1}^n \var(X_i) + (2/n^2) \sum_{i=2}^n \sum_{j=1}^{i-1} \cov(X_i, X_j) \\
    &= \sigma^2 / n.
  \end{align*}
  
  Then $\lVert \Xb - \mu \rVert_2 = \sigma^2/n \to 0$ as $n \to \infty$.
  
\end{itemize}

\section{Convergence in probability}

\begin{itemize}[leftmargin=0pt]

\item Convergence in $L_p$ doesn't cover everything that we'd think of
  as ``convergence''
  \begin{ex}
    Let $W_n$ be a sequence of random variables s.t.
    \begin{equation*}
      W_n =
      \begin{cases}
        n & \text{probability } 1/n \\
        0 & \text{otherwise}.
      \end{cases}
    \end{equation*}
    Then $\Pr[W_n = 0] \to 1$ as $n \to \infty$ but the sequence does not
    converge in $L_1$ (and consequently in $L_p$ for $p > 1$):
    \begin{equation*}
      \E | W_n | = 0 \cdot \Pr[W_n = 0] + n \cdot \Pr[W_n = n] = 1
    \end{equation*}
    for all $n$.
  \end{ex}
  It's pretty obvious that the small divergent mass at $n$ breaks
  $L_p$ convergence.

\item This partially motivates the next version of convergence,
  convergence in probability.
  \begin{defn}
    A sequence of random variables $Y_n$ \emph{converges in
    probability} to $Y$ if
    \begin{equation*}
      \Pr[ | Y_n - Y | > \epsilon] \to 0
    \end{equation*}
    for any positive constant $\epsilon$.
  \end{defn}
  This convergence can be written as
  \begin{itemize}
  \item $\plim Y_n = Y$
  \item $Y_n \to^p Y$
  \item $Y_n \to Y$ i.p. (or ``in probability'').
  \end{itemize}

  Convergence in probability is implied by convergence in $L_p$
  as a consequence of Markov's inequality.
  \begin{thm}[Markov's inequality]
    If $X$ is an r.v., $\Pr[ |X|^p \geq \epsilon ] \leq \E |X|^p /\epsilon^p$.
  \end{thm}
  \begin{proof}
    Start with the obvious inequality
    \begin{equation*}
      \ind\{ |X|^p \geq \epsilon \} \leq |X|^p / \epsilon^p
    \end{equation*}
    and then take the expectation of both sides.
  \end{proof}
  When $p = 2$ this is called ``Chebychev's inequality.''

\item We've seen that convergence in probability does not imply $L_p$
  convergence.  But the problem in our $W_n$ example was that some of
  the mass diverged.  It turns out that a sufficient condition for
  convergence in probability to imply convergence in $L_p$ is for the
  sequence of random variables to have uniformly bounded $p + \delta$
  moments for $\delta > 0$.

\item A useful result holds for continuous functions: suppose that
  $X_n$ converges to $c$ in probability and that $g$ is a function and
  is continuous at $c$.  Then $g(X_n)$ converges in probability to
  $g(c)$.

  To prove this, remember the definition of a continuous function.
  $g$ is continuous at $c$ if for every $\epsilon >0$, there exists a $\delta > 0$
  such that $|g(c + h) - g(c)| < \epsilon$ for all $-\delta \leq h \leq \delta$.

  Now we want to prove that $\Pr[|g(X_n) - g(c)| > \gamma] \to 0$ for all $\gamma
  > 0$.  So take $\gamma$ as given.  Then define $\delta$ so that $|g(c + h) -
  g(c)| \leq \gamma$ for all $-\delta \leq h \leq \delta$.

  Since $|X_n - c| < \delta$ implies $|g(X_n) - g(c)| \leq \gamma$, we have the
  inequality
  \begin{equation*}
    \Pr[ |X_n - c| < \delta] \leq \Pr[ |g(X_n) - g(c)| \leq \gamma ]
  \end{equation*}
  and then
  \begin{equation*}
    \Pr[|g(X_n) - g(c)| > \gamma] \leq \Pr[|X_n - c| \geq \delta] \to 0
  \end{equation*}
  since $X_n \to^p c$.

  To wrap it up, since $X_n$ converges to $c$, there exists an $N$
  such that $P[|X_n - c| \geq \delta] < \beta$ for all $n \geq N$ and this last step
  completes the proof.

\item This result can be very useful: suppose you show that the
  sequence of matrices $W_n$ converges in probability to $\sigma$.  Then we
  know that $W_n^{-1}$ converges in probability to $\sigma^{-1}$ automatically.


\item As mentioned earlier, the Weak LLN is a family of results that
  can be basically stated: Suppose $\{X_n\}$ is a sequence of random
  variables with mean $\mu$.  Under some conditions, $n^{-1} \sum_{i=1}^n
  X_i \to \mu$ in probability.

  These conditions trade off the possibility of large deviations of
  individual observations (i.e. fat tails) for dependence conditions.
  The weakest WLLN for \iid\ sequences is \emph{Khinchine's WLLN},
  which requires $\{X_i\}$ to be an \iid\ sequence where each $X_i$
  has mean $\mu$ but is not required to have any higher moments.
  Obviously, in this case we can't use the proof based on convergence
  of the mean and variance from earlier.

  A variation that can be proven with the earlier argument is
  \emph{Chebychev's WLLN},\footnote{These names are coming from
  Appendix D of \citet{Gre12} and need to be verified and sourced
  better.} which requires that the $X_i$'s be independent but not
  necessarily identically distributed with means $\mu_i$ and variances
  $\sigma^2_i$ that satisfy $(1/n^2) \sum_i \sigma^2_i \to 0$ as $n \to \infty$.\footnote{An
  example where this variance condition fails is if $\sigma^2_i = i$; then
  \begin{equation*}
    (1/n^2) \sum_{i=1}^n \sigma^2_i
    = (1/n^2) \times n (n+1) / 2
    \to 1/2.
  \end{equation*}}
  Under those assumptions $\plim \Xb - \mub = 0$ where $\mub = (1/n)
  \sum_{i=1}^n \mu_i$.

\item When working with asymptotic arguments, there are often
  remainder terms that will converge to zero when we take limits.
  Since it is a little awkward to carry around those terms in full,
  there are notational shortcuts to make work easier.

  The first notation is for terms that vanish in the limit:
  \begin{defn}
    A sequence of random variables $X_n$ \emph{is $o_p$ as $n \to \infty$} if
    $X_n \to^p 0$, which is written as $X_n = o_p$.
  \end{defn}
  Note that this expression uses an equal sign, but the order matters.
  $X_n = o_p$ is not the same as $o_p = X_n$ (the second statement is
  more or less meaningless).  As an example, if $\Xb_n$ satisfies the
  WLLN and converges in probability to $\mu$, we could write
  \begin{equation*}
    \Xb = \mu + o_p
  \end{equation*}
  which can be useful in an intermediate step of an argument.

\item There is also notation for terms that do not vanish but are
  non-explosive in the limit:
  \begin{defn}
    A sequence of random variables $X_n$ \emph{is $O_p$ as $n \to \infty$}
    if, for each $\epsilon > 0$, there are constants $c$ and $N$ such that
    $\Pr[|X_n| > c] < \epsilon$ for every $n > N$.  Again, this is written as
    $X_n = O_p$.
  \end{defn}
  Order matters here too.  $X_n = O_p$ means that the sequence $X_n$
  is ``bounded in probability''---even if it is not a convergent
  sequence, it is not divergent either.

  Note that $o_p = O_p$ (any sequence that converges in probability to
  zero is also asymptotically bounded in probability) and, more
  generally, if $X_n \to \mu$ i.p., then $X_n = O_p$.  Moreover, if $X_n =
  o_p$ and $Y_n = O_p$ then $X_n Y_n = o_p$ or, put much terser, $o_p
  O_p = o_p$.

\item We can also introduce variations.  If $m_n$ is a sequence that
  either diverges or converges to zero as $n \to \infty$, we can write $X_n =
  o_p(m_n)$ if $X_n / m_n = o_p$ and $X_n = O_p(m_n)$ if $X_n / m_n =
  O_p$.
\end{itemize}

\section{Convergence in distribution}

\begin{itemize}[leftmargin=0pt]

\item We've been talking about convergence to random variables or
  constants.  Sometimes that's too limiting a concept: we often have
  cases where there's not a particular \emph{variable} that our
  sequence converges to, but instead its behavior gets closer and
  closer to that of a different distribution.

  Fortunately, just such a concept already exists.

  \begin{defn}
    Let $\{X_n\}$ be a sequence of random variables with cdfs $F_n$.
    $X_n$ converges in distribution to the distribution $F$ if
    \begin{equation*}
      \lvert F_n(c) - F(c) \rvert \to 0
    \end{equation*}
    as $n \to \infty$ for all continuity points $c$ of $F$.
  \end{defn}

  The restriction to continuity points rules out uninteresting (from a
  probability or statistics standpoint) counterexamples where the the
  limit of $F_n$ is not technically a distribution function but
  trivial modifications are.

  \begin{ex}\label{convergenceInDist}
    Suppose that $\{X_n\}_n$ is a sequence of r.v.s that take on the
    values
    \begin{equation*}
      X_n =
      \begin{cases}
        -1/n & \text{with probability 1/4} \\
        1/n  & \text{with probability 1/4} \\
        1 - 1/n & \text{with probability 1/4} \\
        1 + 1/n & \text{with probability 1/4}
      \end{cases}
    \end{equation*}
    Then $X_n \to \bernoulli(1/2)$ in distribution but the limit of the
    cdfs of $X_n$ is not a proper distribution.
  \end{ex}

  This leads to the implicit homework exercise.
  \begin{hw}
    Prove the assertions of Example~\ref{convergenceInDist}.
  \end{hw}

\item There are many ways to signify convergence in distribution.  In
  this text, we will typically write $X_n \to^d F$ or $X_n \to^d X$ where
  $X \sim F$.  The definition for random vectors is the same as for
  scalars as long as $c$ is understood to be a vector with the same
  length as $X_n$.

\item In later parts of the book, we'll use the following simple
  results.
  \begin{enumerate}
  \item If $Y_n \to^d c$ where $c$ is a constant, then $Y_n \to^p c$.
  \item If $X_n \to^d X$ and $Y_n \to^p c$, where $c$ is a constant, then
    $X_n Y_n \to^d c X$ and $X_n + Y_n \to^d X + c$.
  \item If $Y_n \to^d Y$ and $X_n - Y_n \to^p 0$ then $X_n \to^d Y$.
  \end{enumerate}
  The third is an obvious implication of the second, but it underpins
  many asymptotic approximation results: if $Y_n$ is relatively easy
  to analyze, we can find its limiting distribution and then show
  that $X_n$ must have the same limiting distribution as long as its
  deviations from $Y_n$ vanish in the limit.

  A more general result is the continuous mapping theorem,
  \begin{thm}[Continuous Mapping Theorem]
    If $X_n \to^d X$ and $g$ is a continuous function, then $g(X_n) \to^d
    g(X)$.
  \end{thm}
  
\end{itemize}

\section{Central limit theorem}

\begin{itemize}[leftmargin=0pt]

\item The biggest application of convergence in distribution is the
  central limit theorem.  (I assume you're familiar).

  \begin{thm}[Lindeberg-L\'evy CLT]
    If $X_1,...,X_n$ are \iid\ $(\mu, \sigma^2)$ then
    \begin{equation*}
      \sqrt{n} (\Xb - \mu) \to^d N(0,\sigma^2).
    \end{equation*}
  \end{thm}

  \begin{thm}[Lindeberg-Feller CLT]
    Suppose that $X_1,...,X_n$ are independent random variables with
    $X_i \sim (0, \sigma^2_i)$ and $\sigmab^2 = (1/n) \sum_{i=1}^n \sigma^2_i$.  If $\sigmab^2 \to 1$
    and
    \begin{equation}\label{a2}
      (1/n) \sum_{i=1}^n \E( X_i^2 \ind\{ |X_i| > c \sqrt{n} \} ) \to 0
    \end{equation}
    as $n \to \infty$ for all $c > 0$, then $\sqrt{n} \Xb \to^d N(0, 1)$.
  \end{thm}
  Note that the condition $\sigmab^2 \to 1$ is typically just a normalization
  requirement that can be satisfied by dividing $\sqrt{n} \Xb$ by
  $\sigmab/n$ and the condition that each $X_i$ have mean zero can be
  trivially satisfied by subtracting off each observation's mean.

\item Equation~\eqref{a2} is known as \emph{Lindeberg's condition}
  and is a restriction on the tails of the observations.  A more
  natural condition that ensures~\eqref{a2} is for $\E |X_i|^{2 +
  \delta}$ to be bounded for each $i$.

\item Proofs use the moment generating function or (for the least
  restrictive conditions) the characteristic function.

  \begin{defn}
    The \emph{moment generating function} of a random vector $X$ is
    defined as the function $M_X(t) = \E e^{t'X}$ and the
    \emph{characteristic function} of $X$ is the function $\psi_X(t) = \E
    e^{it'X}$ ($i = \sqrt{-1}$).
  \end{defn}

  The moment generating function is somewhat easier to work with but is
  not always finite.  The characteristic function is defined so that
  it's always finite and exists, but is harder to work with because it
  uses complex analysis.

  These are useful because if two random variables have the same
  moment generating function (as long as it is finite) or
  characteristic function then they have the same distribution
  function.

\item There are multivariate extensions.  A simple one is for \iid\
  random vectors.
  \begin{thm}[Multivariate Lindeberg-L\'evy CLT]\label{multivariateCLT}
    If $X_1,...,X_n$ are \iid\ random vectors with mean $\mu$ and vcv $\Sigma$,
    then
    \begin{equation*}
      \sqrt{n}(\Xb - \mu) \to^d N(0,\Sigma).
    \end{equation*}
  \end{thm}

  The proof uses the Crame\`er-Wold device.
  \begin{thm}
    Let $X$ be a random $k$-vector.  If $a'X$ is normally distributed
    for every nonzero deterministic $k$-vector $a$, then $X$ is
    multivariate normal.
  \end{thm}

  \begin{proof}[Proof of Theorem~\ref{multivariateCLT}]
    Let $a$ be any nonzero vector with the same dimension as $X_i$.
    The sequence $a'X_1,...,a'X_n$ satisfies the univariate
    Lindeberg-L\'evy CLT and has limiting mean $a'\mu$ and variance
    $a'\Sigma a$.
\end{proof}

\item It's important to remember that if the individual $X_i$'s are
  joint normal, $\Xb$ is normal in finite samples.  This suggests that
  the CLT will be an accurate approximation when the $X_i$'s are
  nearly normally distributed, but may be inaccurate when they're far
  from normal.  The important features of the normal here are thin
  tails and symmetry.

\end{itemize}

\section{Delta method}

\begin{itemize}[leftmargin=0pt]
\item Suppose we have have a sequence of random variables that obeys a
  central limit theorem (an estimator, for example).  We might want to
  know the distribution of smooth functions of that r.v.  The
  \emph{delta method} gives a simple approximation:

  \begin{thm}
    Let $\{Y_n\}$ be a sequence of random variables that satisfies
    \begin{equation*}
      \sqrt{n} (Y_n - y_0) \to^d N(0,\sigma^2)
    \end{equation*}
    and let $g$ be a measurable function that is differentiable with
    nonzero derivative at $y_0$.  Then
    \begin{equation*}
      \sqrt{n}[g(Y_n) - g(y_0)] \to^d N(0, \sigma^2[g'(y_0)]^2).
    \end{equation*}
  \end{thm}

  This result follows from a Taylor expansion of $g(Y_n)$ around
  $g(y_0)$.

  Note that the assumption of nonzero derivative at $y_0$ is only
  necessary to ensure that the asymptotic variance of $\sqrt{n}
  g(Y_n)$ is positive.  If that condition holds, we can still get a
  distributional result by taking a second order expansion (a
  higher-order expansion if the necessary).

\end{itemize}

% LocalWords:  Lindeberg's Lindeberg CLT univariate

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../core_econometrics"
%%% End: 
