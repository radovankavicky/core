% Copyright © 2013, authors of the "Core Econometrics Textbook;" a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\part*{Heteroskedasticity}%
\addcontentsline{toc}{part}{Heteroskedasticity}
\section{Heteroskedasticity-robust standard errors}

\begin{itemize}
\item what happens if $\E(ε ε' ∣ X) ≠ σ² I$?
\item maybe instead we have
\begin{description}
\item[zero correlation] $\E(ε_i ε_j ∣ X) = 0$
\item[heteroskedasticity] $\E(ε²_i ∣ X) = σ²_i(x_i)$ where the $σ²_i$
  are all (uniformly) finite and positive.
\end{description}
\item we want to know which of our previous conclusions hold
\end{itemize}

\paragraph{examples}
\begin{itemize}
\item may have (say) quarterly earnings data on different firms, and
        you might expect that earnings in certain industries are more
        volatile than others
\item may have GDP data; we'd expect that percent-change in GDP is
        going to be more volatile some quarters compared to others
\end{itemize}

\paragraph{unbiasedness}
\begin{itemize}
\item For unbiasedness, we just used the fact that $\E(ε ∣ X) = 0$
\begin{itemize}
\item exogeneity of the regressors
\end{itemize}
\item unbiasedness holds with heteroskedasticy errors
\end{itemize}

\paragraph{consistency}
\begin{itemize}
\item To prove consistency, we just needed to show that
        $n^{-1} ∑_{i=1}^n x_i ε_i$ has mean zero and obeys a
        law of large numbers
\item zero mean holds just like it does for unbiasedness
\item for LLN, we can show that each $x_i ε_i$ has finite
        variance;
\item observe that
  \[\var(x_i ε_i) = \E \var(x_i ε_i ∣ X) = \E(x_i x_i') σ²_i\]
  which is finite
\item so $\βh$ is consistent even under heteroskedasticity
\end{itemize}

\paragraph{formula for the variance}
\begin{itemize}
\item our original formula for the variance of $\βh$ is
  $σ² (X'X)^{-1}$
\begin{itemize}
\item obviously, this is a problem since we don't have a constant
  value of $σ²$ any more.
\end{itemize}
\item we can calculate the variance of $\βh$ under our new
  assumption (with $D$ the diagonal matrix with elements $σ²_i$)
  \begin{align*}
    E((\βh - β)(\βh - β)' ∣ X)
    &= (X'X)^{-1} \E(X'ε ε'X ∣ X) (X'X)^{-1} \\
    &= (X'X)^{-1} X'DX (X'X)^{-1} \\
    &= (X'X)^{-1} (∑_{i=1}^n x_i x_i' σ²_i)(X'X)^{-1}
  \end{align*}
\item This matrix is different than $σ²(X'X)^{-1}$
\item any hypothesis tests that we conduct assuming homoskedasticity
        will be invalid under heteroskedasticity
\end{itemize}

\paragraph{finite sample normality}
\begin{itemize}
\item Suppose now that $ε_i ∼ N(0, σ²_i)$ given X
\item We know that $\βh - β = (X'X)^{-1}X'ε$ which
        is still just a linear transformation of normal random variables
\item So $\βh$ is still normal (given X)
\begin{itemize}
\item variance-covariance matrix is obviously different than it was
          earlier
\item $\βh ∼ N\left(0, (X'X)^{-1} \left(∑_{i=1}^n x_i x_i' σ²_i \right) (X'X)^{-1}\right)$
  given $X$
\end{itemize}
\end{itemize}

\paragraph{asymptotic normality}
\begin{itemize}
\item To prove asymptotic normality, we needed to show that
\begin{itemize}
\item $n^{-1} X'X$ obeyed a law of large numbers
\begin{itemize}
\item we assumed this
\item doesn't have anything to do with the errors
\end{itemize}
\item $n^{-1/2} ∑_{i=1}^n x_i ε_i$ obeys a central limit
          theorem
\end{itemize}
\item If you look back at the proof, $x_i ε_i$ has
  heteroskedasticity even if $ε_i$ has constant variances
\item The exact same CLT we used earlier applies here
\begin{itemize}
\item we used the Lindeberg-Feller CLT earlier
\end{itemize}
\item So we have asymptotic normality under heteroskedasticity
\begin{itemize}
\item let $Q = \lim n^{-1} X'X$
\item let $Ω = \lim n^{-1} ∑_i x_i x_i' σ²_i$
\item $\sqrt{n}(\βh - β) → N(0, Q^{-1} Ω Q^{-1})$ in
          distribution.
\end{itemize}
\end{itemize}

\paragraph{summary}
\begin{itemize}
\item heteroskedasticity only affects the variance of $\βh$
\item it will make the tests we've studied invalid
\item if we can estimate the new variance
\begin{itemize}
\item we can replace the old variance in formulae for test statistics
\item these new tests will be (asymptotically) valid
\end{itemize}
\end{itemize}

\subsection{White's heteroscedasticity robust estimator}
\begin{itemize}
\item The goal is going to be getting new test statistics
\item To do that, we're going to work out an estimator for the
       asymptotic variance-covariance matrix, $Q^{-1} Ω Q^{-1}$
\item Derived by \citet{Whi80}
\item We can replace $s²(X'X)^{-1}$ with this new estimator to get
       aysmptotically valid versions of the t-test and the F-test
\item reading is 4.3 of \citet{KZ08}.
\end{itemize}

\paragraph{split up the sandwich}
\begin{itemize}
\item goals
\begin{itemize}
\item We need to estimate $Q^{-1}$
\item We need to estimate $Ω$.
\end{itemize}
\item $Q^{-1}$ is easy: we know that $n^{-1} X'X$ is a consistent
        estimator of $Q$
\item For $Ω$, we're going to apply our usual trick, the LLN.
\begin{itemize}
\item $Ω_n = n^{-1} ∑ \E(ε²_i ∣ x_i) x_i x_i'$
\item $\Ωh = \lim_n n^{-1} ∑ \εh²_i x_i x_i'$
\end{itemize}
\item want to prove that $\Ωh - Ω_n → 0$ in probability
\end{itemize}

\paragraph{consistency of $\Ωh$}
We can rewrite the difference:
  \begin{align*}
    \Ωh - Ω_n &= n^{-1} ∑_i x_i x_i' (\εh²_i - σ²_i) \\
    &= n^{-1} ∑_i x_i x_i'(\εh²_i - ε²_i) 
       + n^{-1} ∑_i x_i x_i'(ε²_i - σ²_i)
  \end{align*}

\paragraph{LLN for the second term}
\begin{itemize}
\item We know that each summand has mean zero
  \[\E(x_i x_i'(ε²_i - σ²_i)) = \E(x_i x_i' \E(ε²_i - σ²_i ∣ x_i)) = 0\]
\item Under conditions similar to those we used earlier, we know
         that $x_i x_i' (ε²_i - σ²_i)$ has finite and
         positive variance.
\item so the sum obeys (for example) Chebychev's LLN
\end{itemize}

\paragraph{convergence of the first term}
\begin{itemize}
\item note that
  \[ \εh²_i = (y_i - x_i'\βh)² = (ε_i + x_i'β - x_i'\βh)²
  = ε²_i - 2 ε_i x_i'(\βh - β) + (x_i'(\βh - β))²\]
\item so the first average becomes
  \[n^{-1}∑ x_i x_i' (x_i'(\βh - β))²
  - 2 n^{-1} ∑  x_i x_i' (ε_i x_i'(\βh - β)) \]
\item (informally) we can see why these terms converge to zero
\begin{itemize}
\item look the (j,k) element of the first matrix
  \[ n^{-1} ∑_i x_{ij} x_{ik}(x_i'(\βh - β))² =
  (\βh - β)' \Big(n^{-1} ∑_i x_i x_{ij} x_{ik} x_{i}'\Big) (\βh- β)\]
\begin{itemize}
\item under usual conditions, the term inside the sum is going to
             converge to its average
\item each $\βh - β$ is going to converge to zero
\end{itemize}
\item look at the (j,k) element of the second matrix
  \[ n^{-1} ∑_i x_{ij} x_{ik}(ε_i x_i'(\βh - β)) =
  \Big(n^{-1} ∑_i ε_i x_{ij} x_{ik} x_{i}'\Big) (\βh- β)\]
\begin{itemize}
\item the term inside the sum is going to converge to its
             average (zero, in this case)
\item $\βh - β$ is going to converge to zero
\end{itemize}
\end{itemize}
\end{itemize}

\paragraph{conclusion}
\begin{itemize}
\item consequently $Ω - \Ωh = Ω - Ω_n + Ω_n - \Ωh → 0$ in probability
\end{itemize}

\subsection{uses of the robust variance-covariance matrix}

\begin{itemize}
\item Define $\Σh$ as 
  \[\Σh = (n^{-1} X'X)^{-1} n^{-1} ∑_i x_i x_i' \εh²_i (n^{-1} X'X)^{-1}\]
\item variation of the t-test
\begin{itemize}
\item $\sqrt{n} \frac{\βh_j - β_j}{\Σh_{jj}} → N(0,1)$ in distribution
\end{itemize}
\item variation of the F-test (called the wald test)
\begin{itemize}
\item suppose that $R$ is a $J × K$ matrix with full rank
\item $n (R(\βh - β))'(R\Σh R')^{-1} (R(\βh - β))$ is asymptotically
  chi-square with $J$ degrees of freedom
\end{itemize}
\item obviously, you can use this for a hypothesis test by imposing
  $Rβ  = q$
\end{itemize}

\section{testing for heteroskedasticity}

\begin{itemize}
\item reading is 8.5 of \citet{Gre12} and 4.2 of \citet{KZ08}
\item we're not going to work through the proofs of this section, but
      we will explain the intuition behind them
\end{itemize}

\subsection{heuristic idea for test}

\begin{itemize}
\item $\βh$ is consistent even under heteroskedasticity
\item $\εh_i → ε_i$ for each $i$.
\item If $σ²_i$ is different for different individuals, it implies
  that $ε²_i$ should be correlated with $x_i$.
\item Why not regress $ε²_i$ on $x_i$ and do an F-test for
       the significance of the overall regression?
\end{itemize}

\subsection{more formal idea for test}

\begin{itemize}
\item Want to test the null hypothesis
  \[ H₀: \quad σ²_i = σ \qquad i = 1,...,n \]
\item If the null hypothesis is true, then $s² (n^{-1} X'X)^{-1} → Ω$
  in probability as $n → ∞$
\begin{itemize}
\item $Ω = \lim n^{-1} ∑_i x_i x_i' σ²_i$.
\item this equals $σ² \lim n^{-1} ∑_i x_i x_i'$ for
         homoskedasticity
\end{itemize}
\item We know that $n^{-1} ∑_i \εh²_i x_i x_i' → Ω$ as well
\begin{itemize}
\item holds with or without heteroskedasticity.
\end{itemize}
\item So, if the null hypothesis is true, $n^{-1} (∑_i \εh²_i - s²)
  x_i x_i' → Ω$ in probability.
\begin{itemize}
\item if the null is false, this doesn't happen.
\end{itemize}
\item This looks like an average
\end{itemize}

\subsection{explanation of the test}

\begin{itemize}
\item Let $ψ_i$ denote the vector of unique elements of $x_ix_i'$,
       along with a constant term (if not in $x_i$).
\begin{itemize}
\item say that $ψ_i$ has length $p$.
\end{itemize}
\item We'd expect that $n^{-1/2} ∑_i (\εh²_i - s²)
       ψ_i$ should obey a central limit theorem, so $→ N(0, V)$
       (say) in distribution (where $V$ is a pretty large matrix).
\item Then we'd have 
  \[
  \Big(n^{-1/2} ∑_i (\εh²_i - s²)ψ_i\Big)' \Vh^{-1} \Big(n^{-1/2} ∑_i (\εh²_i - s²)ψ_i\Big)
  →^d χ²_p
  \]
\begin{itemize}
\item holds under the null hypothesis
\item fails under the alternative (which gives us power).
\end{itemize}
\end{itemize}

\subsection{implementation of the test}

     Statistic is quite easy to implement (derived by \citealp{Whi80})

\begin{enumerate}
\item Regress $y_i$ on $x_i$ and save the OLS residuals.
\item Regress $\εh_i$ on $φ_i$ and calculate the $R²$
        from this regression
\item $n R²$ is asymptotically $χ²_{p-1}$ under the null
        hypothesis.
\end{enumerate}

\section{Generalized Least Squares}

Reading is \citet[8.3]{Gre12}

\subsection{motivation}

\begin{itemize}
\item Suppose that instead of having $\E( ε ε' ∣ X) = σ² I$ we knew
  that $\E(ε ε ∣ X) = σ² Ω$ for some other known matrix $Ω$.
\item we could still do MLE to estimate $β$ and $σ$
\item Would generally give you a different estimate of $β$ than OLS.
\end{itemize}n

\subsection{Infeasible GLS}

\paragraph{Draw pictures}

\paragraph{mathematics}
\begin{itemize}
\item We know that $Ω^{-1/2}ε ∼ (0, σ² I)$ given $X$
\item What happens if we regress $Ω^{-1/2} Y$ on $Ω^{-1/2} X$?
\begin{itemize}
\item Let $\Yt = Ω^{-1/2} Y$
\item Let $\Xt = Ω^{-1/2} X$
\item $\εt = \Yt - \Xt β$
\end{itemize}
\item If $\E(Y ∣ X) = Xβ$ then $\E(\Yt ∣ X) = \Xt β$
\item If we regress $\Yt$ on $\Xt$, we're back in standard
        OLS with homoskedastic errors.
\item Estimator is $\βh_{GLS} = (X'Ω^{-1}X)^{-1}X'Ω^{-1}Y$
\begin{itemize}
\item Unbiased (obviously)
\item Satisfies Gauss-Markov (BLUE)
\item Variance is $σ² (\Xt' \Xt)^{-1} = σ² (X' Ω^{-1} X)^{-1}$
\item Normal if the error terms are normal
\item Asymptotically normal otherwise.
\end{itemize}
\end{itemize}

\subsection{Weighted Least Squares (ie example)}

\begin{itemize}
\item Suppose that we believe that $σ²_i = σ² x_{ij}²$ for one of the
  regressors $j$.
\begin{itemize}
\item textbook gives an example: dependent variable is firm profits
         and indep variable is size.
\end{itemize}
\item To implement GLS, we want to regress
  \[ y/x_k = β + β₁(x₁/x_k) + β₂(x₂/x_k) + ⋯ + ε/x_k \]
\begin{itemize}
\item intuitively, if the firm is bigger, profit is more volatile
         and so we want to count that observation less in our estimation.
\end{itemize}
\end{itemize}

\subsection{Feasible GLS}

\begin{itemize}
\item Suppose that $Ω$ has a few unknown parameters:
\begin{itemize}
\item Two examples from TS:
\begin{itemize}
\item $Ω = (1, ρ, ρ², ...)$ (as a matrix)
\item $Ω = (1, γ, 0, 0; γ, 1, ...)$ (as a matrix)
\end{itemize}
\end{itemize}
\item We could just estimate those parameters along with the others.
\item Two different approaches
\begin{itemize}
\item MLE (next semester)
\item two-step least squares (FGLS)
\end{itemize}
\end{itemize}

\paragraph{Explanation of approach}
\begin{itemize}
\item Remember how we tested for heteroskedasticity
\item regress $\εh$ on the (squared) regressors and test
        for significance
\item use a similar approach here, but use for modeling.
\item Suppose that
        \[ \E(ε²_i ∣ X) = z_i'α \]
        where $z_i$ is some function of the regressors.
\item If we could regress $ε²_i$ on $z_i$, we could estimate $α$ consistently.
\item Since $\εh_i$ is consistent for $ε_i$, we can regress $ε²_i$ on
  $z_i$ to estimate $α$.
\end{itemize}

\paragraph{Algorithm}
\begin{itemize}
\item Regress $y$ on $x$ to get $\εh$
\item Regress $\εh$ on $z$ to estimate $α$
\item Regress $y_i/w_i$ on $x_i/w_i$ to estimate $\βh_{FGLS}$ where
  $w_i = sqrt{z_i'\αh}$
\end{itemize}

\paragraph{some more math}
\begin{itemize}
\item if we let $\Ωh$ be the estimate of $Ω$ that uses $\αh$,
  consistency of $\αh$ ensures that $\Ωh$ behaves asymptotically like
  $Ω$.
\item All of the asymptotic results from the GLS estimator apply to
        the Feasible GLS estimator
\item Finite sample results don't necessarily hold.
\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../regression"
%%% End:
