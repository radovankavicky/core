% Copyright (c) 2013â€“2014, authors of "Core Econometrics;" a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts. A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\chapter{Additional problems}

\begin{hw}
  Prove that the OLS estimator $\betah$ and the OLS residuals $\hat u$
  are uncorrelated.
\end{hw}

\begin{hw}
  Suppose that
  \begin{equation}
    y_i = \beta_1 + \beta_2 x_i + u_i,
  \end{equation}
  with $u_i \mid X \sim (0, \sigma^2)$ and $x_i \sim (\mu,\tau^2)$.
  Please derive a test statistic for the null hypothesis $\beta_1 =
  \beta_2^2$ against the alternative $\beta_1 \neq \beta_2^2$.
\end{hw}

\begin{hw}
  Suppose that $y_1,\dots,y_n$ are i.i.d. with $y_i \mid x_i \sim
  N(x_i'\beta, \sigma^2)$ and each $x_i$ a $k \times 1$ vector. Prove
  that the OLS estimator is the best unbiased estimator of $\beta$.
  Note that this is a stronger result than the Gauss-Markov theorem,
  since we are claiming that OLS is also better than nonlinear
  unbiased estimators.
\end{hw}

\begin{hw}
  We have the model $Y = X\beta + u$, where $n^{-1} X'X$ obeys a law
  of large numbers, $u$ is homoskedastic, and $n^{-1/2} \sum_i x_i
  u_i$ obeys a central limit theorem. Let $F$ be the F-statistic for
  the null hypothesis that $R\beta = 0$, where $R$ is an arbitrary $J
  \times (K + 1)$ vector
  \begin{enumerate}
  \item Suppose that the null hypothesis is true. How does the
    variance of $F$ behave as $n \to \infty$? How does the mean of
    $F$ behave?
  \item Suppose that $R\beta = \delta$ for some nonzero vector
    $\delta$, so the null hypothesis is false. How do your answers
    from the first part change?
  \item What do your answers to the previous questions tell you about
    the power of the F-test as $n \to \infty$?
  \end{enumerate}
\end{hw}

\begin{hw}
  Suppose that you are interested in the model $y_i = \beta_0 +
  x_i\beta_1 + u_i$ where $x_i$ is a random scalar, $\E(u_i \mid X) =
  0$ and the observations are i.i.d. But suppose that we do not
  observe $x_i$ directly but instead we observe $w_i = x_i + u_i$,
  where $u_i$ is another error term.
  \begin{enumerate}
  \item Prove that, if $u_i$ is perfectly correlated with $x_i$, OLS
    is essentially consistent in the following sense: let $\dot x_i =
    (x_i - \E x_i) / sd(x_i)$ and let $\dot w_i = (w_i - \E w_i) /
    sd(w_i)$. Then the OLS coefficient in the regression of $y_t$ on
    $\dot x_i$ is the same as that of the regression of $y_i$ on $\dot
    w_i$.
  \item Prove that, if $u_i$ is independent of $x_i$ and $u_i$, OLS is
    inconsistent even after standardizing the variables.
  \item Derive the MLE of $\beta_1$ under the assumption that $u_i$ and
    $u_i$ are independent mean-zero normal and determine whether it is
    consistent.
  \item Suppose now that $u_i$ and $u_i$ are independent, mean zero
    normal random variables, but now assume that we have another
    regressor $z_i = x_i + v_i$, where $v_i$ is another mean-zero,
    normal error term that's independent of $u_i$ and $u_i$. Derive
    the MLE of $\beta_1$ and determine whether it is consistent. For bonus
    points and well-deserved pride: is it asymptotically normal?
  \end{enumerate}
\end{hw}

\begin{hw}
  Suppose that you estimate the model $y_i = \beta_0 + \beta_1 x_{i1}
  + \beta_2 x_{i2} + u_i$ with OLS and calculate the F-test for the
  null hypothesis $\beta_1 = 1$. The $p$-value for this test is
  $0.03$ and $\betah_1$ is 0.54. In the following questions, you can
  assume that all of the necessary OLS assumptions hold.

  \begin{enumerate}
  \item How would you use this information to test against the
    one-sided alternative that $\beta_1 > 1$ at the 5\% level? Do you
    reject the null in favor of this alternative?
  \item How does your answer change if the alternative is $\beta_1 <
    1$? Do you reject in favor of this alternative?
  \item Based on your answers to the previous two questions, please
    outline a procedure to test the general null hypothesis that
    $R\beta \leq q$, where the inequality holds element by element.
  \end{enumerate}
\end{hw}

\begin{hw}
  Let $y_i$ and $x_i$ be i.i.d. random scalars and $\E x_i = 0$.
  Suppose that you estimate two models: $y_i = \mu + u_i$ and $y_i =
  \beta_0 + \beta_1 x_i + v_i.$ Calculate the bias and variance of
  $\hat \mu$ and $\betah_0$. How do they depend on $\beta_1$?
\end{hw}

\begin{hw}
  Suppose that we have the model $y_i = \beta_0 + \beta_1 x_{1i} +
  \beta_2 x_{2i} + \beta_3 x_{3i} + u_i$ where $\E(u_i \mid X) = 0$
  but the errors may be heteroskedastic.

  \begin{enumerate}
  \item Suppose you want to test the null hypothesis $\beta_i \leq 0$
    for all $i = 1,\dots,3$ against the alternative that $\beta_i > 0$
    for at least one $i$ (note that this is a single null hypothesis
    against a single alternative). How could you use the multiple
    hypothesis techniques we discussed in class to test this
    hypothesis?
  \item If this procedure rejects the null that all of the $\beta_i$
    are weakly negative, do we know why? Specifically, do we know
    which of the $\beta_i$ is positive? In what sense?
  \item Please write an R program to implement your answer to the
    first part of the question.
  \end{enumerate}
\end{hw}

\begin{hw}
  Consider this situation: you are interested in the effect of a
  variable $X_1$ (say, education level) on $Y$ (say, earnings), so you
  want to test whether the coefficient $\beta_1$ is significant in the
  equation
  \begin{equation*}
    Y_{i} = \beta_0 + \beta_1 X_{i1} + \vep_i.
  \end{equation*}
  But, you don't know whether or not you should include a second
  regressor, $X_2$ (maybe some measure of ability, like a test score).

  Three of the approaches we've discussed are:
  \begin{enumerate}
  \item regress $Y$ on $X_1$ and test for significance.
  \item regress $Y$ on $X_1$ and $X_2$ and test $\beta_1$ for
    significance.
  \item regress $Y$ on $X_1$ and $X_2$ and test $\beta_2$ for
    significance.
    \begin{itemize}
    \item If $\beta_2$ is significant, test $\beta_1$ for
      significance.
    \item If $\beta_2$ is \emph{not} significant, regress $Y$ on only
      $X_1$ and test $\beta_1$ for significance in that model.
    \end{itemize}
  \end{enumerate}

  Please design a Monte Carlo experiment in R that would let you study
  how well each of these approaches would work.
\end{hw}

\begin{hw}
  Suppose that we're interested in the effect of a binary treatment on
  some outcome of interest. Let $x_i$ denote the (univariate)
  treatment variable for individual $i$ and $y_i$ the outcome.
  Suppose that the DGP is
  \begin{equation}\label{eq:1}
    y_i = \beta_0 + \beta_1 x_i + \varepsilon_i
  \end{equation}
  but $\varepsilon_i$ and $x_i$ are potentially correlated.  For a
  concrete example, imagine that $x_i$ indicates whether individual
  $i$ smokes and $y_i$ is a measure of the individual's overall
  health.

  Fortunately, we have run an experiment that assigned individuals to
  either be in a treatment or a control group: let $x_i^*$ be 1 when
  the $i$th individual is in the treatment group and 0 otherwise and
  assume that
  \begin{equation}
    \Pr[x_i^* = 0] = \Pr[x_i^* = 1] = 1/2
  \end{equation}
  and $x_i^*$ is set independently of $\varepsilon_i$.

  \paragraph{The complication:} Individuals do not always comply with
  treatment. In particular,
  \begin{equation}
    \Pr[x_i = 1 \mid x_i^* = 1] = \Pr[x_i = 0 \mid x_i^* = 0] = 0.90
  \end{equation}
  and (just to complete the algebra for you)
  \begin{equation}
    \Pr[x_i = 1 \mid x_i^* = 0] = \Pr[x_i = 0 \mid x_i^* = 1] = 0.10.
  \end{equation}
  (Maintain this assumption in answering the questions below. You
  probably wouldn't know this information in real research, but there
  are times that you would.)

  \begin{enumerate}
  \item Suppose that $x_i^*$ is observed but $x_i$ is not.  This is
    the common situation where the experimenter knows whether or not
    an individual was treated but not whether or not the individual
    complied with treatment.  Derive the probability limit and
    asymptotic distribution for the OLS regression of $y_i$ on $x_i^*$
    and show that it is inconsistent for $\beta_1$.

  \item Based on your answer to the first part, how could you
    construct (asymptotic) 95\% confidence intervals for $\beta_1$?
    {\bfseries You do not necessarily need a consistent estimator of
      $\beta_1$ to construct these intervals.}  We know (by fiat) that
    $x_i^*$ and $x_i$ will different in 10\% of the sample on average,
    so you can work out the worst-case misclassification to derive
    your answer.

    If you need to impose additional assumptions to derive an answer,
    feel free to. (It's much better than no answer at all.)

  \item Now suppose that you observe $x_i$ but not $x_i^*$.  Repeat
    your steps for the previous two parts, but using $x_i$.  Show that
    the OLS regression of $y$ on $x_i$ is inconsistent for $\beta_1$
    and give a procedure for constructing 95\% confidence intervals
    for $\beta_1$ using only data on $y_i$ and $x_i$.

  \item Now suppose that both $x_i$ and $x_i^*$ are observed. Prove
    that $x_i^*$ can be used as an instrument for $x_i$ and show that
    the two-stage least squares estimator can be used to consistently
    estimate $\beta_1$ in~\eqref{eq:1}.

  \end{enumerate}

\end{hw}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../core_econometrics"
%%% End: 
