% Copyright (c) 2013â€“2014, authors of "Core Econometrics;" a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\chapter{Nonlinear regression}

\section{Semiparametric and nonparametric regression}

\subsection{Splines}

\paragraph{motivation}
\begin{itemize}
\item Draw scatter plot for y vs. 1-5 (education) again
\item have a general relationship between expected value of income
         and observed level of education (not linear)
\item transformed that relationship to something we can estimate by
         OLS
\item Can we do the same thing if the regressors are \underline{not}
         discrete?
\end{itemize}

\paragraph{basic idea}
\begin{itemize}
\item Suppose x is unrestricted
\item draw nonlinear function $\E(y \mid x)$
\item We can discretize the regressors (draw blocks) and fit
         separate OLS regressions within each block
\begin{itemize}
\item gives a better fit than OLS alone
\item may be biased, but bias will be small if you have small
           blocks
\item decreasing the size of the block will increase the variance
           of each estimate, so there is a tradeoff.
\end{itemize}
\end{itemize}

\paragraph{improvements on the basic idea}
\begin{itemize}
\item may (probably) want to impose that $\E(y_i \mid x_i)$ is
         continuous in $x_i$.
\item formally, we can get our model from (assuming p different blocks)
  \[
  y_i = \delta_0 + \gamma_0 x_i + \sum_{j=1}^{p-1} (\delta_j + \gamma_j x_i) \ind\{c_j \leq x_i\} + \epsilon_i
  \]
\begin{itemize}
\item continuity implies that
  \[ \delta_0 + \gamma_0 c_l + \sum_{j=1}^l (\delta_j + \gamma_j c_l) = \delta_0 + \gamma_0 c_l +
  \sum_{j=1}^{l+1} (\delta_j + \gamma_l c_l)\] for $l = 1,...,p-1$, which implies
  that $\delta_j = - \gamma_j c_j$ for all $l$
\item plugging this in to the initial equation gives
  \[
  y_i = \delta_0 + \gamma_0 x_i + \sum_{j=1}^{p-1} \gamma_j(x_i-c_j) \ind\{c_j \leq x_i\} + \epsilon_i
  \]
\end{itemize}
\item we can estimate the model by regressing $y_i$ on a constant,
  $x_i$, and the variables $(x_i - c_j) \ind\{c_j \leq x_i\}$
\end{itemize}

\paragraph{another improvement}
\begin{itemize}
\item We can combine this idea with the polynomial regression we
         looked at earlier
\begin{itemize}
\item We could fit a quadratic or cubic polynomial in each block
           instead of a straight line
\item now we could impose that the function is continuous and
           continuously differentiable
\begin{itemize}
\item requires additional restrictions on the coefficients
\item no longer is OLS, but gives a similar estimator
\end{itemize}
\end{itemize}
\item approach is called ``splines''
\item formal discussion is beyond the scope of this class
\end{itemize}

\subsection{Local linear models}

\begin{itemize}
\item draw scatter plot again
\item instead of setting up boxes first, we might just try to
        estimate the regression model at this point
\item create a box around the point and fit
\item could do the same thing at each possible point and build up
        the conditional expectation that way
\item Obviously, could be improved by using a weighting scheme
\item example of ``local linear regression''
\item also beyond the scope of the class
\end{itemize}

\paragraph{common themes and discussion}
\begin{itemize}
\item Splines are an example of a ``semi-parametric'' estimator
\begin{itemize}
\item trying to estimate an unspecified function, but using a
          finite number of parameters to do so
\end{itemize}
\item Local linear regression is an example of a nonparametric estimator
\begin{itemize}
\item estimating the unknown function directly
\item different coefficient values for each possible value of the
          function
\end{itemize}
\item For both, the coefficients themselves aren't very useful
\begin{itemize}
\item want to interpret changes in the expected value given
          chagnges in the coefficient
\item can derive them from the unknown coefficients (like we saw
          with the binary regressor case
\item use tools like the delta method to calculate standard errors.
\end{itemize}
\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../regression"
%%% End:
