% Copyright (c) 2013â€“2014, authors of "Core Econometrics;" a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\chapter{Estimation of linear regression}

\section{Properties of quadratic forms}

\begin{itemize}[leftmargin=0pt]

\item Quadratic forms will come up a lot when we study linear
  regression.  If $X$ be a random $k$-vector and let $A$ be a $k \times k$
  deterministic matrix then $X'A X$ is a (random) quadratic form.
  These are (relatively) easy to study because they can be written as
  summations:
  \begin{equation*}
    X'AX = \sum_{i,j} X_i X_j A_{ij}.
  \end{equation*}

\item Usually, when we discuss quadratic forms, we assume (at least
  implicitly) that $A$ is symmetric.  This assumption is usually
  without loss of generality because
  \begin{equation*}
    X' A X = X'(A/2 + A'/2)X
  \end{equation*}
  almost surely, and $X'(A/2 + A'/2)X$ is symmetric by construction.

\item Here's a representative result for quadratic forms that
  illustrates a useful trick.
  \begin{thm}
    Let $X$ be an $n \times 1$ vector of random variables, and
    let $A$ be an $n \times n$ symmetric matrix.  If $\E X = \mu$ and $\var(X) =
    \Sigma$, then
    \begin{equation}
      \E X'AX = \tr(A \Sigma) + \mu'A\mu.
    \end{equation}
  \end{thm}

  The proof uses three concepts that show up often: a scalar equals
  its own trace, linearity of the expectation, and a property of the
  trace: $\tr(XY) = \tr(YX)$ as long as both are conformable (the last
  property is difficult to see, so write out the matrix operations to
  convince yourself it is true).
  \begin{align*}
    \E(X'AX) &= \tr(\E(X'AX)) \\
    &= \E(\tr(X'AX)) \\
    &= \E(\tr(AXX')) \\
    &= \tr(\E(AXX')) \\
    &= \tr(A \E(XX')) \\
    &= \tr(A (\var(X) + \mu\mu')) \\
    &= \tr(A\Sigma) + \tr(A \mu\mu') \\
    &= \tr(A\Sigma) + \mu'A\mu.
  \end{align*}

\item Similar results obviously exist for other moments, but the
  algebra isn't quite as nice (and doesn't illuminate any key
  technique other than raw tenacity).  Here's an example for the
  variance under a few simplifying assumptions (this particular
  statement comes from \citealp{SL03})
  \begin{thm}
    let $X_1,...,X_n$ be independent random variables with means
    $\theta_1,...,\theta_n$, and the same 2nd, 3rd and 4th central
    moments $\mu_2$, $\mu_3$, $\mu_4$.  If $A$ is an $n \times n$
    symmetric matrix and $a$ is a column vector of the diagonal
    elements of $A$, then
    \begin{equation*}
      \var(X'AX) = 
      (\mu_4 - 3 \mu_2^2)a'a + 2 \mu_2^2 \tr(A^2) + 4 \mu_2 \theta'A^2 \theta + 4 \mu_3 \theta' A a.
    \end{equation*}
  \end{thm}
  To prove the result, take
  \begin{equation*}
    X'A X = \sum_{ij} ((X_i - \theta_i) + \theta_i) ((X_j - \theta_j) + \theta_j) A_{ij},
  \end{equation*}
  multiply out and take the variance.

\end{itemize}

\section{Gaussian random variables}

\begin{itemize}[leftmargin=0pt]

\item The \emph{Normal distribution} is particularly important when
  studying linear regression.  To the extent that we have finite
  sample results, many will hold only if the errors are Normal.
  Moreover, whenever we use the Central Limit Theorem, we get an
  approximately normal random variable.

\item If $X$ is an $N(\mu, \Sigma)$ random vector, its density is
  \begin{equation*}
    f(x) = \frac{1}{\sqrt{2^n \pi^n \det(\Sigma)}} exp(-1/2(x - \mu)'\Sigma^{-1}(x- \mu)).
  \end{equation*}
  The parameters $\mu$ and $\Sigma$ can be shown to be the mean and variance
  of the r.v. and completely determine its distribution.

  [you should draw the shape of the density and some elliptical
  contour plots]

  If $X$ is a $k$-dimensional multivariate normal with mean $\mu$
  and variance $\Sigma$, $A X + b$ is also multivariate normal for any
  constant $n \times k$ matrix $A$ and $n$-vector $b$.

  If $X$ is multivariate normal with mean $\mu$ and variance $\Sigma$,
  and $\Sigma^{1/2}$ is a symmetric matrix such that $\Sigma^{1/2} \Sigma^{1/2} = \Sigma$,
  then $(\Sigma^{1/2})^{-1} (X - \mu)$ is multivariate standard normal.

\item Independence of normal random variables is especially easy: if
  $X$ and $Y$ are uncorrelated normal random vectors, then they are
  independent.  The result follows from simply multplying out the
  square terms in the density function and then factoring it.

  Marginal and conditional distributions are also especially easy to
  work with.  If 
  \begin{equation*}
    (X_1,X_2) \sim N((\mu_1,\mu_2), \Sigma)
  \end{equation*}
  with
  \begin{equation*}
    \Sigma = \begin{pmatrix}
      \Sigma_{11} & \Sigma_{12} \\ \Sigma_{12}' & \Sigma_{22}
    \end{pmatrix}
  \end{equation*}
  then $X_1 \sim N(\mu_1, \Sigma_{11})$, $X_2 \sim N(\mu_2, \Sigma_{22})$, and
  \begin{equation*}    
    X_1 \mid X_2 \sim N(\mu_1 + \Sigma_{12} \Sigma_{22}^{-1} (X_2 - \mu_2),
               \Sigma_{11} - \Sigma_{12}'\Sigma_{22}^{-1} \Sigma_{12}).
  \end{equation*}
  Notice that the conditional mean of $X_1$ depends on $X_2$, but the
  conditional variance doesn't.

\item The fact that zero correlation implies independence also makes
  it easy to determine whether functions of normal r.v.s are
  independent.  Linear combinations and quadratic forms of normal
  r.v.s will come up often, so we'll single those results out.  For
  the next results, let $X$ be a $k$-dimensional standard normal
  random vector (in practice, we will often work with $\Sigma^{-1/2} X$).
  \begin{enumerate}
  \item If $A$ is a $k \times j$ matrix and $B$ a $k \times l$ matrix such that
    $A'B = 0$, then $A'X$ and $B'X$ are independent.
  \item If $P$ is a $k \times k$ idempotent matrix and $PB = 0$ then $X'PX$
    and $B'X$ are independent (an idempotent matrix is a square and
    symmetric matrix that satisfies $P = PP$).
  \item If $P$ and $Q$ are idempotent and $PQ = 0$ then $X'PX$ and
    $X'QX$ are independent.
  \end{enumerate}
  
\item Idempotent matrices come up often, as they represent projections
  in $\RR^n$.  If $z$ is a point in $\RR^n$ and $x_1,...,x_k$ is a set of
  vectors in $\RR^n$, we may want to project $z$ into the space spanned
  by $x_1,...,x_k$.  This amounts to finding a linear operator $P$ so
  that $P z$ is a linear combination of the $x_i$'s and $P z$ is as
  close as possible to $z$ subject to the first constraint.

  We'll worry about making sure that $Pz$ is a linear combination of
  the $x_i$'s later---that's going to be the focus of linear regression.
  But to ensure that $P z$ is as close as possible to $z$, we just
  need to have $z - Pz$ and $Pz$ perpendicular, so $(z - Pz)' Pz = 0$.
  But this requires that $z'(I - P)'P z = 0$ for any $z$, so $(I -
  P)'P = 0$, which amounts to our condition that $P = PP$.

  An idempotent matrix, $P$, has the property that all of its
  eigenvalues must be zero or one, so $\rank(P) = \tr(P)$.

\item Two statistics that are particularly relevant to the normal
  distribution are the sample mean, $\Xb = (1/n) \sum_{i=1}^n X_i$, and
  the sample variance, $S^2 = (1/(n-1)) \sum_{i=1}^n (X_i - \Xb)^2$.  Note
  that we can write $\Xb$ as a linear combination of the $X_i$'s:
  \begin{equation*}
    \Xb = (1/n,...,1/n) \cdot (X_1,...,X_n)' = (1/n) \iota'X,
  \end{equation*}
  where $\iota$ is a vector of $n$ ones, and
  \begin{equation*}
    S^2 = (1/(n-1)) (X - \iota \Xb)'(X - \iota \Xb) = (1/(n-1)) X'(I - (1/n) \iota\iota')X.
  \end{equation*}
  If $X \sim N(\mu, \sigma^2 I)$ then we have $\Xb \sim N(\mu, (\sigma^2/n) I)$.  Also,
  since $(I - (1/n) \iota\iota')$ is idempotent, we know right away that $\Xb$
  and $S^2$ are independent.

\item Some derived distributions are important

\item %
  \begin{defn}
    Let $X_1,...,X_k$ be independent standard normal.  The
    chi-squared distribution with $k$ degrees of freedom is the
    distribution of $\sum_{i=1}^k X^2_i$.
  \end{defn}

  \begin{thm}
    Let $X \sim N(0, I_n)$ and let $P$ be a symmetric $n \times n$ matrix.
    Then $X'PX$ is chi-squared with $k$ degrees of freedom iff $P$ is
    idempotent with rank $k$.
  \end{thm}

  \begin{thm}
    if $X$ and $Y$ are independent chi-squared with $df_X$ and $df_Y$
    degrees of freedom, then $X + Y$ is chi-square with $df_X + df_Y$
    degrees of freedom.
  \end{thm}

  If $X \sim N(\mu, \sigma^2 I)$ then $S^2$ is chi-square with $n-1$ degrees of
  freedom.

\item %
  \begin{defn}
    If $X$ is a standard normal and $Y$ is a chi-squared with $n$ dof
    and $X$ and $Y$ are independent, then $X / \sqrt{Y/n}$ is a $t(n)$
    random variable.
  \end{defn}
  From the quadratic form results, this is going to come up when $Z$
  is multivariate standard normal and $X = B'Z$, $Y = Z'PZ$, and $B'P
  = 0$.

\item %
  \begin{defn}
    If $X$ and $Y$ are independent chi-squared with $df_X$ and $df_Y$
    degrees of freedom, then $\frac{X/df_X}{Y/df_Y}$ is an $F(df_X,
    df_Y)$ random variable.
  \end{defn}

  Expect this to come up when $Z$ is multivariate standard normal and
  $X = Z'QZ$, $Y = Z'PZ$ and $Q$ and $P$ are idempotent and orthogonal.

\end{itemize}

\section{Statistical aspects of linear regression}

\begin{itemize}[leftmargin=0pt]
\item We typically will invoke some of the following assumptions:
  \begin{enumerate}
  \item $\E(\vep_i \mid X) = 0$ for all $i$
  \item $\var(\vep_i \mid X) = \sigma^2$
  \item $\cov(\vep_i, \vep_j \mid X) = 0$ for $i \neq j$
  \item $\vep_i \sim \iid(0,\sigma^2)$ given $X$
  \item $\vep_i \sim \iid\; N(0,\sigma^2)$ given $X$
  \end{enumerate}
  note that 5 implies 4 which implies 1, 2, and 3.

\item To derive OLS as the MLE, we assume that $\vep \sim N(0,\sigma^2 I)$.
  The mathematical derivation is presented in earlier sections, so
  we'll discuss some of the intuition now.

  This part is pretty terrible and it needs to be replaced with the
  actual drawings:
  \begin{itemize}
  \item draw plane for $n = 3$ and $k = 1$
  \item draw spheres for the different epsilons (sphere here is a
    consequence of normality and independence); each layer of the
    sphere is an iso-probability line
  \item draw a new picture to show estimating $Y$
    \begin{itemize}
    \item start with $Y$ and $X$ given
    \item choose some $\beta = B_1$
    \item draw different spheres for the ``epsilon'' corresponding to that mean
    \item draw a second picutre corresponding to $\beta = B_2$
    \item We can judge which of the values of $\beta$ is more plausible by
      looking at the density function, and choosing the one that's
      larger (which is just MLE); it should be clear that the
      coefficients that maximize the likelihood are going to be the
      coefficients that minimize the distance between $Y$ and $\Yh$.
    \end{itemize}
  \end{itemize}

\item Define a few terms: the OLS estimator is given by
  \begin{equation*}
    \betah = (X'X)^{-1}X'Y = \left(\sum_i x_i x_i'\right) \sum_i x_i y_i;
  \end{equation*}
  The \emph{fitted values} are defined as $\Yh = X \betah$ and the OLS
  \emph{residuals} are defined as $\veph = Y - \Yh$.

\item Some of the finite sample properties are relatively easy to
  prove.  The first is unbiasedness; or really conditional
  unbiasedness.
  \begin{thm}
    Suppose that $Y = X\beta + \vep$ where $\E(\vep \mid X) = 0$ and $X$ has
    full rank.  Then $\E(\betah \mid X) = \beta$.
  \end{thm}
  This conditional result implies unconditional unbiasedness through
  the LIE: $\E \betah = \E \E(\betah \mid X) = \beta$.  The proof is pretty
  straightforward:
  \begin{align*}
    \E(\betah \mid X) &= \E((X'X)^{-1} X'Y \mid X) \\
    &= (X'X)^{-1} X'\E(Y \mid X) \\
    &= (X'X)^{-1} X'\E(X\beta + e \mid X) \\
    &= (X'X)^{-1} X'X \beta \\
    &= \beta.
  \end{align*}
  One thing to notice is that this is a slightly stronger result than
  unbiasedness on its own: the expected value of the OLS coefficient
  estimator is $\beta$ \emph{regardless} of the value of $X$.

\item If we also assume that the errors are homoskedastic and
  uncorrelated, the same sort of approach gives us a formula for the
  variance of the OLS estimator.
  \begin{thm}
    Suppose that $Y = X\beta + \vep$ where $\E(\vep \mid X) = 0$ and $X$ has
    full rank, and also assume that $\E(\vep \vep' \mid X) = \sigma^2 I$ for some
    $\sigma^2$.  Then $\var(\betah \mid X) = \sigma^2(X'X)^{-1}$.
  \end{thm}

  The proof is similar to before.  Notice that now the variance
  \emph{does} depend on the particular value of the $X$'s drawn;
  inspection will show you that the variance of the $i$th coefficient
  estimator increases with the variance of the $i$th regressor.

  Since $\betah - \beta = (X'X)^{-1}X'\vep$, we have
  \begin{align*}
    \var(\betah \mid X) &= \E((\betah - \beta)(\betah-\beta)' \mid X) \\
    &= \E((X'X)^{-1}X'\vep\vep'X(X'X)^{-1} \mid X) \\
    &= (X'X)^{-1} X' \E(\vep\vep' \mid X) X (X'X)^{-1} \\
    &= \sigma^2 (X'X)^{-1} X'X (X'X)^{-1} \\
    &= \sigma^2 (X'X)^{-1}
  \end{align*}

\item It's also worth thinking about optimality properties under only
  the exogeneity and heteroskedasticity assumtions; you've probably
  heard of the Gauss-Markov Theorem before.  Notice that OLS can be
  viewed as a weighted average: $\betah = (X'X) X'Y = \sum_{i=1}^n w_i y_i$
  where $w_i$ is a $k$-vector, $w_i = (X'X)^{-1} x_i$.  The
  Gauss-Markov Theorem shows that OLS has the smallest variance of all
  of the unbiased linear estimators.
  \begin{thm}[Gauss-Markov Theorem]
    Assume that $Y = X\beta + \vep$ where $\vep \sim (0, \sigma^2 I)$ given $X$.
    Then $\betah$ is the unique estimator with minimum variance (given
    $X$) among all linear, unbiased estimators (i.e. OLS is BLUE).
  \end{thm}

  Just as in our earlier proofs of optimaility, the trick here will be
  to show that any other linear, unbiased estimator can be written as
  $\betah$ plus some additional uncorrelated noise term.  Suppose that
  $V$ is a $k \times n$ matrix s.t. $\E V'Y = \beta$ is unbiased for $\beta$, so
  $V'Y$ is a linear unbiased estimator.  We can write down immediately
  that
  \begin{equation*}
    V'Y = (X'X)^{-1}X'Y + [V'Y - (X'X)^{-1} X'Y],
  \end{equation*}
  and the proof then follows after showing that these two parts are
  uncorrelated.
  
  By construction, $V'Y = V'X\beta + V'e$, so
  \begin{equation*}
    \beta = \E(V'Y \mid X) = \E(V'X\beta + V'e \mid X) = V'X \beta + V'\E(e \mid X) = V'X\beta.
  \end{equation*}
  This holds for any choice of $\beta$, so $V'X$ must be equal to $I$ and,
  as a result, $V'Y - \beta = V'e$.

  Now it is straightforward to calculate the covariance between
  $(X'X)^{-1}X'Y$ and $(V - (X'X)^{-1}X')Y$,
  \begin{align*}
    \cov((X'X)^{-1}X'Y, (V - (X'X)^{-1}X')Y \mid X)
    &= \E((X'X)^{-1}X'e e'(V - X(X'X)^{-1}) \mid X) \\
    &= \sigma^2 (X'X)^{-1}X'(V - X(X'X)^{-1}) \\
    &= \sigma^2 [(X'X)^{-1}X'V - (X'X)^{-1} X'X (X'X)^{-1}].
  \end{align*}
  Both equal $(X'X)^{-1}$, so the whole term is zero and we've shown
  that the covariance is zero, completing the proof.

\item We've proved this result conditional on $X$, but the
  unconditional extension is easy.  If $\var(\betah \mid X) \leq \var(V'Y \mid X)$
  for all values of $X$, then $\E \var(\betah \mid X) \leq \E \var(V'Y \mid X)$ as
  well.

\item Answering the next question may help your intuition
  \begin{hw}
    Suppose the true DGP is
    \begin{equation*}
      y_i = x_i'\beta + z_i'\alpha + \varepsilon_i
    \end{equation*}
    where
    \begin{itemize}
    \item $Y_i$, $X_i$, $Z_i$, and $\varepsilon_i$ are all stochastic,
    \item $Y_i$, $X_i$, and $Z_i$ are all observed but $\varepsilon_i$ is not,
    \item $\E(\varepsilon_i \mid X_i, Z_i) = 0$ almost surely,
    \item $\E(Z_i \mid X_i) = 0$ almost surely.
    \end{itemize}
    Under these assumptions, the estimator $\hat \beta = (X'X)^{-1}
    X'Y$ is the BLUE of $\beta$, by the Gauss-Markov theorem.  But
    these assumptions also ensure that
    \begin{equation*}
      \begin{pmatrix} \tilde \beta \\ \tilde \alpha \end{pmatrix} =
      \begin{pmatrix} X'X & X'Z \\ Z'X & Z'Z \end{pmatrix}^{-1}
      \begin{pmatrix} X'Y \\ Z'Y \end{pmatrix}
    \end{equation*}
    is the BLUE for $(\beta',\alpha')'$, making $\tilde\beta$ the BLUE
    for $\beta$, again by the Gauss Markov theorem.

    How are both estimators the ``best linear unbiased estimator'' of
    the same parameter?  (Please do not claim that $\hat\beta$ and
    $\tilde\beta$ have the same value. They don't.)
  \end{hw}

\item This argument can be generalized.  Suppose that instead of
  having $\E(\vep \vep' \mid X) = \sigma^2 I$ we knew that $\E(\vep \vep' \mid X) = \sigma^2
  \Omega$ for some other known matrix $\Omega$.  We could still do MLE to
  estimate $\beta$ and $\sigma$, and this would generally give a different
  estimate than OLS.  But observe that $\Omega^{-1/2} \vep \sim (0, \sigma^2 I)$ given
  $X$, so if we transform the regression relationship to be
  \begin{equation*}
    \Omega^{-1/2} Y = \Omega^{-1/2} X \beta + \Omega^{-1/2} \vep,
  \end{equation*}
  this new relationship meets the conditions of the Gauss-Markov
  theorem.

  This implies that the efficient estimator is the OLS regression of
  $\Omega^{-1/2} Y$ on $\Omega^{-1/2} X$, or
  \begin{equation*}
    \betah_{GLS} = (X'\Omega^{-1}X)^{-1}X'\Omega^{-1}Y.
  \end{equation*}
  So $\betah_{GLS}$ is BLUE and has variance equal to $\sigma^2 (X' \Omega^{-1}
  X)^{-1}$.

  Now, it is unlikely that we will know $\Omega$ in practice.  We'll
  discuss how to estimate $\Omega$ when we discuss asymptotic properties of
  these estimators in part 3.

\item The sampling distributions also matter.  Finite sample
  distributions are going to depend on the distribution of the error
  term, $\vep$.  $\betah$ is relatively easy to work with, since it equals
  $\beta + (X'X)^{-1} X'\vep$; if $\vep \sim N(0, \sigma^2)$ given $X$, clearly
  $\betah \sim N(\beta, \sigma^2(X'X)^{-1})$ given $X$.

  For $s^2$, we can get results pretty quickly by relating it back to
  the quadratic form results. We have $s^2 = (n-k)^{-1} \sum_{i=1}^n \veph^2_i$
  which can also be written as
  \begin{align*}
    s^2 &= (n-k)^{-1} \sum_i (y_i - x_i'\betah)^2 \\
    &= (n-k)^{-1} (Y - X\betah)'(Y - X\betah) \\
    &= (n-k)^{-1} (Y - X(X'X)^{-1}X'Y)'(Y - X(X'X)^{-1}X'Y) \\
    &= (n-k)^{-1} Y'(I - X(X'X)^{-1}X')'(I - X(X'X)^{-1}X')Y \\
    &= (n-k)^{-1} Y'(I - X(X'X)^{-1}X')Y \\
    &= (n-k)^{-1} (X\beta + \vep)'(I - X(X'X)^{-1}X')(X\beta + \vep) \\
    &= (n-k)^{-1} \vep'(I - X(X'X)^{-1}X')\vep.
  \end{align*}
  Now $I - X(X'X)^{-1}X'$ is a projection matrix with rank $n-k$, so
  $s^2 \sim (n-k)^{-1} \chi^2(n-k)$ given $X$.  Moreover, it is easy to verify
  that $s^2$ and $\betah$ are independent given $X$.

\item Two unconditional results are more useful.  If we let $q_i$ be
  the $i,i$ element of $(X'X)^{-1}$ and look at the unconditional
  density of $(\betah_i - \beta_i)/\sqrt{q_i \sigma^2}$, we see that
  \begin{align*}
    f_{(\betah_i - \beta_i)/\sqrt{q_i \sigma^2}}(b)
    &= \int f_{(\betah_i - \beta_i)/\sqrt{q_i \sigma^2}, q_i} (b, q) dq \\
    &= \int f_{(\betah_i - \beta_i)/\sqrt{q_i \sigma^2}}(b \mid q_i = q) f_{q_i}(q) dq \\
    &= \int \phi(b) f_{q_i}(q) dq \\
    &= \phi(b) \int f_{q_i}(q) dq \\
    &= \phi(b).
  \end{align*}
  Similarly, replacing $\sigma^2$ with $s^2$ gives us a $t(n-k)$ distribution.

\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../core_econometrics"
%%% End: 
