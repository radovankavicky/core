% Copyright (c) 2013, authors of "Core Econometrics;" a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\chapter{Systems of equations}

\begin{itemize}
\item There are lots of panel implications to what we're discussing,
      but we won't cover them
\item Basic idea:
\begin{itemize}
\item Suppose you have two relationships you are interested in
        (return data, say)
        \[r_{jt} - r_{ft} = α_j + β_j (r_{mt} - r_{ft}) + ε_{it}\]
\begin{itemize}
\item $j = 1,2$
\item $t = 1,2,3,...$
\item $r_{it}$ is the return on asset $i$
\item $r_{ft}$ is the risk-free rate of return (ie T-bills)
\item $r_{mt}$ is the return on the entire market.
\end{itemize}
\end{itemize}
\item here's the question: should you do OLS on each equation
      individually, or do something different?
\begin{itemize}
\item we'll derive a new estimator
\item called seemingly unrelated regression
\item obviously, if the errors are exogenous you can do ols and it
        will be fine; the question here is if we can do something
        better than OLS.
\end{itemize}
\end{itemize}

\section{setup}
\subsection{notation}

\begin{itemize}
\item We want to write these two models as
       \[y₁ = X₁ β₁ + ε₁\]
       and
       \[y₂ = X₂ β₂ + ε₂\]
\begin{itemize}
\item each $y_i$ is $n × 1$
\end{itemize}
\item alternatively
  \[ \binom{y_{1i}}{y_{2i}} = \binom{x_{1i}'β₁}{x_{2i}'β₂} + 
  \binom{ε_{1i}}{ ε_{2i}}\]
       for $i = 1,...,n$.
\end{itemize}

\subsection{assumptions}

\begin{description}
\item[Exogeneity] \[E(\binom{\ep_{1i}}{\ep_{2i}} ∣ X) = 0\] for all $i$
\item[homoskedasticity] \[E\Big(\binom{ε_{1i}}{ε_{2i}} \binom{ε_{1i}}{ε_{2i}}' ∣ X \Big) = Σ\]
\item[uncorrelated errors] \[E\Big(\binom{ε_{1i}}{ε_{2i}}
  \binom{ε_{1j}}{ε_{2j}}' ∣ X \Big) = 0\] if $i ≠ j$.
\begin{itemize}
\item allow errors to be correlated across equations but not across
         observations
\end{itemize}
\item[usual] assume that the usual technical conditions on the
                regressors and errors hold to allow for us to apply
                laws of large numbers, etc.
\end{description}

\subsection{estimation with known covariance matrix}

\begin{itemize}
\item We want to rewrite the equations as
       \[ \binom{\mathbf{y}₁}{\mathbf{y}₂} = \begin{pmatrix} \mathbf{X}₁
       & 0 \\ 0 & \mathbf{X}₂  \end{pmatrix}
       \binom{β₁}{β₂} +
      \binom{\mathbf{\ep}₁}{\mathbf{\ep}₂} \]
\begin{itemize}
\item ie $\mathbf{y} = \mathbf{X} \mathbf{β} + \mathbf{ε}$
\end{itemize}
\item Remarkable thing is that we can estimate this model by GLS
\begin{itemize}
\item Note that $E(\mathbf{ε} ∣ \mathbf{X}) = 0$
\item Moreover,
  \[E(\mathbf{ε} \mathbf{ε}' ∣ \mathbf{X}) = Σ ⊗ I =
  \begin{pmatrix} σ²_{11} I & σ_{12} I \\ σ_{21} I & σ²_{22} I \end{pmatrix}\]
\item So the model satisfies the requirements for GLS regression to
         be BLUE, even though the dependent variable is different for
         the two equations.
\end{itemize}
\item Assume that $Σ$ is known for now, 
  \[\βh_{GLS} = (X'(Σ ⊗ I)^{-1}X)^{-1}X'(Σ ⊗ I)^{-1}Y\]
\begin{itemize}
\item simplify this further:
\begin{itemize}
\item $(Σ ⊗ I)^{-1} = Σ^{-1} ⊗ I$
\begin{itemize}
\item Let $γ_{ij}$ denote the $i,j$ element of $Σ^{-1}$
\end{itemize}
\item as a result
  \[\βh_{SUR} = \begin{pmatrix}
    γ_{11} X₁'X₁ & γ_{12} X₁'X₂ \\ & γ_{22} X₂'X₂ 
  \end{pmatrix}^{-1} 
  \begin{pmatrix}
    γ_{11} X₁'y₁ + γ_{12} X₁'y₂ \\ γ_{12} X₂'y₁ + γ_{22} X₂'y₂
  \end{pmatrix} \]
\end{itemize}
\end{itemize}
\item variance-covariance matrix is found as usual:
  \[\var(\βh_{GLS} ∣ X) = (X'(Σ ⊗ I)^{-1}X)^{-1}\]
\begin{itemize}
\item if you don't see this right away, please work through the
         math on your own.  The logic is identical to OLS or GLS.
\end{itemize}
\end{itemize}

\subsection{estimation with unknown covariance}

\begin{itemize}
\item Big difference from single-equation case
\begin{itemize}
\item with single equations, we were concerned about heteroskedasticity
\item here, we assume homoskedasticity and uncorrelated observations
\item allow correlation across equations.
\end{itemize}
\item We can estimate $Σ$ pretty easily:
\begin{itemize}
\item let $(\εh_{i1}, \εh_{i2})$ be the pair of
         residuals for observation $i$ from the OLS estimation
\begin{itemize}
\item \textbf{remember} OLS is consistent
\item so $(\εh_{i1}, \εh_{i2})$ is consistent for $(ε_{i1}, ε_{i2})$
\end{itemize}
\item LLN would impliy that
  \[ n^{-1} ∑_i (ε_{i1}, ε_{i2})' (ε_{i1}, ε_{i2}) → Σ\] in
  probability
\end{itemize}
\item Consistency of the residuals implies
  \[\Σh = n^{-1} ∑_i (\εh_{i1}, \εh_{i2})'(\εh_{i1},\hat ε_{i2})→ Σ
  \quad i.p.\]
\item plug in to GLS formula for a feasible estimator
\end{itemize}

\subsection{Simplification of SUR}

     The SUR becomes the OLS estimator in two specific and informative
     cases
\begin{itemize}
\item errors are uncorrelated across models
\item all of the same regressors appear in both models
\begin{itemize}
\item VARs in macro, for example
\end{itemize}
\end{itemize}

\paragraph{zero correlation}
\begin{itemize}
\item formula for SUR estimator:
  \[\βh_{SUR} = (X'(Σ ⊗ I)^{-1}X)^{-1}X'(Σ ⊗ I)^{-1}Y\]
\item Suppose that $σ_{12} = 0$, so
  \[ Σ ⊗ I = (\begin{matrix} σ_{11}² I & 0 \\ 0 & σ_{22}² I \end{matrix})\]
\begin{itemize}
\item First part becomes
  \[(X'( Σ ⊗ I)^{-1} X) =
  \begin{pmatrix} X₁'X₁ / σ_{11}² & 0 \\ 0 & X₂'X₂ / σ_{22}² \end{pmatrix}\]
\item Second part becomes 
  \[X'(Σ ⊗ I)^{-1}Y = \binom{X₁' y₁ / σ_{11}²}{X₂'y₂ / σ_{22}²}\]
\item $\βh_{SUR} = \binom{(X₁'X₁)^{-1}X₁'y₁}{(X₂'X₂)^{-1}X₂'y₂} =
  \binom{\βh_{1,ols}}{\βh_{2,ols}}$
\end{itemize}
\end{itemize}

\paragraph{Identical regressors in both models}
\begin{itemize}
\item Let $X₁$ and $X₂$ both equal $X$
\item Use the simplified form of the SUR estimator:
  \begin{align*}
    \βh_{SUR}
    &= \begin{pmatrix}
      γ_{11} X₁'X₁ & γ_{12} X₁'X₂ \\ & γ_{22} X₂'X₂ 
    \end{pmatrix}^{-1}
    \begin{pmatrix}
      γ_{11} X₁'y₁ + γ_{12} X₁'y₂ \\ γ_{12} X₂'y₁ + γ_{22} X₂'y₂
    \end{pmatrix} \\
    &= \begin{pmatrix}
      γ_{11} X'X & γ_{12} X'X \\ & γ_{22}X'X
    \end{pmatrix}^{-1}
    \begin{pmatrix}
      γ_{11} X'y₁ + γ_{12}X'y₂ \\ γ_{12} X'y₁ + γ_{22} X'y₂
    \end{pmatrix}
  \end{align*}
\item First term becomes $(Σ^{-1} ⊗ X'X)^{-1}$ which equals
  $Σ ⊗ (X'X)^{-1}$
\item second term becomes
  \[\begin{pmatrix}
    X'X (γ_{11}\βh_{1,ols} + γ_{12}\βh_{2,ols}) \\
    X'X (γ_{21}\βh_{1,ols} + γ_{22}\βh_{2,ols})
  \end{pmatrix}\]
\begin{itemize}
\item Premultipy each $X'y_j$ with $X'X(X'X)^{-1}$
\end{itemize}
\item Multiply through and everything cancels, giving
  \[\βh_{SUR} = \binom{\βh_{1,ols}}{\βh_{2,ols}}\]
\end{itemize}

\subsection{Hypothesis testing}

\paragraph{similarities to OLS/GLS}
\begin{itemize}
\item $\βh_{SUR}$ is the estimate of $β₁$ and $β₂$
        stacked on top of eachother
\item We can test hypotheses about the $β$ as usual
\begin{itemize}
\item hypotheses about a single equation
          \[ β_{1,1} = 0 \]
          for example
\item hypotheses about coefficients from different equations
\begin{itemize}
\item For example:
\begin{itemize}
\item $Y₁$ is log consumption of one good (imagine an individual)
\item $Y₂$ is log consumption of another, similar good
\item Regressors include log prices
\item may want to test that the elasticty of both goods are
              the same
\end{itemize}
\end{itemize}
\end{itemize}
\end{itemize}

\paragraph{nature of hypothesis}
\begin{itemize}
\item We can write both sorts of hypotheses as
        \[ R β = q \]
\item the variance of $R\βh - Rβ$ is given by $R(X'(Σ ⊗ I)^{-1}X)^{-1}R'$
\item Under the null, 
  \[(R\βh - q)'\left(R(X'(Σ ⊗ I)^{-1}X)R'\right)^{-1}(R\βh - q)\]
  is asymptotically chi-square with $J$-degrees of freedom
\item Since $\Σh$ is consistent for $Σ$, so is
  \[(R\βh - q)'\left(R(X'(\Σh ⊗ I)^{-1}X)R'\right)^{-1}(R\βh - q)\]
\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../core_econometrics"
%%% End:
