% Copyright (c) 2013-2014, Gray Calhoun.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\section{Systems of equations}

\begin{itemize}[leftmargin=0pt]

\item The basic idea here is that it is sometimes more efficient to
  estimate multiple equations simultaneously than individually.  For
  example, suppose you have two relationships you are interested in
  (returns data, say)
  \begin{equation*}
    r_{jt} - r_{ft} = \alpha_j + \beta_j (r_{mt} - r_{ft}) + \vep_{it}
  \end{equation*}
  where the variables are
  \begin{itemize}
  \item $r_{it}$ is the return on asset $i$
  \item $r_{ft}$ is the risk-free rate of return (i.e. T-bills)
  \item $r_{mt}$ is the return on the entire market.
  \end{itemize}

  In this setting, should you do OLS on each equation individually, or
  do something different? This section will present the
  \emph{Seemingly Unrelated Regressions} estimator and will discuss
  when it is possible to do better than OLS.

\item More generally, suppose we have two models
  \begin{equation}\label{eq:3}
    Y_1 = X_1 \beta_1 + \vep_1
  \end{equation}
  and
  \begin{equation}\label{eq:4}
    Y_2 = X_2 \beta_2 + \vep_2
  \end{equation}
  where each $Y_i$ is $n \times 1$.  Alternatively, we can write
  \begin{equation*}
    \binom{y_{1i}}{y_{2i}}
    = \binom{x_{1i}'\beta_1}{x_{2i}'\beta_2} +  \binom{\vep_{1i}}{ \vep_{2i}}
  \end{equation*}
  for $i = 1,...,n$.

\item We're going to make standard assumptions from our treatment of
  OLS:
  \begin{description}
  \item[Exogeneity.] For all $i$,
    \begin{equation*}
      E\Bigg(\binom{\vep_{1i}}{\vep_{2i}} \ \Big|\ X\Bigg) = 0
    \end{equation*}
  \item[Homoskedasticity.] For all $i$,
    \begin{equation*}
      E\Bigg(\binom{\vep_{1i}}{\vep_{2i}} \binom{\vep_{1i}}{\vep_{2i}}'
      \ \Big|\ X \Bigg) = \Sigma
    \end{equation*}
  \item[Uncorrelated errors.] For $i \neq j$,
    \begin{equation*}
      E\Bigg(\binom{\vep_{1i}}{\vep_{2i}}
      \binom{\vep_{1j}}{\vep_{2j}}' \ \Big|\ X \Bigg) = 0
    \end{equation*}
    This assumption allows errors to be correlated across equations
    but not across observations.
  \end{description}
  
  Also assume that the usual technical conditions on the regressors
  and errors hold to allow for us to apply laws of large numbers, etc.

\end{itemize}

\subsection{Estimation of $\beta$}

\begin{itemize}[leftmargin=0pt]
\item We can rewrite Equations~\eqref{eq:3} and~\eqref{eq:4} as
  \begin{equation}\label{eq:5}
    \underbrace{\binom{Y_1}{Y_2}}_Y
    = \underbrace{\begin{pmatrix} X_1 & 0 \\ 0 & X_2 \end{pmatrix}
    \binom{\beta_1}{\beta_2}}_{X\beta} +
    \underbrace{\binom{\vep_1}{\vep_2}}_\vep.
  \end{equation}

  Under the assumptions listed earlier, we can estimate the
  ``stacked'' model by OLS or GLS.
  Since $E(\vep \mid X) = 0$ and
  \begin{equation*}
    E(\vep \vep' \mid X) = \Sigma \otimes I =
    \begin{pmatrix}
      \sigma^2_{11} I & \sigma_{12} I \\ \sigma_{21} I & \sigma^2_{22}
      I
    \end{pmatrix},
  \end{equation*}
  the model satisfies the requirements for GLS regression to be BLUE,
  even though the dependent variable is different for the two
  equations.

  \begin{hw}
    Verify that the direct OLS estimator of~\eqref{eq:5} is
    numerically equivalent to the individual OLS estimators
    of~\eqref{eq:3} and~\eqref{eq:4} individually.
  \end{hw}

\item Under the assumption that $\Sigma$ is known, 
  \begin{equation*}
    \betah_{GLS} = ( X' (\Sigma \otimes I)^{-1} X )^{-1} 
                  X' (\Sigma \otimes I)^{-1} Y.
  \end{equation*}
  This expression can be simplified further. Using the equality
  $(\Sigma \otimes I)^{-1} = \Sigma^{-1} \otimes I$ and letting
  $\gamma_{ij}$ denote the $i,j$ element of $\Sigma^{-1}$, we have
  \begin{equation*}
    \betah_{SUR} = \begin{pmatrix}
      \gamma_{11} X_1'X_1 & \gamma_{12} X_1'X_2 \\
      \cdot & \gamma_{22} X_2'X_2
    \end{pmatrix}^{-1}
    \begin{pmatrix}
      \gamma_{11} X_1'y_1 + \gamma_{12} X_1'y_2 \\
      \gamma_{12} X_2'y_1 + \gamma_{22} X_2'y_2
    \end{pmatrix}.
  \end{equation*}

\item The variance-covariance matrix is found as usual:
  \begin{equation*}
    \var(\betah_{SUR} \mid X) = (X'(\Sigma \otimes I)^{-1}X)^{-1}.
  \end{equation*}

\item Of course, the assumption that $\Sigma$ is known is
  unreasonable. It can be estimated, pretty easily here, though,
  because OLS is consistent (if inefficient) and so $(\veph_{i1},
  \veph_{i2})$ is consistent for $(\vep_{i1}, \vep_{i2})$.

  The LLN then implies that
  \begin{equation*}
    (1/n) \sum_i (\veph_{i1}, \veph_{i2})' (\veph_{i1}, \veph_{i2})
    \equiv \Sigmah \to^p \Sigma
  \end{equation*}
  and we can plug $\Sigmah$ into the GLS formula for a feasible
  estimator.

\end{itemize}

\subsection{Simplifications of the SUR estimator}

\begin{itemize}[leftmargin=0pt]

\item The SUR becomes the OLS estimator in two specific and
  informative cases. First, if the errors are uncorrelated across
  models, and second, if all of the same regressors appear in both
  models (which is the case for Vector Autoregressions in time series,
  for example).

\item First, suppose that the errors in the two equations are
  uncorrelated. Remember the formula for SUR estimator:
  \begin{equation*}
    \betah_{SUR} = (X'(\Sigma \otimes I)^{-1}X)^{-1}X'(\Sigma \otimes I)^{-1}Y
  \end{equation*}
  If $\sigma_{12} = 0$, the first term is
  \begin{equation*}
    (X'( \Sigma \otimes I)^{-1} X)^{-1} =
    \begin{pmatrix}
      X_1'X_1 / \sigma_{11}^2 & 0 \\ 0 & X_2'X_2 / \sigma_{22}^2
    \end{pmatrix}^{-1}.
  \end{equation*}
  The second term is
  \begin{equation*}
    X'(\Sigma \otimes I)^{-1}Y =
    \begin{pmatrix}      
      X_1' Y_1 / \sigma_{11}^2 \\ X_2'Y_2 / \sigma_{22}^2
    \end{pmatrix}.
  \end{equation*}
  And the estimator becomes
  \begin{equation*}
    \betah_{SUR} =
    \begin{pmatrix}
      (X_1'X_1)^{-1}X_1'Y_1 \\ (X_2'X_2)^{-1}X_2'Y_2
    \end{pmatrix}
    =
    \begin{pmatrix}
      \betah_{1,OLS} \\ \betah_{2,OLS}
    \end{pmatrix}.
  \end{equation*}

\item Now suppose that $X_1$ and $X_2$ both equal $X$.
  The SUR estimator becomes
  \begin{align*}
    \betah_{SUR}
    &= \begin{pmatrix}
      \gamma_{11} X_1'X_1 & \gamma_{12} X_1'X_2 \\ & \gamma_{22} X_2'X_2 
    \end{pmatrix}^{-1}
    \begin{pmatrix}
      \gamma_{11} X_1'Y_1 + \gamma_{12} X_1'Y_2 \\
      \gamma_{12} X_2'Y_1 + \gamma_{22} X_2'Y_2
    \end{pmatrix} \\
    &= \begin{pmatrix}
      \gamma_{11} X'X & \gamma_{12} X'X \\ & \gamma_{22}X'X
    \end{pmatrix}^{-1}
    \begin{pmatrix}
      \gamma_{11} X'Y_1 + \gamma_{12}X'Y_2 \\
      \gamma_{12} X'Y_1 + \gamma_{22} X'Y_2
    \end{pmatrix}
  \end{align*}

  The first term becomes $(\Sigma^{-1} \otimes X'X)^{-1}$ which equals
  $\Sigma \otimes (X'X)^{-1}$. The second term becomes
  \[\begin{pmatrix}
    X'X (\gamma_{11}\betah_{1,OLS} + \gamma_{12}\betah_{2,OLS}) \\
    X'X (\gamma_{21}\betah_{1,OLS} + \gamma_{22}\betah_{2,OLS})
  \end{pmatrix}\] after premultiplying each $X'Y_j$ with
  $X'X(X'X)^{-1}$. Now Multiply through and everything cancels, giving
  \begin{equation*}
    \betah_{SUR} =
    \begin{pmatrix} \betah_{1,OLS} \\ \betah_{2,OLS} \end{pmatrix}
  \end{equation*}
\end{itemize}

\subsection{Hypothesis testing}

\begin{itemize}[leftmargin=0pt]
\item Testing is exactly like in OLS or GLS: we have an asymptotically
  normal estimator with a covariance matrix that can be consistently
  estimated.  The (robust, usually) \ttest\ and \ftest\ work fine.
\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../core_econometrics"
%%% End:
