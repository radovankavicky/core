% Copyright (c) 2013, authors of "Core Econometrics;" a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\chapter{Systems of equations}

\begin{itemize}[leftmargin=0pt]
\item There are lots of panel implications to what we're discussing,
      but we won't cover them
\item Basic idea:
\begin{itemize}
\item Suppose you have two relationships you are interested in
        (return data, say)
        \[r_{jt} - r_{ft} = \alpha_j + \beta_j (r_{mt} - r_{ft}) + \epsilon_{it}\]
\begin{itemize}
\item $j = 1,2$
\item $t = 1,2,3,...$
\item $r_{it}$ is the return on asset $i$
\item $r_{ft}$ is the risk-free rate of return (ie T-bills)
\item $r_{mt}$ is the return on the entire market.
\end{itemize}
\end{itemize}
\item here's the question: should you do OLS on each equation
      individually, or do something different?
\begin{itemize}
\item we'll derive a new estimator
\item called seemingly unrelated regression
\item obviously, if the errors are exogenous you can do ols and it
        will be fine; the question here is if we can do something
        better than OLS.
\end{itemize}
\end{itemize}

\section{setup}
\subsection{notation}

\begin{itemize}[leftmargin=0pt]
\item We want to write these two models as
       \[y_1 = X_1 \beta_1 + \epsilon_1\]
       and
       \[y_2 = X_2 \beta_2 + \epsilon_2\]
\begin{itemize}
\item each $y_i$ is $n \times 1$
\end{itemize}
\item alternatively
  \[ \binom{y_{1i}}{y_{2i}} = \binom{x_{1i}'\beta_1}{x_{2i}'\beta_2} + 
  \binom{\epsilon_{1i}}{ \epsilon_{2i}}\]
       for $i = 1,...,n$.
\end{itemize}

\subsection{assumptions}

\begin{description}
\item[Exogeneity] \[E(\binom{\vep_{1i}}{\vep_{2i}} \mid X) = 0\] for all $i$
\item[homoskedasticity] \[E\Big(\binom{\epsilon_{1i}}{\epsilon_{2i}} \binom{\epsilon_{1i}}{\epsilon_{2i}}' \mid X \Big) = \Sigma\]
\item[uncorrelated errors] \[E\Big(\binom{\epsilon_{1i}}{\epsilon_{2i}}
  \binom{\epsilon_{1j}}{\epsilon_{2j}}' \mid X \Big) = 0\] if $i \neq j$.
\begin{itemize}
\item allow errors to be correlated across equations but not across
         observations
\end{itemize}
\item[usual] assume that the usual technical conditions on the
                regressors and errors hold to allow for us to apply
                laws of large numbers, etc.
\end{description}

\subsection{estimation with known covariance matrix}

\begin{itemize}[leftmargin=0pt]
\item We want to rewrite the equations as
       \[ \binom{\mathbf{y}_1}{\mathbf{y}_2} = \begin{pmatrix} \mathbf{X}_1
       & 0 \\ 0 & \mathbf{X}_2  \end{pmatrix}
       \binom{\beta_1}{\beta_2} +
      \binom{\mathbf{\vep}_1}{\mathbf{\vep}_2} \]
\begin{itemize}
\item ie $\mathbf{y} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}$
\end{itemize}
\item Remarkable thing is that we can estimate this model by GLS
\begin{itemize}
\item Note that $E(\mathbf{\epsilon} \mid \mathbf{X}) = 0$
\item Moreover,
  \[E(\mathbf{\epsilon} \mathbf{\epsilon}' \mid \mathbf{X}) = \Sigma \otimes I =
  \begin{pmatrix} \sigma^2_{11} I & \sigma_{12} I \\ \sigma_{21} I & \sigma^2_{22} I \end{pmatrix}\]
\item So the model satisfies the requirements for GLS regression to
         be BLUE, even though the dependent variable is different for
         the two equations.
\end{itemize}
\item Assume that $\Sigma$ is known for now, 
  \[\betah_{GLS} = (X'(\Sigma \otimes I)^{-1}X)^{-1}X'(\Sigma \otimes I)^{-1}Y\]
\begin{itemize}
\item simplify this further:
\begin{itemize}
\item $(\Sigma \otimes I)^{-1} = \Sigma^{-1} \otimes I$
\begin{itemize}
\item Let $\gamma_{ij}$ denote the $i,j$ element of $\Sigma^{-1}$
\end{itemize}
\item as a result
  \[\betah_{SUR} = \begin{pmatrix}
    \gamma_{11} X_1'X_1 & \gamma_{12} X_1'X_2 \\ & \gamma_{22} X_2'X_2 
  \end{pmatrix}^{-1} 
  \begin{pmatrix}
    \gamma_{11} X_1'y_1 + \gamma_{12} X_1'y_2 \\ \gamma_{12} X_2'y_1 + \gamma_{22} X_2'y_2
  \end{pmatrix} \]
\end{itemize}
\end{itemize}
\item variance-covariance matrix is found as usual:
  \[\var(\betah_{GLS} \mid X) = (X'(\Sigma \otimes I)^{-1}X)^{-1}\]
\begin{itemize}
\item if you don't see this right away, please work through the
         math on your own.  The logic is identical to OLS or GLS.
\end{itemize}
\end{itemize}

\subsection{estimation with unknown covariance}

\begin{itemize}[leftmargin=0pt]
\item Big difference from single-equation case
\begin{itemize}
\item with single equations, we were concerned about heteroskedasticity
\item here, we assume homoskedasticity and uncorrelated observations
\item allow correlation across equations.
\end{itemize}
\item We can estimate $\Sigma$ pretty easily:
\begin{itemize}
\item let $(\eph_{i1}, \eph_{i2})$ be the pair of
         residuals for observation $i$ from the OLS estimation
\begin{itemize}
\item \textbf{remember} OLS is consistent
\item so $(\eph_{i1}, \eph_{i2})$ is consistent for $(\epsilon_{i1}, \epsilon_{i2})$
\end{itemize}
\item LLN would impliy that
  \[ n^{-1} \sum_i (\epsilon_{i1}, \epsilon_{i2})' (\epsilon_{i1}, \epsilon_{i2}) \to \Sigma\] in
  probability
\end{itemize}
\item Consistency of the residuals implies
  \[\Sigmah = n^{-1} \sum_i (\eph_{i1}, \eph_{i2})'(\eph_{i1},\hat \epsilon_{i2})\to \Sigma
  \quad i.p.\]
\item plug in to GLS formula for a feasible estimator
\end{itemize}

\section{Simplification of SUR}

     The SUR becomes the OLS estimator in two specific and informative
     cases
\begin{itemize}
\item errors are uncorrelated across models
\item all of the same regressors appear in both models
\begin{itemize}
\item VARs in macro, for example
\end{itemize}
\end{itemize}

\paragraph{zero correlation}
\begin{itemize}
\item formula for SUR estimator:
  \[\betah_{SUR} = (X'(\Sigma \otimes I)^{-1}X)^{-1}X'(\Sigma \otimes I)^{-1}Y\]
\item Suppose that $\sigma_{12} = 0$, so
  \[ \Sigma \otimes I = (\begin{matrix} \sigma_{11}^2 I & 0 \\ 0 & \sigma_{22}^2 I \end{matrix})\]
\begin{itemize}
\item First part becomes
  \[(X'( \Sigma \otimes I)^{-1} X) =
  \begin{pmatrix} X_1'X_1 / \sigma_{11}^2 & 0 \\ 0 & X_2'X_2 / \sigma_{22}^2 \end{pmatrix}\]
\item Second part becomes 
  \[X'(\Sigma \otimes I)^{-1}Y = \binom{X_1' y_1 / \sigma_{11}^2}{X_2'y_2 / \sigma_{22}^2}\]
\item $\betah_{SUR} = \binom{(X_1'X_1)^{-1}X_1'y_1}{(X_2'X_2)^{-1}X_2'y_2} =
  \binom{\betah_{1,ols}}{\betah_{2,ols}}$
\end{itemize}
\end{itemize}

\paragraph{Identical regressors in both models}
\begin{itemize}
\item Let $X_1$ and $X_2$ both equal $X$
\item Use the simplified form of the SUR estimator:
  \begin{align*}
    \betah_{SUR}
    &= \begin{pmatrix}
      \gamma_{11} X_1'X_1 & \gamma_{12} X_1'X_2 \\ & \gamma_{22} X_2'X_2 
    \end{pmatrix}^{-1}
    \begin{pmatrix}
      \gamma_{11} X_1'y_1 + \gamma_{12} X_1'y_2 \\ \gamma_{12} X_2'y_1 + \gamma_{22} X_2'y_2
    \end{pmatrix} \\
    &= \begin{pmatrix}
      \gamma_{11} X'X & \gamma_{12} X'X \\ & \gamma_{22}X'X
    \end{pmatrix}^{-1}
    \begin{pmatrix}
      \gamma_{11} X'y_1 + \gamma_{12}X'y_2 \\ \gamma_{12} X'y_1 + \gamma_{22} X'y_2
    \end{pmatrix}
  \end{align*}
\item First term becomes $(\Sigma^{-1} \otimes X'X)^{-1}$ which equals
  $\Sigma \otimes (X'X)^{-1}$
\item second term becomes
  \[\begin{pmatrix}
    X'X (\gamma_{11}\betah_{1,ols} + \gamma_{12}\betah_{2,ols}) \\
    X'X (\gamma_{21}\betah_{1,ols} + \gamma_{22}\betah_{2,ols})
  \end{pmatrix}\]
\begin{itemize}
\item Premultipy each $X'y_j$ with $X'X(X'X)^{-1}$
\end{itemize}
\item Multiply through and everything cancels, giving
  \[\betah_{SUR} = \binom{\betah_{1,ols}}{\betah_{2,ols}}\]
\end{itemize}

\section{Hypothesis testing}

\paragraph{similarities to OLS/GLS}
\begin{itemize}
\item $\betah_{SUR}$ is the estimate of $\beta_1$ and $\beta_2$
        stacked on top of eachother
\item We can test hypotheses about the $\beta$ as usual
\begin{itemize}
\item hypotheses about a single equation
          \[ \beta_{1,1} = 0 \]
          for example
\item hypotheses about coefficients from different equations
\begin{itemize}
\item For example:
\begin{itemize}
\item $Y_1$ is log consumption of one good (imagine an individual)
\item $Y_2$ is log consumption of another, similar good
\item Regressors include log prices
\item may want to test that the elasticty of both goods are
              the same
\end{itemize}
\end{itemize}
\end{itemize}
\end{itemize}

\paragraph{nature of hypothesis}
\begin{itemize}
\item We can write both sorts of hypotheses as
        \[ R \beta = q \]
\item the variance of $R\betah - R\beta$ is given by $R(X'(\Sigma \otimes I)^{-1}X)^{-1}R'$
\item Under the null, 
  \[(R\betah - q)'\left(R(X'(\Sigma \otimes I)^{-1}X)R'\right)^{-1}(R\betah - q)\]
  is asymptotically chi-square with $J$-degrees of freedom
\item Since $\Sigmah$ is consistent for $\Sigma$, so is
  \[(R\betah - q)'\left(R(X'(\Sigmah \otimes I)^{-1}X)R'\right)^{-1}(R\betah - q)\]
\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../core_econometrics"
%%% End:
