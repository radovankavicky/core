% Copyright (c) 2013-2014, Gray Calhoun.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\section{Inference}

\subsection{Estimation of linear regression under restrictions}

\begin{itemize}[leftmargin=0pt]

\item Let's start with a simple example (one that uses economic
  variables instead of letters; coincedentally, this is the same
  example used in \citealp[p. 81]{Gre12}).  Suppose we have a simple
  model of investment,
  \begin{equation}\label{eq:1}
    \log I_t = \beta_1 + \beta_2 i_t + \beta_3 \Delta p_t + \beta_4 \log Y_t + \beta_5 t + \vep_t
  \end{equation}
  where
  \begin{itemize}
  \item $I_t$ is investment in period $t$
  \item $i_t$ is the nominal interest rate
  \item $\Delta p_t$ is the rate of inflation
  \item $Y_t$ is real output
  \item $t$ is a time trend.
  \end{itemize}
  We might think that the inflation rate and the nominal interest rate
  don't matter individually, but that the real interest rate is a key
  driver of investment decisions.  That would imply that the
  investment equation should be (to a rough approximation),
  \begin{equation}\label{f11}
    \log I_t = \beta_1 + \beta_2 (i_t - \Delta p_t) + \beta_4 \log Y_t + \beta_5 t + \vep_t.
  \end{equation}
  
  We can estimate this new model by using $i_t - \Delta p_t$ as a regressor
  instead of $i_t$ and $\Delta p_t$, but we can also estimate the model by
  estimating~\eqref{f11} under the constraint $\beta_2 = - \beta_3$.

\item In general, this amounts to solving an optimization problem
  under a constraint.  We'll focus on linear constraints here for
  simplicity, so the estimator becomes
  \begin{equation}\label{f4}
    \betah = \argmin_\beta \sum_{i=1}^n (y_i - x_i'\beta)^2 = \argmin_\beta (Y - X\beta)(Y-X\beta)
  \end{equation}
  subject to the constraint
  \begin{equation}\label{f5}
    R \beta = q
  \end{equation}
  where $R$ and $q$ are an arbitrary $j \times k$ matrix ($j \leq k$) and $j \times
  1$ vector respectively that are known and set by the researcher and
  $R$ is assumed to have rank $j$.

  Most economics graduate students will have solved dozens of
  constrained optimization problems by the time they read this
  passage, so we'll just do a sketch of the
  solution.\footnote{\citet{SB94} is a reasonably comprehensive
    resource for results like these and it is typically required
    reading by graduate economics programs.}  Set up the Lagrangian
  \begin{equation}\label{f6}
    (Y - X\beta)'(Y - X\beta) + (R\beta - q)' \lambda
  \end{equation}
  and take derivatives with respect to $\beta$ to get the first order
  conditions
  \begin{equation}\label{f3}
    0 = 2X'X \beta^* - 2 X'Y + R'\lambda^*
  \end{equation}
  and the original constraint~\eqref{f5}, where the star indicates
  that the variable solves the constrained optimization problem.
  
  We can rewrite Equation~\eqref{f3} as
  \begin{equation}\label{f8}
    \beta^* = \betah - (1/2) (X'X)^{-1} R'\lambda^*,
  \end{equation}
  where $\betah$ is the usual OLS estimator, and premultiplying by $R$
  gives
  \begin{equation}\label{f7}
    R\beta^* = R \betah - (1/2) R (X'X)^{-1} R'\lambda^*.
  \end{equation}
  Since $R\beta^* = q$,~\eqref{f7} determines $\lambda^*$:
  \begin{equation}\label{f9}
    (1/2) \lambda^* = (R (X'X)^{-1} R')^{-1} (R \betah - q)
  \end{equation}
  and substituting~\eqref{f9} into~\eqref{f8} gives the solution,
  \begin{equation}\label{f10}
    \beta^* = \betah - (X'X)^{-1} R' (R (X'X)^{-1} R')^{-1} (R \betah - q)
  \end{equation}

\item Notice that $X\beta^*$ can be interpreted as a projection onto a
  subspace of the columns of $X$.  If we define $Z = X (X'X)^{-1} R'$
  then
  \begin{equation*}
    X \beta^* = (X(X'X)^{-1}X' - Z(Z'Z)Z') Y \equiv (P_X - P_Z) Y,
  \end{equation*}
  and notice that $P_X P_Z = P_Z$.  On reflection this should not be
  suprising.  If the restriction amounts to imposing that some of the
  coefficients are zero, it's obvious.  Otherwise, there's a rotation
  of the design matrix $X$ such that the restriction is equivalent to
  imposing zero on some of the coefficients in the rotated design
  matrix.

\item The restricted estimator of $\sigma^2$ under this restriction is going
  to be
  \begin{equation*}
    (1/(n - k + \rank(R))) \sum_i (y_i - x_i'\beta^*)^2.
  \end{equation*}

\end{itemize}

\subsection{Finite-sample hypothesis testing in linear regression}

\begin{itemize}[leftmargin=0pt]

\item Suppose that we want to test an arbitrary linear hypothesis about
  $\beta$; ie \[R \beta = q\] against the alternative \[R \beta \neq q\]
\begin{itemize}
\item ie $R = (1, 0, 0, ...)$ and $q=0$ gives us a test that $\beta_0=0$
\item $R = I$ and $q = (0,0,...,0)$ gives us a test that all of the
         coefficients are equal to zero.
\end{itemize}
\item for now, assume we have normal errors
\end{itemize}

\paragraph{change in SSR under the null}
      we can look at the change in the SSR when we impose the null
        hypothesis
\begin{itemize}
\item ie ${SSR_R - SSR \over SSR}$
\begin{itemize}
\item $SSR = \sum_i \veph^2_i$
\item don't need to present this, but can be written as \[
  {(R^2 - R^2_R) / J \over (1-R^2) / (n-k-1)} \]
\end{itemize}
\item Our test is actually a scaled version of that:
  \[ F = {(SSR_R - SSR)/J \over SSR / (n-k-1)} \]
\begin{itemize}
\item $J$ is the number of restrictions (ie dimension of $q$)
\end{itemize}
\end{itemize}

\paragraph{Distribution of F under null}
      now, suppose the null is true, what is the distribution of $F$?

\paragraph{distribution of numerator}
\begin{itemize}
\item reexpress the numerator
  \begin{align*}
    SSR_R - SSR
    &= (Y - X\betah_R)'(Y - X\betah_R) - (Y - X\betah)'(Y - X\betah) \\
    &= (\betah - \betah_R)'X'X(\betah - \betah_R)
  \end{align*}
  (you're proving this for homework).  Remember that
  \begin{align*}
    \betah_R &= \betah + (X'X)^{-1} R'(R(X'X)^{-1}R')(q - R\betah) \\
    &= (q - R\betah)' (R(X'X)^{-1}R')^{-1} R(X'X)^{-1}X'X(X'X)^{-1}R' (R(X'X)^{-1}R')^{-1} (q - R\betah) \\
    &= (q - R\betah)' (R(X'X)^{-1}R')^{-1} R(X'X)^{-1} R'(R(X'X)^{-1}R')^{-1} (q - R\betah) \\
    &= (q - R\betah)' (R(X'X)^{-1}R')^{-1} (q - R\betah)
  \end{align*}
\item distribution of numerator
\begin{itemize}
\item $q - R\betah$ is normal with mean $q - R\beta$ and variance $\sigma^2
  R(X'X)^{-1}R'$
\item under the null, this mean is zero.
\item so we have a normal divided by its variance covariance matrix...
\item so $(q - R\betah)'(R(X'X)^{-1}R')^{-1}(q - R\betah)$ equals $\sigma^2$
  chi-square r.v. with $J$ degrees of freedom
\end{itemize}
\end{itemize}

\paragraph{distribution of denominator}
\begin{itemize}
\item just like earlier, we know that $SSR = (n-k-1) s^2$ and $s^2$
         and $\betah$ are independent given $X$.
\item denominator is $\sigma^2$ times a chi-square with $n-k$
         degrees of freedom and indpendent of the numerator.
\end{itemize}

\paragraph{distribution of statistic}
       distribution of $F$
\begin{itemize}
\item $F = {\sigma^2 \chi^2_J / J \over \sigma^2 \chi^2_{n-k-1} / (n-k-1)}$ in
  distribution.
\item numerator and denominator are independent
\item so this has an $F_{J, n-k-1}$ distribution under the null.
\end{itemize}

\paragraph{Distribution of F under alternative}
\begin{itemize}
\item Denominator is not affected
\item Numerator is
\begin{itemize}
\item for $R\betah - q$ to have mean zero, we need the null to
          be true
\item otherwise the numerator will get larger
\end{itemize}
\end{itemize}

\subsection{t-test}

\begin{itemize}[leftmargin=0pt]

\item Suppose you wanted to test a single restriction, say $\beta_i = b$
  for some known $b$.
\item We know that ${\betah_i - \beta_i \over \sqrt{s^2 q_i}}$ is the ratio of
  a standard normal r.v. and a chi-square/(n-k-1) random variable
\item so it is $t$ with (n-k-1) degrees of freedom
\item under the null, we know $\beta_i = b$, so we also have ${\betah_i - b
    \over \sqrt{s^2 q_i}}$
\item we can use this as a test statistic:
  \begin{itemize}
  \item calculate the value of the r.v.
  \item get the appropriate critical values from the t-distribution
    table (or the computer)
  \item reject if the statistic is farther from zero than the critical
    value
  \end{itemize}
\item If we're testing an equality, this will give us exactly the same
  test as the F-test with 1 and $n-k-1$ degrees of freedom
\item if we're testing an inequality, this test can be a little easier
  to work with.
\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../core_econometrics"
%%% End:
