% Copyright (c) 2013, authors of "Core Econometrics;" a
% complete list of authors is available in the file AUTHORS.tex.

% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation; with no Invariant Sections, no Front-Cover Texts, and no
% Back-Cover Texts.  A copy of the license is included in the file
% LICENSE.tex and is also available online at
% <http://www.gnu.org/copyleft/fdl.html>.

\chapter{Algebraic properties of OLS}

\section{Algebraic and geometric properties of linear regression}

\begin{itemize}[leftmargin=0pt]

\item Before worrying too much about statistical issues, we'll discuss
  a few issues that come up just because of the math involved in the
  linear regression estimator.  These issues have no statistical
  content, but are numeric identities (almost, that's not a fantastic
  way to say it).

\item Draw pictures in $\RR^n$ for $n = 3$.  A good set of values is
  \begin{align*}
    Y &= \begin{pmatrix}1.5 \\ 0.5 \\ 3\end{pmatrix}&
    X &=
    \begin{pmatrix}
      1  &  3 \\
      1  &  1 \\
      1  &  2
    \end{pmatrix}
  \end{align*}

\item The same pictures can be useful to understand the operations
  when adding a variable.  Suppose you regress $Y$ on $X_1$ and get an
  estimate $\betat_1$, then decide to regress $Y$ on $X_1$ and $X_2$, giving
  $\betah_1$ and $\betah_2$.  There is a relationship between these estimates
  \begin{thm}[Frisch-Waugh-Lovel Theorem]
    Let $e = Y - X_1\betat_1$ and define
    \begin{equation*}
      Z = (I - X_1(X_1'X_1)X_1')X_2,
    \end{equation*}
    (i.e. $Z$ consists of the residuals after regressing each column
    of $X_2$ on $X_1$.  Then $\betah_2 = (Z'Z)^{-1}Z'e$.
  \end{thm}
  Similar formulae exist for getting $\betah_1$ from $\betat_1$

  [Need to scan pictures and add them]

  This result is less useful than it used to be, because computers are
  far more powerful.  But the intuition is still just as useful---this
  is what we mean by ``partial out''---and the result itself can still
  be useful when dealing with very large data sets.

\item We may be interested in knowing how far $Y$ is from $\Yh$; a
  good comparison is how far relative to the distance between $Y$ and
  $\iota \Yb$.  This is what the centered $R$-squared measures:
  \begin{equation*}
    R^2 = 1 - \frac{ \lVert Y - \Yh \rVert_2^2}{\lVert Y - \iota \Yb \rVert^2}.
  \end{equation*}
  As long as the model contains a constant, this will be between zero
  and one.

  You should realize that $R^2$ is almost useless as a guide to whether
  or not your model is ``good.''  There are a few specific
  applications (mostly forecasting) where I was somewhat interested in
  the model's $R^2$, but that's it (knowing that it's high can
  sometimes reassure you that there's no need to look for a model with
  more predictive power).

\end{itemize}

\section{Making predictions from regression models}

\subsection{motivation and setup}
\begin{itemize}[leftmargin=0pt]
\item Say we've estimated a regression model.  What do we do with it?
\item Easiest possible use: make forecasts
\item Setup:
\begin{itemize}
\item have observations $(y_i, x_i)$ for $i = 1,...,n$
\item observe regressors for period $n+1$
\item want to predict $y_{n+1}$
\end{itemize}
\item model \[ y_i = x_i'\beta + \epsilon_i \]
\item Estimated by OLS (or, maybe, GLS)
\end{itemize}

\subsection{natural forecast}

\begin{itemize}[leftmargin=0pt]
\item The natural forecast for $y_{n+1}$ is $x_{n+1}'\betah$
\begin{itemize}
\item $x_{n+1}'\beta$ would be the forecast that minimizes expected
         squared error
\item $\betah$ is our best (minimum-variance) estimator of $\beta$
\item Expected value of $\hat y_{n+1}$ is $y_{n+1}$
\end{itemize}
\item How reliable is our forecast?
\begin{itemize}
\item more precisely, what is the variance of the forecast error?
\item Define the forecast error as 
  \[ e_{n+1} = y_{n+1} - \hat y_{n+1} = \epsilon_{n+1} - x_{n+1}'(\betah - \beta) \]
\end{itemize}
\item Variance of forecast erro is going to reflect
\begin{itemize}
\item variance of $\betah$
\item variance of $\epsilon_{n+1}$
\end{itemize}
\end{itemize}

\subsection{calculation of variance}

     We're going to assume that the errors are uncorrelated (like we
     have) and homoskedastic.
\begin{itemize}[leftmargin=0pt]
\item conditionally heteroeskedastic errors can be dealt with by GLS
\item The usual calculation gives us
  \[ \var(e_{n+1} \mid X, x_{n+1}) = \E(e_{n+1}^2 \mid X_{n+1}) = \E(\epsilon_{n+1}^2
  \mid X, x_{n+1}) + \E(((\betah - \beta)'x_{n+1})^2 \mid X, x_{n+1})\]
\item the first term is just $\sigma^2$
\item the second is 
  \[ x_{n+1}' \var(\betah \mid X) x_{n+1} = \sigma^2 x_{n+1}'(X'X)^{-1}x_{n+1}\]
\item draw a scatterplot to illustrate why $\hat y$ will depend on
       the particular value of $x$.
\end{itemize}

\subsection{common use of this variance}

\begin{itemize}[leftmargin=0pt]
\item \textbf{if the errors are normal}, we can construct a confidence
       interval for $y_{n+1}$ from this result
\item $\hat y_{n+1} \pm t_{\alpha/2} \sqrt{s^2 (1 + x_{n+1}'(X'X)^{-1}x_{n+1})}$
\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../core_econometrics"
%%% End: 
